{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:53:14,045][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s1a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235315-hsg1m3hk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s1a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/hsg1m3hk\u001b[0m\n",
      "[2023-08-08 23:53:20,252][root][INFO] - => Done in 6.206 s\n",
      "[2023-08-08 23:53:20,252][root][INFO] - \n",
      "[2023-08-08 23:53:20,252][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:53:20,255][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:53:20,255][root][INFO] - => Done in 3.267 ms\n",
      "[2023-08-08 23:53:20,255][root][INFO] - \n",
      "[2023-08-08 23:53:20,255][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:53:20,868][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:53:20,868][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:53:20,868][root][INFO] - => Done in 612.879 ms\n",
      "[2023-08-08 23:53:20,868][root][INFO] - \n",
      "[2023-08-08 23:53:20,868][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:53:20,869][root][INFO] - => Done in 112.057 us\n",
      "[2023-08-08 23:53:20,869][root][INFO] - \n",
      "[2023-08-08 23:53:20,869][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:53:20,869][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:53:21,718][root][INFO] - => Done in 849.211 ms\n",
      "[2023-08-08 23:53:21,718][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.861 MB of 1.873 MB uploaded (0.676 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s1a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/hsg1m3hk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 77 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235315-hsg1m3hk/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:53:56,991][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235358-q688plg1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/q688plg1\u001b[0m\n",
      "[2023-08-08 23:54:02,937][root][INFO] - => Done in 5.946 s\n",
      "[2023-08-08 23:54:02,938][root][INFO] - \n",
      "[2023-08-08 23:54:02,938][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:54:02,944][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:54:02,945][root][INFO] - => Done in 6.661 ms\n",
      "[2023-08-08 23:54:02,945][root][INFO] - \n",
      "[2023-08-08 23:54:02,945][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:54:03,510][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:54:03,510][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:54:03,510][root][INFO] - => Done in 565.461 ms\n",
      "[2023-08-08 23:54:03,510][root][INFO] - \n",
      "[2023-08-08 23:54:03,510][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:54:03,511][root][INFO] - => Done in 144.243 us\n",
      "[2023-08-08 23:54:03,511][root][INFO] - \n",
      "[2023-08-08 23:54:03,511][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:54:03,511][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:54:04,286][root][INFO] - => Done in 775.650 ms\n",
      "[2023-08-08 23:54:04,287][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.967 MB of 0.967 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/q688plg1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235358-q688plg1/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:54:38,345][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235439-irl1tlit\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/irl1tlit\u001b[0m\n",
      "[2023-08-08 23:54:43,667][root][INFO] - => Done in 5.322 s\n",
      "[2023-08-08 23:54:43,668][root][INFO] - \n",
      "[2023-08-08 23:54:43,668][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:54:43,671][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:54:43,671][root][INFO] - => Done in 3.283 ms\n",
      "[2023-08-08 23:54:43,671][root][INFO] - \n",
      "[2023-08-08 23:54:43,671][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:54:44,224][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:54:44,224][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:54:44,224][root][INFO] - => Done in 552.874 ms\n",
      "[2023-08-08 23:54:44,224][root][INFO] - \n",
      "[2023-08-08 23:54:44,224][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:54:44,224][root][INFO] - => Done in 115.156 us\n",
      "[2023-08-08 23:54:44,224][root][INFO] - \n",
      "[2023-08-08 23:54:44,224][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:54:44,224][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:54:44,988][root][INFO] - => Done in 763.787 ms\n",
      "[2023-08-08 23:54:44,988][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/irl1tlit\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235439-irl1tlit/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:55:19,291][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235520-z6fdh0ko\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/z6fdh0ko\u001b[0m\n",
      "[2023-08-08 23:55:25,005][root][INFO] - => Done in 5.715 s\n",
      "[2023-08-08 23:55:25,006][root][INFO] - \n",
      "[2023-08-08 23:55:25,006][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:55:25,010][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:55:25,010][root][INFO] - => Done in 4.494 ms\n",
      "[2023-08-08 23:55:25,010][root][INFO] - \n",
      "[2023-08-08 23:55:25,010][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:55:25,568][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:55:25,568][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:55:25,569][root][INFO] - => Done in 558.234 ms\n",
      "[2023-08-08 23:55:25,569][root][INFO] - \n",
      "[2023-08-08 23:55:25,569][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:55:25,569][root][INFO] - => Done in 107.050 us\n",
      "[2023-08-08 23:55:25,569][root][INFO] - \n",
      "[2023-08-08 23:55:25,569][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:55:25,569][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:55:26,332][root][INFO] - => Done in 762.997 ms\n",
      "[2023-08-08 23:55:26,332][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/z6fdh0ko\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235520-z6fdh0ko/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:56:01,176][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235602-fno6rhes\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fno6rhes\u001b[0m\n",
      "[2023-08-08 23:56:07,151][root][INFO] - => Done in 5.975 s\n",
      "[2023-08-08 23:56:07,151][root][INFO] - \n",
      "[2023-08-08 23:56:07,151][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:56:07,158][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:56:07,158][root][INFO] - => Done in 6.556 ms\n",
      "[2023-08-08 23:56:07,158][root][INFO] - \n",
      "[2023-08-08 23:56:07,158][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:56:07,724][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:56:07,724][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:56:07,725][root][INFO] - => Done in 566.567 ms\n",
      "[2023-08-08 23:56:07,725][root][INFO] - \n",
      "[2023-08-08 23:56:07,725][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:56:07,725][root][INFO] - => Done in 159.979 us\n",
      "[2023-08-08 23:56:07,725][root][INFO] - \n",
      "[2023-08-08 23:56:07,725][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:56:07,725][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:56:08,485][root][INFO] - => Done in 760.429 ms\n",
      "[2023-08-08 23:56:08,486][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.958 MB of 0.970 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fno6rhes\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235602-fno6rhes/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:56:42,197][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s1a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235643-51j4fbb0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s1a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/51j4fbb0\u001b[0m\n",
      "[2023-08-08 23:56:48,694][root][INFO] - => Done in 6.496 s\n",
      "[2023-08-08 23:56:48,694][root][INFO] - \n",
      "[2023-08-08 23:56:48,694][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:56:48,697][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:56:48,697][root][INFO] - => Done in 3.130 ms\n",
      "[2023-08-08 23:56:48,697][root][INFO] - \n",
      "[2023-08-08 23:56:48,697][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:56:49,279][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:56:49,279][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:56:49,279][root][INFO] - => Done in 582.045 ms\n",
      "[2023-08-08 23:56:49,279][root][INFO] - \n",
      "[2023-08-08 23:56:49,279][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:56:49,279][root][INFO] - => Done in 113.964 us\n",
      "[2023-08-08 23:56:49,279][root][INFO] - \n",
      "[2023-08-08 23:56:49,280][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:56:49,280][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:56:50,040][root][INFO] - => Done in 760.174 ms\n",
      "[2023-08-08 23:56:50,040][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s1a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/51j4fbb0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235643-51j4fbb0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:57:24,903][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235725-8hefgpwp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/8hefgpwp\u001b[0m\n",
      "[2023-08-08 23:57:29,664][root][INFO] - => Done in 4.761 s\n",
      "[2023-08-08 23:57:29,664][root][INFO] - \n",
      "[2023-08-08 23:57:29,664][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:57:29,668][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:57:29,668][root][INFO] - => Done in 3.303 ms\n",
      "[2023-08-08 23:57:29,668][root][INFO] - \n",
      "[2023-08-08 23:57:29,668][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:57:30,256][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:57:30,256][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:57:30,256][root][INFO] - => Done in 587.932 ms\n",
      "[2023-08-08 23:57:30,256][root][INFO] - \n",
      "[2023-08-08 23:57:30,256][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:57:30,256][root][INFO] - => Done in 113.726 us\n",
      "[2023-08-08 23:57:30,256][root][INFO] - \n",
      "[2023-08-08 23:57:30,256][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:57:30,256][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:57:31,024][root][INFO] - => Done in 768.212 ms\n",
      "[2023-08-08 23:57:31,025][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/8hefgpwp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235725-8hefgpwp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:58:05,344][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235806-p0uqkfpg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/p0uqkfpg\u001b[0m\n",
      "[2023-08-08 23:58:10,560][root][INFO] - => Done in 5.216 s\n",
      "[2023-08-08 23:58:10,560][root][INFO] - \n",
      "[2023-08-08 23:58:10,560][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:58:10,564][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:58:10,564][root][INFO] - => Done in 3.246 ms\n",
      "[2023-08-08 23:58:10,564][root][INFO] - \n",
      "[2023-08-08 23:58:10,564][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:58:11,149][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:58:11,149][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:58:11,149][root][INFO] - => Done in 585.385 ms\n",
      "[2023-08-08 23:58:11,149][root][INFO] - \n",
      "[2023-08-08 23:58:11,149][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:58:11,149][root][INFO] - => Done in 113.726 us\n",
      "[2023-08-08 23:58:11,149][root][INFO] - \n",
      "[2023-08-08 23:58:11,149][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:58:11,150][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:58:11,916][root][INFO] - => Done in 766.089 ms\n",
      "[2023-08-08 23:58:11,916][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/p0uqkfpg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235806-p0uqkfpg/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:58:47,164][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235848-mpo32q4j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/mpo32q4j\u001b[0m\n",
      "[2023-08-08 23:58:52,634][root][INFO] - => Done in 5.471 s\n",
      "[2023-08-08 23:58:52,635][root][INFO] - \n",
      "[2023-08-08 23:58:52,635][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:58:52,639][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:58:52,639][root][INFO] - => Done in 3.973 ms\n",
      "[2023-08-08 23:58:52,639][root][INFO] - \n",
      "[2023-08-08 23:58:52,639][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:58:53,201][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:58:53,201][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:58:53,201][root][INFO] - => Done in 562.302 ms\n",
      "[2023-08-08 23:58:53,201][root][INFO] - \n",
      "[2023-08-08 23:58:53,201][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:58:53,201][root][INFO] - => Done in 110.865 us\n",
      "[2023-08-08 23:58:53,201][root][INFO] - \n",
      "[2023-08-08 23:58:53,201][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:58:53,201][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:58:53,961][root][INFO] - => Done in 759.319 ms\n",
      "[2023-08-08 23:58:53,961][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/mpo32q4j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235848-mpo32q4j/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 23:59:29,078][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_235930-eoxkii3m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/eoxkii3m\u001b[0m\n",
      "[2023-08-08 23:59:33,980][root][INFO] - => Done in 4.902 s\n",
      "[2023-08-08 23:59:33,980][root][INFO] - \n",
      "[2023-08-08 23:59:33,980][root][INFO] - => Env setup ...\n",
      "[2023-08-08 23:59:33,983][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 23:59:33,983][root][INFO] - => Done in 2.993 ms\n",
      "[2023-08-08 23:59:33,983][root][INFO] - \n",
      "[2023-08-08 23:59:33,984][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 23:59:34,553][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 23:59:34,553][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 23:59:34,553][root][INFO] - => Done in 569.586 ms\n",
      "[2023-08-08 23:59:34,553][root][INFO] - \n",
      "[2023-08-08 23:59:34,553][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 23:59:34,553][root][INFO] - => Done in 108.719 us\n",
      "[2023-08-08 23:59:34,553][root][INFO] - \n",
      "[2023-08-08 23:59:34,553][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 23:59:34,554][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 23:59:35,308][root][INFO] - => Done in 754.110 ms\n",
      "[2023-08-08 23:59:35,308][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/eoxkii3m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_235930-eoxkii3m/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:00:09,730][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s1a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000010-gy4f6f68\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s1a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/gy4f6f68\u001b[0m\n",
      "[2023-08-09 00:00:15,361][root][INFO] - => Done in 5.631 s\n",
      "[2023-08-09 00:00:15,361][root][INFO] - \n",
      "[2023-08-09 00:00:15,361][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:00:15,365][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:00:15,365][root][INFO] - => Done in 3.970 ms\n",
      "[2023-08-09 00:00:15,365][root][INFO] - \n",
      "[2023-08-09 00:00:15,365][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:00:15,922][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:00:15,922][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:00:15,922][root][INFO] - => Done in 556.851 ms\n",
      "[2023-08-09 00:00:15,922][root][INFO] - \n",
      "[2023-08-09 00:00:15,922][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:00:15,922][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-09 00:00:15,922][root][INFO] - \n",
      "[2023-08-09 00:00:15,922][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:00:15,922][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:00:16,688][root][INFO] - => Done in 765.767 ms\n",
      "[2023-08-09 00:00:16,688][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s1a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/gy4f6f68\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000010-gy4f6f68/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:00:50,325][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000051-pvv4cx2t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pvv4cx2t\u001b[0m\n",
      "[2023-08-09 00:00:55,003][root][INFO] - => Done in 4.678 s\n",
      "[2023-08-09 00:00:55,003][root][INFO] - \n",
      "[2023-08-09 00:00:55,003][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:00:55,009][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:00:55,009][root][INFO] - => Done in 6.148 ms\n",
      "[2023-08-09 00:00:55,009][root][INFO] - \n",
      "[2023-08-09 00:00:55,009][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:00:55,602][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:00:55,602][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:00:55,602][root][INFO] - => Done in 592.999 ms\n",
      "[2023-08-09 00:00:55,603][root][INFO] - \n",
      "[2023-08-09 00:00:55,603][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:00:55,603][root][INFO] - => Done in 117.064 us\n",
      "[2023-08-09 00:00:55,603][root][INFO] - \n",
      "[2023-08-09 00:00:55,603][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:00:55,603][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:00:56,375][root][INFO] - => Done in 772.585 ms\n",
      "[2023-08-09 00:00:56,376][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pvv4cx2t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000051-pvv4cx2t/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:01:30,166][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000131-uavrl40z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/uavrl40z\u001b[0m\n",
      "[2023-08-09 00:01:34,833][root][INFO] - => Done in 4.666 s\n",
      "[2023-08-09 00:01:34,833][root][INFO] - \n",
      "[2023-08-09 00:01:34,833][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:01:34,837][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:01:34,837][root][INFO] - => Done in 3.836 ms\n",
      "[2023-08-09 00:01:34,837][root][INFO] - \n",
      "[2023-08-09 00:01:34,837][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:01:35,432][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:01:35,433][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:01:35,433][root][INFO] - => Done in 595.459 ms\n",
      "[2023-08-09 00:01:35,433][root][INFO] - \n",
      "[2023-08-09 00:01:35,433][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:01:35,433][root][INFO] - => Done in 127.077 us\n",
      "[2023-08-09 00:01:35,433][root][INFO] - \n",
      "[2023-08-09 00:01:35,433][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:01:35,433][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:01:36,199][root][INFO] - => Done in 765.906 ms\n",
      "[2023-08-09 00:01:36,199][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/uavrl40z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000131-uavrl40z/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:02:09,863][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000210-0hosnmze\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/0hosnmze\u001b[0m\n",
      "[2023-08-09 00:02:14,727][root][INFO] - => Done in 4.864 s\n",
      "[2023-08-09 00:02:14,727][root][INFO] - \n",
      "[2023-08-09 00:02:14,727][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:02:14,730][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:02:14,730][root][INFO] - => Done in 2.936 ms\n",
      "[2023-08-09 00:02:14,730][root][INFO] - \n",
      "[2023-08-09 00:02:14,730][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:02:15,305][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:02:15,305][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:02:15,305][root][INFO] - => Done in 574.728 ms\n",
      "[2023-08-09 00:02:15,305][root][INFO] - \n",
      "[2023-08-09 00:02:15,305][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:02:15,305][root][INFO] - => Done in 108.004 us\n",
      "[2023-08-09 00:02:15,305][root][INFO] - \n",
      "[2023-08-09 00:02:15,305][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:02:15,305][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:02:16,066][root][INFO] - => Done in 760.749 ms\n",
      "[2023-08-09 00:02:16,066][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.980 MB of 0.980 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/0hosnmze\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000210-0hosnmze/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:02:50,097][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000251-fc8ztqls\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fc8ztqls\u001b[0m\n",
      "[2023-08-09 00:02:55,017][root][INFO] - => Done in 4.919 s\n",
      "[2023-08-09 00:02:55,017][root][INFO] - \n",
      "[2023-08-09 00:02:55,017][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:02:55,020][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:02:55,020][root][INFO] - => Done in 3.466 ms\n",
      "[2023-08-09 00:02:55,021][root][INFO] - \n",
      "[2023-08-09 00:02:55,021][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:02:55,579][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:02:55,580][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:02:55,580][root][INFO] - => Done in 559.068 ms\n",
      "[2023-08-09 00:02:55,580][root][INFO] - \n",
      "[2023-08-09 00:02:55,580][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:02:55,580][root][INFO] - => Done in 226.021 us\n",
      "[2023-08-09 00:02:55,580][root][INFO] - \n",
      "[2023-08-09 00:02:55,580][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:02:55,580][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:02:56,339][root][INFO] - => Done in 758.352 ms\n",
      "[2023-08-09 00:02:56,339][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fc8ztqls\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000251-fc8ztqls/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:03:31,771][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s1a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000332-fhlvlc33\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s1a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fhlvlc33\u001b[0m\n",
      "[2023-08-09 00:03:36,939][root][INFO] - => Done in 5.168 s\n",
      "[2023-08-09 00:03:36,939][root][INFO] - \n",
      "[2023-08-09 00:03:36,939][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:03:36,942][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:03:36,942][root][INFO] - => Done in 3.065 ms\n",
      "[2023-08-09 00:03:36,942][root][INFO] - \n",
      "[2023-08-09 00:03:36,942][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:03:37,506][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:03:37,507][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:03:37,507][root][INFO] - => Done in 564.336 ms\n",
      "[2023-08-09 00:03:37,507][root][INFO] - \n",
      "[2023-08-09 00:03:37,507][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:03:37,507][root][INFO] - => Done in 126.839 us\n",
      "[2023-08-09 00:03:37,507][root][INFO] - \n",
      "[2023-08-09 00:03:37,507][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:03:37,507][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:03:38,268][root][INFO] - => Done in 760.777 ms\n",
      "[2023-08-09 00:03:38,268][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s1a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/fhlvlc33\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000332-fhlvlc33/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:04:13,243][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000414-wg6wx480\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/wg6wx480\u001b[0m\n",
      "[2023-08-09 00:04:19,161][root][INFO] - => Done in 5.919 s\n",
      "[2023-08-09 00:04:19,162][root][INFO] - \n",
      "[2023-08-09 00:04:19,162][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:04:19,166][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:04:19,166][root][INFO] - => Done in 4.498 ms\n",
      "[2023-08-09 00:04:19,166][root][INFO] - \n",
      "[2023-08-09 00:04:19,166][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:04:19,725][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:04:19,725][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:04:19,725][root][INFO] - => Done in 558.555 ms\n",
      "[2023-08-09 00:04:19,725][root][INFO] - \n",
      "[2023-08-09 00:04:19,725][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:04:19,725][root][INFO] - => Done in 112.057 us\n",
      "[2023-08-09 00:04:19,725][root][INFO] - \n",
      "[2023-08-09 00:04:19,725][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:04:19,725][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:04:20,482][root][INFO] - => Done in 756.622 ms\n",
      "[2023-08-09 00:04:20,482][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/wg6wx480\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000414-wg6wx480/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:04:53,839][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000454-2g9csih9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/2g9csih9\u001b[0m\n",
      "[2023-08-09 00:04:58,954][root][INFO] - => Done in 5.115 s\n",
      "[2023-08-09 00:04:58,954][root][INFO] - \n",
      "[2023-08-09 00:04:58,954][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:04:58,958][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:04:58,958][root][INFO] - => Done in 3.436 ms\n",
      "[2023-08-09 00:04:58,958][root][INFO] - \n",
      "[2023-08-09 00:04:58,958][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:04:59,510][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:04:59,511][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:04:59,511][root][INFO] - => Done in 552.954 ms\n",
      "[2023-08-09 00:04:59,511][root][INFO] - \n",
      "[2023-08-09 00:04:59,511][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:04:59,511][root][INFO] - => Done in 107.765 us\n",
      "[2023-08-09 00:04:59,511][root][INFO] - \n",
      "[2023-08-09 00:04:59,511][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:04:59,511][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:05:00,272][root][INFO] - => Done in 760.555 ms\n",
      "[2023-08-09 00:05:00,272][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01549\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/2g9csih9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000454-2g9csih9/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:05:37,232][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000538-paplfgpi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/paplfgpi\u001b[0m\n",
      "[2023-08-09 00:05:44,757][root][INFO] - => Done in 7.525 s\n",
      "[2023-08-09 00:05:44,757][root][INFO] - \n",
      "[2023-08-09 00:05:44,757][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:05:44,761][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:05:44,761][root][INFO] - => Done in 3.513 ms\n",
      "[2023-08-09 00:05:44,761][root][INFO] - \n",
      "[2023-08-09 00:05:44,761][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:05:45,346][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:05:45,347][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:05:45,347][root][INFO] - => Done in 585.909 ms\n",
      "[2023-08-09 00:05:45,347][root][INFO] - \n",
      "[2023-08-09 00:05:45,347][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:05:45,347][root][INFO] - => Done in 132.084 us\n",
      "[2023-08-09 00:05:45,347][root][INFO] - \n",
      "[2023-08-09 00:05:45,347][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:05:45,347][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:05:46,109][root][INFO] - => Done in 762.305 ms\n",
      "[2023-08-09 00:05:46,110][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.982 MB of 0.986 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1978.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/paplfgpi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000538-paplfgpi/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:06:23,433][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000624-04vxk8w8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/04vxk8w8\u001b[0m\n",
      "[2023-08-09 00:06:29,151][root][INFO] - => Done in 5.718 s\n",
      "[2023-08-09 00:06:29,151][root][INFO] - \n",
      "[2023-08-09 00:06:29,152][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:06:29,158][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:06:29,158][root][INFO] - => Done in 6.242 ms\n",
      "[2023-08-09 00:06:29,158][root][INFO] - \n",
      "[2023-08-09 00:06:29,158][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:06:29,725][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:06:29,725][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:06:29,725][root][INFO] - => Done in 566.922 ms\n",
      "[2023-08-09 00:06:29,725][root][INFO] - \n",
      "[2023-08-09 00:06:29,725][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:06:29,725][root][INFO] - => Done in 111.818 us\n",
      "[2023-08-09 00:06:29,725][root][INFO] - \n",
      "[2023-08-09 00:06:29,725][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:06:29,725][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:06:30,546][root][INFO] - => Done in 820.869 ms\n",
      "[2023-08-09 00:06:30,546][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.975 MB of 0.987 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 4.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1976.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/04vxk8w8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000624-04vxk8w8/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:07:07,169][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s1a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000708-asu0u44e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s1a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/asu0u44e\u001b[0m\n",
      "[2023-08-09 00:07:12,300][root][INFO] - => Done in 5.130 s\n",
      "[2023-08-09 00:07:12,300][root][INFO] - \n",
      "[2023-08-09 00:07:12,300][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:07:12,303][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:07:12,303][root][INFO] - => Done in 3.282 ms\n",
      "[2023-08-09 00:07:12,303][root][INFO] - \n",
      "[2023-08-09 00:07:12,303][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:07:12,866][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:07:12,866][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:07:12,866][root][INFO] - => Done in 563.253 ms\n",
      "[2023-08-09 00:07:12,867][root][INFO] - \n",
      "[2023-08-09 00:07:12,867][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:07:12,867][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-09 00:07:12,867][root][INFO] - \n",
      "[2023-08-09 00:07:12,867][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:07:12,867][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:07:13,643][root][INFO] - => Done in 775.800 ms\n",
      "[2023-08-09 00:07:13,643][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s1a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/asu0u44e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000708-asu0u44e/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:07:48,009][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000749-pk9jsel6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pk9jsel6\u001b[0m\n",
      "[2023-08-09 00:07:54,192][root][INFO] - => Done in 6.183 s\n",
      "[2023-08-09 00:07:54,193][root][INFO] - \n",
      "[2023-08-09 00:07:54,193][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:07:54,196][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:07:54,196][root][INFO] - => Done in 3.179 ms\n",
      "[2023-08-09 00:07:54,196][root][INFO] - \n",
      "[2023-08-09 00:07:54,196][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:07:54,785][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:07:54,785][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:07:54,785][root][INFO] - => Done in 589.446 ms\n",
      "[2023-08-09 00:07:54,785][root][INFO] - \n",
      "[2023-08-09 00:07:54,786][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:07:54,786][root][INFO] - => Done in 106.096 us\n",
      "[2023-08-09 00:07:54,786][root][INFO] - \n",
      "[2023-08-09 00:07:54,786][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:07:54,786][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:07:55,587][root][INFO] - => Done in 801.170 ms\n",
      "[2023-08-09 00:07:55,587][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.977 MB of 0.989 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01557\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pk9jsel6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000749-pk9jsel6/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:08:29,978][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000831-4k9m8i4k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4k9m8i4k\u001b[0m\n",
      "[2023-08-09 00:08:34,969][root][INFO] - => Done in 4.991 s\n",
      "[2023-08-09 00:08:34,969][root][INFO] - \n",
      "[2023-08-09 00:08:34,969][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:08:34,973][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:08:34,973][root][INFO] - => Done in 3.600 ms\n",
      "[2023-08-09 00:08:34,973][root][INFO] - \n",
      "[2023-08-09 00:08:34,973][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:08:35,557][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:08:35,557][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:08:35,557][root][INFO] - => Done in 584.324 ms\n",
      "[2023-08-09 00:08:35,557][root][INFO] - \n",
      "[2023-08-09 00:08:35,558][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:08:35,558][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-09 00:08:35,558][root][INFO] - \n",
      "[2023-08-09 00:08:35,558][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:08:35,558][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:08:36,321][root][INFO] - => Done in 762.914 ms\n",
      "[2023-08-09 00:08:36,321][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01534\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1978.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4k9m8i4k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000831-4k9m8i4k/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:09:10,590][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000911-26w5vwt9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/26w5vwt9\u001b[0m\n",
      "[2023-08-09 00:09:15,534][root][INFO] - => Done in 4.944 s\n",
      "[2023-08-09 00:09:15,535][root][INFO] - \n",
      "[2023-08-09 00:09:15,535][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:09:15,538][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:09:15,538][root][INFO] - => Done in 3.147 ms\n",
      "[2023-08-09 00:09:15,538][root][INFO] - \n",
      "[2023-08-09 00:09:15,538][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:09:16,112][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:09:16,112][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:09:16,112][root][INFO] - => Done in 573.985 ms\n",
      "[2023-08-09 00:09:16,112][root][INFO] - \n",
      "[2023-08-09 00:09:16,112][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:09:16,112][root][INFO] - => Done in 130.892 us\n",
      "[2023-08-09 00:09:16,112][root][INFO] - \n",
      "[2023-08-09 00:09:16,112][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:09:16,112][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:09:16,871][root][INFO] - => Done in 759.145 ms\n",
      "[2023-08-09 00:09:16,872][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/26w5vwt9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000911-26w5vwt9/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:09:50,889][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_000952-sgmofyp9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/sgmofyp9\u001b[0m\n",
      "[2023-08-09 00:09:55,433][root][INFO] - => Done in 4.544 s\n",
      "[2023-08-09 00:09:55,433][root][INFO] - \n",
      "[2023-08-09 00:09:55,433][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:09:55,436][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:09:55,436][root][INFO] - => Done in 3.501 ms\n",
      "[2023-08-09 00:09:55,436][root][INFO] - \n",
      "[2023-08-09 00:09:55,436][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:09:56,033][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:09:56,033][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:09:56,033][root][INFO] - => Done in 596.750 ms\n",
      "[2023-08-09 00:09:56,033][root][INFO] - \n",
      "[2023-08-09 00:09:56,033][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:09:56,034][root][INFO] - => Done in 186.920 us\n",
      "[2023-08-09 00:09:56,034][root][INFO] - \n",
      "[2023-08-09 00:09:56,034][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:09:56,034][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:09:56,801][root][INFO] - => Done in 767.300 ms\n",
      "[2023-08-09 00:09:56,801][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/sgmofyp9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_000952-sgmofyp9/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:10:31,294][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001032-n2xneidy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/n2xneidy\u001b[0m\n",
      "[2023-08-09 00:10:36,290][root][INFO] - => Done in 4.996 s\n",
      "[2023-08-09 00:10:36,290][root][INFO] - \n",
      "[2023-08-09 00:10:36,290][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:10:36,293][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:10:36,294][root][INFO] - => Done in 3.179 ms\n",
      "[2023-08-09 00:10:36,294][root][INFO] - \n",
      "[2023-08-09 00:10:36,294][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:10:36,867][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:10:36,867][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:10:36,868][root][INFO] - => Done in 573.817 ms\n",
      "[2023-08-09 00:10:36,868][root][INFO] - \n",
      "[2023-08-09 00:10:36,868][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:10:36,868][root][INFO] - => Done in 256.062 us\n",
      "[2023-08-09 00:10:36,868][root][INFO] - \n",
      "[2023-08-09 00:10:36,868][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:10:36,868][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:10:37,624][root][INFO] - => Done in 755.813 ms\n",
      "[2023-08-09 00:10:37,624][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.993 MB of 0.993 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▇▇▇▇██▇▇▇█▇▇▇▇▇█▇▇▇███▇██▇▇█▇▇█▇▇██▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▄▅▆▇▇▇▇████████████████████████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇▇██▇▇▇█▇▇▇▇▇█▇▇▇███▇██▇▇█▇▇█▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▆▆▄▃▂▃▃▂▁▁▂▁▁▂▁▂▂▂▁▁▂▂▂▁▂▂▁▂▂▂▁▃▁▁▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▆▇▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▆▆▄▃▂▃▃▂▁▁▂▁▁▂▁▂▂▂▁▁▂▂▂▁▂▂▁▂▂▂▁▃▁▁▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▆█▇▆▄▇▇▆▅▅▆▅▅▅▅▆▆▅▅▅▆▆▆▅▆▆▅▅▆▅▅▇▅▅▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▄▅▆▇▇▇▇██▇█████▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▆█▇▆▄▇▇▆▅▅▆▅▅▅▅▆▆▅▅▅▆▆▆▅▆▆▅▅▆▅▅▇▅▅▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▄▅▆▇▇▇▇██▇█████▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▆█▇▆▄▇▇▆▅▅▆▅▅▅▅▆▆▅▅▅▆▆▆▅▆▆▅▅▆▅▅▇▅▅▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▄▅▆▇▇▇▇██▇█████▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▆█▇▆▄▇▇▆▅▅▆▅▅▅▅▆▆▅▅▅▆▆▆▅▆▆▅▅▆▅▅▇▅▅▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▄▅▆▇▇▇▇██▇█████▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.24133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.70807\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.42518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.50463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.483\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.2525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.7255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 505.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1451.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.2525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.7255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/n2xneidy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001032-n2xneidy/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:11:13,072][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a1_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001114-nxig5gtq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a1_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/nxig5gtq\u001b[0m\n",
      "[2023-08-09 00:11:17,782][root][INFO] - => Done in 4.710 s\n",
      "[2023-08-09 00:11:17,783][root][INFO] - \n",
      "[2023-08-09 00:11:17,783][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:11:17,786][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:11:17,786][root][INFO] - => Done in 3.419 ms\n",
      "[2023-08-09 00:11:17,786][root][INFO] - \n",
      "[2023-08-09 00:11:17,786][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:11:18,377][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:11:18,377][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:11:18,377][root][INFO] - => Done in 590.402 ms\n",
      "[2023-08-09 00:11:18,377][root][INFO] - \n",
      "[2023-08-09 00:11:18,377][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:11:18,377][root][INFO] - => Done in 101.805 us\n",
      "[2023-08-09 00:11:18,377][root][INFO] - \n",
      "[2023-08-09 00:11:18,377][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:11:18,377][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:11:19,145][root][INFO] - => Done in 767.417 ms\n",
      "[2023-08-09 00:11:19,145][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇▇██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▄▅▆▇▇▇▇████████████████████████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▅▆▇▇▇██▇▇▇█▇▇▇▇▇█▇▇▇███▇██▇▇██▇█▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▆▅▄▃▃▂▂▂▁▁▂▁▁▂▁▂▂▁▁▁▂▂▂▁▂▂▁▂▂▁▁▂▁▁▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▆▇▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▆▅▄▃▃▂▂▂▁▁▂▁▁▂▁▂▂▁▁▁▂▂▂▁▂▂▁▂▂▁▁▂▁▁▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▇▇█▅▇▇▆▆▅▅▆▅▅▆▅▆▆▅▅▅▆▆▆▄▆▆▆▅▆▆▅█▅▅▇▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D █▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▃▄▅▆▆▇▇▇██▇████▇▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▇▇█▅▇▇▆▆▅▅▆▅▅▆▅▆▆▅▅▅▆▆▆▄▆▆▆▅▆▆▅█▅▅▇▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D █▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▃▄▅▆▆▇▇▇██▇████▇▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▇▇█▆▇▇▇▆▆▅▆▆▆▆▅▆▆▆▅▅▆▇▇▅▇▆▆▆▆▆▆█▅▅▇▇▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC █▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▄▅▆▆▇▇▇██▇████▇▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▇▇█▆▇▇▇▆▆▅▆▆▆▆▅▆▆▆▅▅▆▇▇▅▇▆▆▆▆▆▆█▅▅▇▇▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC █▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▄▅▆▆▇▇▇██▇████▇▇███▇▇▇█▇▇██▇██▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.23783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.71545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.41918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.51219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.7275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.4745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.5115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.5115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.2485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.7295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 497.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1459.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 22.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.2485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.7295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1459\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a1_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/nxig5gtq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001114-nxig5gtq/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:11:53,589][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a1_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001154-6j6449oi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a1_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/6j6449oi\u001b[0m\n",
      "[2023-08-09 00:11:58,357][root][INFO] - => Done in 4.768 s\n",
      "[2023-08-09 00:11:58,357][root][INFO] - \n",
      "[2023-08-09 00:11:58,357][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:11:58,360][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:11:58,361][root][INFO] - => Done in 3.437 ms\n",
      "[2023-08-09 00:11:58,361][root][INFO] - \n",
      "[2023-08-09 00:11:58,361][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:11:58,945][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:11:58,945][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:11:58,945][root][INFO] - => Done in 584.603 ms\n",
      "[2023-08-09 00:11:58,945][root][INFO] - \n",
      "[2023-08-09 00:11:58,945][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:11:58,945][root][INFO] - => Done in 109.673 us\n",
      "[2023-08-09 00:11:58,946][root][INFO] - \n",
      "[2023-08-09 00:11:58,946][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:11:58,946][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:11:59,707][root][INFO] - => Done in 761.618 ms\n",
      "[2023-08-09 00:11:59,707][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.995 MB of 0.995 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▅▄▂▃▂▂▁▂▂▁▁▁▁▂▂▂▁▁▂▂▁▁▁▂▁▂▂▁▁▂▁▁▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁████▇▇▆▆▅▇▆▆▆▅▆▅▅▆▅▅▅▄▅▅▄▆▄▆▅▅▅▅▆▅▄▆▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▄▃▃▃▄▅▆▆▆▅▆▆▆▇▆▇▇▆▆▇▇█▇▆█▆█▆▆▇▇▇▆▆▇▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ██▇▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ██▇▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▆█▇▄▃▁▂▂▂▁▂▁▂▁▂▂▂▂▁▂▁▂▂▃▁▂▁▁▂▁▂▂▂▂▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▆▅▄▂▃▁▄▄▃▅▁▄▃▄▅▃▅▅▄▄▅▅▇▄▄▇▅█▃▄▅▅▆▃▃▇▄▃▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▃▇███▇▅▄▄▃▅▄▃▃▃▃▂▂▄▃▂▂▁▂▃▁▃▁▃▃▂▂▃▃▃▂▃▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▂▃▄▆▆▇▇▇▇▇████▇█▇███████▇██▇██▇██▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▆█▇▄▃▁▂▂▂▁▂▁▂▁▂▂▂▂▁▂▁▂▂▃▁▂▁▁▂▁▂▂▂▂▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▆▅▄▂▃▁▄▄▃▅▁▄▃▄▅▃▅▅▄▄▅▅▇▄▄▇▅█▃▄▅▅▆▃▃▇▄▃▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▃▇███▇▅▄▄▃▅▄▃▃▃▃▂▂▄▃▂▂▁▂▃▁▃▁▃▃▂▂▃▃▃▂▃▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▂▃▄▆▆▇▇▇▇▇████▇█▇███████▇██▇██▇██▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▆█▇▄▃▁▂▂▂▁▂▁▂▁▂▂▂▂▁▂▁▂▂▃▁▂▁▁▂▁▂▂▂▂▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▃▂▁▃▁▄▄▃▅▁▄▃▄▅▃▅▅▄▄▅▅▇▄▄▇▅█▃▄▅▅▆▃▃▇▄▃▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▃▇███▇▅▄▄▃▅▄▃▃▃▃▂▂▄▃▂▂▁▂▃▁▃▁▃▃▂▂▃▃▃▂▃▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▆▅▄▄▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▂▄▄▆▆▇▇█▇▇██████▇███████▇█████▇██▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▆█▇▄▃▁▂▂▂▁▂▁▂▁▂▂▂▂▁▂▁▂▂▃▁▂▁▁▂▁▂▂▂▂▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▃▂▁▃▁▄▄▃▅▁▄▃▄▅▃▅▅▄▄▅▅▇▄▄▇▅█▃▄▅▅▆▃▃▇▄▃▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▃▇███▇▅▄▄▃▅▄▃▃▃▃▂▂▄▃▂▂▁▂▃▁▃▁▃▃▂▂▃▃▃▂▃▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▆▅▄▄▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▂▄▄▆▆▇▇█▇▇██████▇███████▇█████▇██▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.2785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.10511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.12429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.66923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.5595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.5595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.7305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 41.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 224.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 253.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1461.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.7305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a1_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/6j6449oi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001154-6j6449oi/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:12:36,022][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a1_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001237-olpzswu6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a1_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/olpzswu6\u001b[0m\n",
      "[2023-08-09 00:12:40,733][root][INFO] - => Done in 4.711 s\n",
      "[2023-08-09 00:12:40,733][root][INFO] - \n",
      "[2023-08-09 00:12:40,734][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:12:40,738][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:12:40,738][root][INFO] - => Done in 4.708 ms\n",
      "[2023-08-09 00:12:40,738][root][INFO] - \n",
      "[2023-08-09 00:12:40,738][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:12:41,351][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:12:41,351][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:12:41,351][root][INFO] - => Done in 612.913 ms\n",
      "[2023-08-09 00:12:41,351][root][INFO] - \n",
      "[2023-08-09 00:12:41,351][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:12:41,352][root][INFO] - => Done in 102.043 us\n",
      "[2023-08-09 00:12:41,352][root][INFO] - \n",
      "[2023-08-09 00:12:41,352][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:12:41,352][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:12:42,119][root][INFO] - => Done in 767.823 ms\n",
      "[2023-08-09 00:12:42,120][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▅▅▅▆▅▅▆▅▅▆▆▆▅▅▆▆▇▆▅▆▆▇▄▇▇▆▆▆▇█▆▆▇▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▆▆▆▇▆▇▇▆▇▇▇▇▇█▆▆▇▇█▇▇▇▇█▆█▇▇██▇█▇▇██▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▅▅▄▅▅▅▅▅▅▅▆▆▆▆▅▆▆▆▇▇▆▇▆▇▅▇▇▆▆▅▇█▇▆▇▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▄▅▄▅▅▅▅▅▅▄▅▅▃▄▅▅▆▅▄▆▄▆▂▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▆▅▆▆▆▆▆▆▆▆▆▇▇▆▆▇▇▇▇▆▇▇▇▆█▇▇▇▇▇█▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▄▅▄▅▅▅▅▅▅▄▅▅▃▄▅▅▆▅▄▆▄▆▂▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▄▄▄▄▄▅▄▄▄▄▄▄▅▅▃▄▅▅▆▅▅▆▅▆▃▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁██▅▆▅▅▆█▅▄▆▅▅▄▄▅▅▃▅▄▃▃▅▂▆▃▅▅▃▃▃▄▅▅▅▅▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▃▃▃▂▃▂▃▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▅▅▆▇▅▅▆▇▇▅▅▆▇▇▅▄▄▅▅▄▃▃▄▅▄▅▅▅▇█▄▂▅▆▄▇▅▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▇▅▄▆▅▆▅▅▄▅▅▅▅▄▄▇▆▅▅▃▄▆▄▅▃█▂▃▄▄▅▃▁▄▄▂▄▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▄▄▄▄▄▅▄▄▄▄▄▄▅▅▃▄▅▅▆▅▅▆▅▆▃▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁██▅▆▅▅▆█▅▄▆▅▅▄▄▅▅▃▅▄▃▃▅▂▆▃▅▅▃▃▃▄▅▅▅▅▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▃▃▃▂▃▂▃▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▅▅▆▇▅▅▆▇▇▅▅▆▇▇▅▄▄▅▅▄▃▃▄▅▄▅▅▅▇█▄▂▅▆▄▇▅▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▇▅▄▆▅▆▅▅▄▅▅▅▅▄▄▇▆▅▅▃▄▆▄▅▃█▂▃▄▄▅▃▁▄▄▂▄▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▄▄▄▄▄▅▄▄▄▄▄▄▅▅▃▄▅▅▆▅▅▆▅▆▃▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▃▇█▅▇▅▅▅▅▄▄▄▂▃▃▂▁▂▂▃▂▁▂▂▁▂▂▁▂▁▂▂▂▂▃▂▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▆▅▄▄▄▅▆█▅▄▆▆▅▄▅▇▆▄▅▆▅▅▆▄▇▄▇▇▅▄▄▅▇▆▆▆▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▃▃▃▂▃▂▃▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▅▅▆▇▅▅▆▇▇▅▅▆▇▇▅▄▄▅▅▄▃▃▄▅▄▅▅▅▇█▄▂▅▆▄▇▅▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▄█▇▅▇▅▅▄▆▅▄▄▃▄▃▂▂▂▂▃▂▂▁▁▁▂▁▂▁▁▂▂▁▁▄▃▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▇▄▃▅▄▆▄▅▄▄▅▅▅▄▄▇▆▅▅▃▄▆▄▅▃█▂▃▄▄▅▃▁▄▄▂▄▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▄▄▄▄▄▅▄▄▄▄▄▄▅▅▃▄▅▅▆▅▅▆▅▆▃▇▆▅▅▅▆█▅▅▇▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▃▇█▅▇▅▅▅▅▄▄▄▂▃▃▂▁▂▂▃▂▁▂▂▁▂▂▁▂▁▂▂▂▂▃▂▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▆▅▄▄▄▅▆█▅▄▆▆▅▄▅▇▆▄▅▆▅▅▆▄▇▄▇▇▅▄▄▅▇▆▆▆▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▃▃▃▂▃▂▃▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▅▅▆▇▅▅▆▇▇▅▅▆▇▇▅▄▄▅▅▄▃▃▄▅▄▅▅▅▇█▄▂▅▆▄▇▅▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▄█▇▅▇▅▅▄▆▅▄▄▃▄▃▂▂▂▂▃▂▂▁▁▁▂▁▂▁▁▂▂▁▁▄▃▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▇▄▃▅▄▆▄▅▄▄▅▅▅▄▄▇▆▅▅▃▄▆▄▅▃█▂▃▄▄▅▃▁▄▄▂▄▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.38383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.34482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.55907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 2.37967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.5655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.83\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.66075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.83\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.5245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.1315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1049.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 396.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 254.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 263.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.5245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.1885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 19\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a1_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/olpzswu6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001237-olpzswu6/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:13:19,146][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001320-c6ywimyb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/c6ywimyb\u001b[0m\n",
      "[2023-08-09 00:13:24,298][root][INFO] - => Done in 5.152 s\n",
      "[2023-08-09 00:13:24,298][root][INFO] - \n",
      "[2023-08-09 00:13:24,298][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:13:24,301][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:13:24,301][root][INFO] - => Done in 3.105 ms\n",
      "[2023-08-09 00:13:24,301][root][INFO] - \n",
      "[2023-08-09 00:13:24,301][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:13:24,869][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:13:24,869][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:13:24,869][root][INFO] - => Done in 567.735 ms\n",
      "[2023-08-09 00:13:24,869][root][INFO] - \n",
      "[2023-08-09 00:13:24,869][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:13:24,869][root][INFO] - => Done in 112.295 us\n",
      "[2023-08-09 00:13:24,869][root][INFO] - \n",
      "[2023-08-09 00:13:24,869][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:13:24,870][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:13:25,630][root][INFO] - => Done in 760.559 ms\n",
      "[2023-08-09 00:13:25,630][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇█▇▇▇▆▇▇▇▆▆▆▇▆▆▆▇▆▆▆▆▇▆▆█▆█▇▆▇▇▆▅▇▇▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆▇▆▇▆▅▆▇▇▅▆▆▇▆▅▆▇▆▆▆▅▇▆▅█▆█▇▆▆▆▆▅▆▇▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▃▅▅▆▅▆▇▆▅▅▇▆▆▅▆█▆▅▆▆▇▇▅▅▇▄▇▄▅▇▆▇▆▇▇▅▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▆▄▄▃▃▃▂▂▃▂▂▂▂▃▂▁▂▂▂▂▂▂▃▂▂▃▂▃▂▂▂▂▁▁▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇██▇██████████▇█████████████████▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▆▄▄▃▃▃▂▂▃▂▂▂▂▃▂▁▂▂▂▂▂▂▃▂▂▃▂▃▂▂▂▂▁▁▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▂▂▂▃▁▁▄▂▂▁▁▂▁▂▂▂▂▁▃▂▃▂▃▂▃▂▁▂▂▂█▂▂▂▃▂▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▆▅▆▄▅▄▅▆▅▃▃▆▄▄▂▄█▅▄▄▄▆▆▄▃▆▁▅▂▄▆▃▆▅▅▅▃▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁█▅▇▄▅▅▂▄▆▆▃▄▅▆▄▁▄▅▄▃▃▂▅▄▂█▃▇▅▃▃▃▃▂▃▆▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▃▄▅▆▆▆▇▇▆▆▇▇▇▆▇▇▇▆▇▇▇▇▆▇▇▆▇▅▆▇▇▇▇█▇▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▂▂▂▃▁▁▄▂▂▁▁▂▁▂▂▂▂▁▃▂▃▂▃▂▃▂▁▂▂▂█▂▂▂▃▂▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▆▅▆▄▅▄▅▆▅▃▃▆▄▄▂▄█▅▄▄▄▆▆▄▃▆▁▅▂▄▆▃▆▅▅▅▃▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁█▅▇▄▅▅▂▄▆▆▃▄▅▆▄▁▄▅▄▃▃▂▅▄▂█▃▇▅▃▃▃▃▂▃▆▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▃▄▅▆▆▆▇▇▆▆▇▇▇▆▇▇▇▆▇▇▇▇▆▇▇▆▇▅▆▇▇▇▇█▇▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▂▂▂▃▁▁▄▂▂▁▁▂▁▂▂▂▂▁▃▂▃▂▃▂▃▂▁▂▂▂█▂▂▂▃▂▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▄▄▆▄▅▄▅▆▅▃▃▆▄▄▂▄█▅▄▄▄▆▆▄▃▆▁▅▂▄▆▃▆▅▅▅▃▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁█▅▇▄▅▅▂▄▆▆▃▄▅▆▄▁▄▅▄▃▃▂▅▄▂█▃▇▅▃▃▃▃▂▃▆▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ██▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▃▅▅▆▆▆▇▇▆▆▇▇▇▆▇▇▇▆▇▇▇▇▆▇▇▆▇▆▇▇▇▇▇█▇▆█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▂▂▂▃▁▁▄▂▂▁▁▂▁▂▂▂▂▁▃▂▃▂▃▂▃▂▁▂▂▂█▂▂▂▃▂▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▄▄▆▄▅▄▅▆▅▃▃▆▄▄▂▄█▅▄▄▄▆▆▄▃▆▁▅▂▄▆▃▆▅▅▅▃▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁█▅▇▄▅▅▂▄▆▆▃▄▅▆▄▁▄▅▄▃▃▂▅▄▂█▃▇▅▃▃▃▃▂▃▆▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ██▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▃▅▅▆▆▆▇▇▆▆▇▇▇▆▇▇▇▆▇▇▇▇▆▇▇▆▇▆▇▇▇▇▇█▇▆█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.24369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.01585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.6758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.5715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.3015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.6245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 34.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 91.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 603.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1249.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.3015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/c6ywimyb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001320-c6ywimyb/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:14:00,050][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001401-e8qotgm0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/e8qotgm0\u001b[0m\n",
      "[2023-08-09 00:14:05,318][root][INFO] - => Done in 5.268 s\n",
      "[2023-08-09 00:14:05,318][root][INFO] - \n",
      "[2023-08-09 00:14:05,318][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:14:05,322][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:14:05,322][root][INFO] - => Done in 3.648 ms\n",
      "[2023-08-09 00:14:05,322][root][INFO] - \n",
      "[2023-08-09 00:14:05,322][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:14:05,894][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:14:05,894][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:14:05,894][root][INFO] - => Done in 572.621 ms\n",
      "[2023-08-09 00:14:05,895][root][INFO] - \n",
      "[2023-08-09 00:14:05,895][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:14:05,895][root][INFO] - => Done in 128.984 us\n",
      "[2023-08-09 00:14:05,895][root][INFO] - \n",
      "[2023-08-09 00:14:05,895][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:14:05,895][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:14:06,654][root][INFO] - => Done in 759.424 ms\n",
      "[2023-08-09 00:14:06,655][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆▆█▅▇▇▆▆▄▆▅▅▅▅▆▆▆▆▆▆▆▆▅▆▇▆▅▆▆▆█▅▅▅▆▄▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▆▆█▅▇▇▆▆▄▆▅▅▅▅▆▆▆▆▆▆▆▆▅▆▇▆▅▆▆▆▇▅▅▅▆▄▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▇▄▃▄▂▃▃▃▂▂▂▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▇▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▇▄▃▄▂▃▃▃▂▂▂▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▇▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC █▇▄▃▄▂▃▃▃▂▂▂▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▇▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC █▇▄▃▄▂▃▃▃▂▂▂▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▇▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.83399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.23055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.28133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.8775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.05975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.1215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 243.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1736.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.1215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/e8qotgm0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001401-e8qotgm0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:14:41,981][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a1_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001443-ux579ro5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a1_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ux579ro5\u001b[0m\n",
      "[2023-08-09 00:14:47,383][root][INFO] - => Done in 5.402 s\n",
      "[2023-08-09 00:14:47,383][root][INFO] - \n",
      "[2023-08-09 00:14:47,383][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:14:47,386][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:14:47,386][root][INFO] - => Done in 2.920 ms\n",
      "[2023-08-09 00:14:47,386][root][INFO] - \n",
      "[2023-08-09 00:14:47,386][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:14:47,935][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:14:47,935][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:14:47,935][root][INFO] - => Done in 548.969 ms\n",
      "[2023-08-09 00:14:47,935][root][INFO] - \n",
      "[2023-08-09 00:14:47,935][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:14:47,935][root][INFO] - => Done in 118.732 us\n",
      "[2023-08-09 00:14:47,936][root][INFO] - \n",
      "[2023-08-09 00:14:47,936][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:14:47,936][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:14:48,699][root][INFO] - => Done in 763.422 ms\n",
      "[2023-08-09 00:14:48,699][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▄█▄▄▅▂▄▄▃▃▂▃▂▂▂▂▃▃▃▃▃▃▃▃▂▃▄▃▂▃▃▃▄▂▂▂▃▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▄▅▇▇▇▇▇████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁█▇▇█▇█▇▇▇▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▆▇▇▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▆▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▇█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▃▅▆▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▇█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▃▅▆▇▇▇▇▇███████▇█▇██▇▇███▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▆█▆▄▅▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▅▇▇▇▇▇▇█████████▇██▇████▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▆█▆▄▅▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▅▇▇▇▇▇▇█████████▇██▇████▇█████▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.12033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.22878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.29946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.8765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.05875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.1215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 243.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1736.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.1215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a1_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ux579ro5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001443-ux579ro5/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:15:23,804][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a1_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001525-b25ufy5x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a1_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/b25ufy5x\u001b[0m\n",
      "[2023-08-09 00:15:28,400][root][INFO] - => Done in 4.596 s\n",
      "[2023-08-09 00:15:28,400][root][INFO] - \n",
      "[2023-08-09 00:15:28,400][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:15:28,404][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:15:28,404][root][INFO] - => Done in 4.325 ms\n",
      "[2023-08-09 00:15:28,404][root][INFO] - \n",
      "[2023-08-09 00:15:28,404][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:15:29,004][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:15:29,005][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:15:29,005][root][INFO] - => Done in 600.286 ms\n",
      "[2023-08-09 00:15:29,005][root][INFO] - \n",
      "[2023-08-09 00:15:29,005][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:15:29,005][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-09 00:15:29,005][root][INFO] - \n",
      "[2023-08-09 00:15:29,005][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:15:29,005][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:15:29,777][root][INFO] - => Done in 772.317 ms\n",
      "[2023-08-09 00:15:29,778][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 1.001 MB of 1.001 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇█▇▅▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▆▆▆▇█▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▆▇▇▇▇█▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▄▄▃▃▂▃▃▅▄▃▅▇▅▆▆▆▇▆▆▆▆▆▆▄▇▆▇▆▆▆▇▆▇▆▅▄▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▆████▆▄▄▃▃▃▂▂▂▁▂▂▁▂▁▂▂▂▁▂▁▂▁▁▁▁▁▂▂▁▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▇▇▇█▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▆████▆▄▄▃▃▃▂▂▂▁▂▂▁▂▁▂▂▂▁▂▁▂▁▁▁▁▁▂▂▁▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▅█▇▇█▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▄▆▅▄▂▁▂▃▃▁▂▅▂▃▄▄▄▄▄▃▃▄▃▁▅▃▄▃▃▃▃▄▅▂▃▁▂▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▅▃▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▅▅▅▆█▇█▇▇█▇▆▇▆▇▇▆▇▆▇▇▆▆█▆▇▆▆▇▆▅▇▆▆▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▃▂▁▂▁▂▄▄▅▅▅▆▇▆▇▆▆▇▆▇▇▆▆▇▆▇▆█▇▇▇█▆▇▇▆▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▅█▇▇█▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▄▆▅▄▂▁▂▃▃▁▂▅▂▃▄▄▄▄▄▃▃▄▃▁▅▃▄▃▃▃▃▄▅▂▃▁▂▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▅▃▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▅▅▅▆█▇█▇▇█▇▆▇▆▇▇▆▇▆▇▇▆▆█▆▇▆▆▇▆▅▇▆▆▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▃▂▁▂▁▂▄▄▅▅▅▆▇▆▇▆▆▇▆▇▇▆▆▇▆▇▆█▇▇▇█▆▇▇▆▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▅█▇▇█▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▅▅▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▄▂▄▄▃▂▁▂▄▃▂▃█▄▄▆▆▇▆▆▅▅▇▄▂█▅▆▆▅▄▅▇█▄▄▂▃▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▅▃▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▅▅▅▆█▇█▇▇█▇▆▇▆▇▇▆▇▆▇▇▆▆█▆▇▆▆▇▆▅▇▆▆▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▇█▇██▆▃▃▃▂▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▂▁▁▁▁▂▅▄▆▅▅▇▇▆▇▆▇▇▇▇▇▇▇▇▆▇▇█▇▇▇█▆▇▇▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▅█▇▇█▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▅▅▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▃▂▄▃▃▂▁▂▄▃▂▃▇▄▄▆▆▇▆▆▅▅▇▄▂▇▅▆▆▅▄▅▇█▄▄▂▃▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▅▃▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▅▅▅▆█▇█▇▇█▇▆▇▆▇▇▆▇▆▇▇▆▆█▆▇▆▆▇▆▅▇▆▆▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▇█▇██▆▃▃▃▂▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▂▁▁▁▁▂▅▄▆▅▅▇▇▆▇▆▇▇▇▇▇▇▇▇▆▇▇█▇▇▇█▆▇▇▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.52667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.57873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.85424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.9595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.25725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.3355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 53.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 276.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 671.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 976.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.3355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.4865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 671\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a1_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/b25ufy5x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001525-b25ufy5x/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:16:03,443][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a1_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001604-3garmfje\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a1_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/3garmfje\u001b[0m\n",
      "[2023-08-09 00:16:09,090][root][INFO] - => Done in 5.646 s\n",
      "[2023-08-09 00:16:09,090][root][INFO] - \n",
      "[2023-08-09 00:16:09,090][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:16:09,094][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:16:09,094][root][INFO] - => Done in 3.485 ms\n",
      "[2023-08-09 00:16:09,094][root][INFO] - \n",
      "[2023-08-09 00:16:09,094][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:16:09,687][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:16:09,687][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:16:09,687][root][INFO] - => Done in 593.118 ms\n",
      "[2023-08-09 00:16:09,687][root][INFO] - \n",
      "[2023-08-09 00:16:09,687][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:16:09,687][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-09 00:16:09,687][root][INFO] - \n",
      "[2023-08-09 00:16:09,687][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:16:09,687][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:16:10,462][root][INFO] - => Done in 774.424 ms\n",
      "[2023-08-09 00:16:10,462][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▇▆▇████▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▇▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▅▆▇▆▇█▇██▇██▇██▇▇▇▇▇▇▆▇▆▇▆▆▆▆▆▆▆▆▆▇▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▆▅▇▇▇██▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▆▇▇▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▆▇▆▇█▇▇▇▇▆▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▆██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▅▆▆▆▇█▇█▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▇▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▆▇▆▇█▇▇▇▇▆▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▆██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▅▅▆▆▆▇█▇██▇▇▇▇█▇▇▆▇▆▇▇▇▇▆▇▆▆▆▆▆▆▆▅▅▆▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▄▅▃▃▃▂▂▆▂▂▂▂▂▂▂▂▅▃▁▅▃▃▅▄▆▂▇▆▅▄▇▅▅▅█▅▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▄▃▃▂▂▂▂▁▁▂▁▂▁▂▂▁▂▂▂▂▂▃▂▃▂▃▂▃▃▃▃▃▂▂▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▄▅▅▇▆▇▇▇█▇▇▇█▇▇█▆▇▇▇▆▆▅▆▆▇▅▅▆▆▅▅▅▅▅▇▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▆▄▅▃▄▃▂▁▁▃▃▂▂▃▂▁▂▃▃▂▄▃▅▅▃▄▄▅▄▅▅▅▆▇▅▃▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▂▂▂▂▂▂▁▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▅▅▆▆▆▇█▇██▇▇▇▇█▇▇▆▇▆▇▇▇▇▆▇▆▆▆▆▆▆▆▅▅▆▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▄▅▃▃▃▂▂▆▂▂▂▂▂▂▂▂▅▃▁▅▃▃▅▄▆▂▇▆▅▄▇▅▅▅█▅▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▄▃▃▂▂▂▂▁▁▂▁▂▁▂▂▁▂▂▂▂▂▃▂▃▂▃▂▃▃▃▃▃▂▂▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▄▅▅▇▆▇▇▇█▇▇▇█▇▇█▆▇▇▇▆▆▅▆▆▇▅▅▆▆▅▅▅▅▅▇▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▆▄▅▃▄▃▂▁▁▃▃▂▂▃▂▁▂▃▃▂▄▃▅▅▃▄▄▅▄▅▅▅▆▇▅▃▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▂▂▂▂▂▂▁▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▅▅▆▆▆▇█▇██▇▇▇▇█▇▇▆▇▆▇▇▇▇▆▇▆▆▆▆▆▆▆▅▅▆▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▄▄▅▄▅▃▃▂▂▁▃▂▂▂▂▂▂▄▄▃▄▃▃▅▅▅▄▅▅▅▅█▅▅▆▇▆▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▅▅▃▃▄▃▄█▅▃▄▄▄▄▄▅▆▄▃▆▄▄▅▄▆▂▇▆▅▃▄▅▅▅▇▅▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▄▃▃▂▂▂▂▁▁▂▁▂▁▂▂▁▂▂▂▂▂▃▂▃▂▃▂▃▃▃▃▃▂▂▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▄▅▅▇▆▇▇▇█▇▇▇█▇▇█▆▇▇▇▆▆▅▆▆▇▅▅▆▆▅▅▅▅▅▇▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▅▄▃▄▄▃▃▂▂▁▂▃▂▂▃▃▂▃▄▃▃▃▃▄▄▃▅▄▅▆▅▅▇▆█▇▆▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▆▄▅▁▄▃▂▁▁▃▂▃▂▃▂▂▂▂▃▁▃▃▄▄▃▂▄▄▃▅▅▃▄▅▃▁▄▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▂▂▂▂▂▂▁▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▅▅▆▆▆▇█▇██▇▇▇▇█▇▇▆▇▆▇▇▇▇▆▇▆▆▆▆▆▆▆▅▅▆▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▄▄▅▄▅▃▃▂▂▁▃▂▂▂▂▂▂▄▄▃▄▃▃▅▅▅▄▅▅▅▅█▅▅▆▇▆▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▅▅▃▃▄▃▄█▅▃▄▄▄▄▄▅▆▄▃▆▄▄▅▄▆▂▇▆▅▃▄▅▅▅▇▅▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▄▃▃▂▂▂▂▁▁▂▁▂▁▂▂▁▂▂▂▂▂▃▂▃▂▃▂▃▃▃▃▃▂▂▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▄▅▅▇▆▇▇▇█▇▇▇█▇▇█▆▇▇▇▆▆▅▆▆▇▅▅▆▆▅▅▅▅▅▇▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▅▅▃▄▄▃▃▂▂▁▂▃▂▂▃▃▂▃▅▃▃▃▃▄▄▃▅▄▅▆▅▅▇▆█▇▆▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▆▄▅▁▄▃▂▁▁▃▂▃▂▃▂▂▂▂▃▁▃▃▄▄▃▂▄▄▃▅▅▃▄▅▃▁▄▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▂▂▂▂▂▂▁▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.73067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.15352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.18091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 2.09308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.0505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.1665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.3095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 333.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 548.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 81.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 317.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 619.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 82.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.1665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.2075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.2495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a1_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/3garmfje\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001604-3garmfje/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:16:47,085][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001648-btva36b3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/btva36b3\u001b[0m\n",
      "[2023-08-09 00:16:51,678][root][INFO] - => Done in 4.593 s\n",
      "[2023-08-09 00:16:51,679][root][INFO] - \n",
      "[2023-08-09 00:16:51,679][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:16:51,683][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:16:51,683][root][INFO] - => Done in 3.952 ms\n",
      "[2023-08-09 00:16:51,683][root][INFO] - \n",
      "[2023-08-09 00:16:51,683][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:16:52,282][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:16:52,282][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:16:52,283][root][INFO] - => Done in 599.426 ms\n",
      "[2023-08-09 00:16:52,283][root][INFO] - \n",
      "[2023-08-09 00:16:52,283][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:16:52,283][root][INFO] - => Done in 133.991 us\n",
      "[2023-08-09 00:16:52,283][root][INFO] - \n",
      "[2023-08-09 00:16:52,283][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:16:52,283][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:16:53,050][root][INFO] - => Done in 767.217 ms\n",
      "[2023-08-09 00:16:53,050][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▇▇▇██▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▆▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▅▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▆▇▇▇▇▇▆▇▆▇▇▇██▇▆█▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▂▂▅▄▂▄▅▅▃▅▃▂▂▅▄▂▄▅▆▇▄▄▄▃▄▆▃▆▅▄▅▂▁▆█▂▅▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▅▄▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▅▄▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▅▆▄▂▄▅▄▂▄▂▁▂▅▃▂▄▄▅▆▃▄▃▂▃▅▃▅▄▃▄▂▁▅▆▂▅▃▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▅▄▆▇▆▆▆▇▆▇▇▇▆▆▇▆▆▅▅▆▆▆▇▆▅▇▅▆▇▆▇█▆▄▇▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▅▆▆▇▆▆▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▆▆▇█▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▅▆▄▂▄▅▄▂▄▂▁▂▅▃▂▄▄▅▆▃▄▃▂▃▅▃▅▄▃▄▂▁▅▆▂▅▃▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▅▄▆▇▆▆▆▇▆▇▇▇▆▆▇▆▆▅▅▆▆▆▇▆▅▇▅▆▇▆▇█▆▄▇▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▅▆▆▇▆▆▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▆▆▇█▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▇▆▇▅▂▅▆▅▃▅▃▂▂▆▃▂▄▅▆█▃▄▄▃▄▆▃▆▅▄▅▃▁▆█▂▆▃▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▅▄▆▇▆▆▆▇▆▇▇▇▆▆▇▆▆▅▅▆▆▆▇▆▅▇▅▆▇▆▇█▆▄▇▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▄█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▃▅▆▆▇▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇█▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▇▆▇▅▂▅▆▅▃▅▃▂▂▆▃▂▄▅▆█▃▄▄▃▄▆▃▆▅▄▅▃▁▆█▂▆▃▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▅▄▆▇▆▆▆▇▆▇▇▇▆▆▇▆▆▅▅▆▆▆▇▆▅▇▅▆▇▆▇█▆▄▇▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▄█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▃▅▆▆▇▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇█▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.42367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.42413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.95545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.92332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.3995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.8545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.20825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.8545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.2695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.5705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 12.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 284.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 539.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1141.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 4.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.2695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1140\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/btva36b3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001648-btva36b3/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:17:30,434][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001731-1f7xv1nd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/1f7xv1nd\u001b[0m\n",
      "[2023-08-09 00:17:37,387][root][INFO] - => Done in 6.952 s\n",
      "[2023-08-09 00:17:37,387][root][INFO] - \n",
      "[2023-08-09 00:17:37,387][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:17:37,392][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:17:37,392][root][INFO] - => Done in 5.221 ms\n",
      "[2023-08-09 00:17:37,392][root][INFO] - \n",
      "[2023-08-09 00:17:37,392][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:17:37,956][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:17:37,956][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:17:37,956][root][INFO] - => Done in 563.457 ms\n",
      "[2023-08-09 00:17:37,956][root][INFO] - \n",
      "[2023-08-09 00:17:37,956][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:17:37,956][root][INFO] - => Done in 216.961 us\n",
      "[2023-08-09 00:17:37,956][root][INFO] - \n",
      "[2023-08-09 00:17:37,956][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:17:37,957][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:17:38,768][root][INFO] - => Done in 811.064 ms\n",
      "[2023-08-09 00:17:38,768][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/1f7xv1nd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001731-1f7xv1nd/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:18:14,579][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001815-7m2a5a23\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/7m2a5a23\u001b[0m\n",
      "[2023-08-09 00:18:19,562][root][INFO] - => Done in 4.984 s\n",
      "[2023-08-09 00:18:19,563][root][INFO] - \n",
      "[2023-08-09 00:18:19,563][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:18:19,565][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:18:19,566][root][INFO] - => Done in 2.927 ms\n",
      "[2023-08-09 00:18:19,566][root][INFO] - \n",
      "[2023-08-09 00:18:19,566][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:18:20,144][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:18:20,144][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:18:20,145][root][INFO] - => Done in 578.922 ms\n",
      "[2023-08-09 00:18:20,145][root][INFO] - \n",
      "[2023-08-09 00:18:20,145][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:18:20,145][root][INFO] - => Done in 107.050 us\n",
      "[2023-08-09 00:18:20,145][root][INFO] - \n",
      "[2023-08-09 00:18:20,145][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:18:20,145][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:18:20,905][root][INFO] - => Done in 760.349 ms\n",
      "[2023-08-09 00:18:20,905][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01549\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/7m2a5a23\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001815-7m2a5a23/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:18:56,208][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001857-4nxj3sxv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4nxj3sxv\u001b[0m\n",
      "[2023-08-09 00:19:01,351][root][INFO] - => Done in 5.143 s\n",
      "[2023-08-09 00:19:01,352][root][INFO] - \n",
      "[2023-08-09 00:19:01,352][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:19:01,355][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:19:01,355][root][INFO] - => Done in 3.064 ms\n",
      "[2023-08-09 00:19:01,355][root][INFO] - \n",
      "[2023-08-09 00:19:01,355][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:19:01,913][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:19:01,913][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:19:01,913][root][INFO] - => Done in 558.420 ms\n",
      "[2023-08-09 00:19:01,913][root][INFO] - \n",
      "[2023-08-09 00:19:01,914][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:19:01,914][root][INFO] - => Done in 261.784 us\n",
      "[2023-08-09 00:19:01,914][root][INFO] - \n",
      "[2023-08-09 00:19:01,914][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:19:01,914][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:19:02,674][root][INFO] - => Done in 759.766 ms\n",
      "[2023-08-09 00:19:02,674][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▆▇█▇▇▇▇▆▆▇▇▆▆▇▇▅▇▇▆▆▆▆▆▅▆▅▇▆▇▆▇▆▆▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆▇█▇▇▇▇▆▇▇▇▇▆▇▇▆▇▇▆▇▇▆▇▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▇▇███▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▂▁▂▁▁▁▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▂▁▂▁▁▁▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▇█▆▆▆▄▄▃▄▂▃▄▃▃▂▄▄▁▃▃▂▃▂▂▃▁▃▁▃▂▃▃▃▂▂▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇████▇█▇█▇█▇███▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▇█▆▆▆▄▄▃▄▂▃▄▃▃▂▄▄▁▃▃▂▃▂▂▃▁▃▁▃▂▃▃▃▂▂▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇████▇█▇█▇█▇███▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC █▃▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▇█▆▆▆▄▄▃▄▂▃▄▃▃▂▄▄▁▃▃▂▃▂▂▃▁▃▁▃▂▃▃▃▂▂▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC █▃▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▇█▆▆▆▄▄▃▄▂▃▄▃▃▂▄▄▁▃▃▂▃▂▂▃▁▃▁▃▂▃▃▃▂▂▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.27069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 226.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1743.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4nxj3sxv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001857-4nxj3sxv/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:19:38,226][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_001939-9pa9nxnj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/9pa9nxnj\u001b[0m\n",
      "[2023-08-09 00:19:42,765][root][INFO] - => Done in 4.539 s\n",
      "[2023-08-09 00:19:42,765][root][INFO] - \n",
      "[2023-08-09 00:19:42,766][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:19:42,770][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:19:42,770][root][INFO] - => Done in 4.652 ms\n",
      "[2023-08-09 00:19:42,770][root][INFO] - \n",
      "[2023-08-09 00:19:42,771][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:19:43,382][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:19:43,382][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:19:43,382][root][INFO] - => Done in 611.583 ms\n",
      "[2023-08-09 00:19:43,382][root][INFO] - \n",
      "[2023-08-09 00:19:43,382][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:19:43,382][root][INFO] - => Done in 100.136 us\n",
      "[2023-08-09 00:19:43,382][root][INFO] - \n",
      "[2023-08-09 00:19:43,382][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:19:43,383][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:19:44,164][root][INFO] - => Done in 781.481 ms\n",
      "[2023-08-09 00:19:44,164][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01549\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/9pa9nxnj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_001939-9pa9nxnj/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:20:20,295][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002021-em1mu1oq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/em1mu1oq\u001b[0m\n",
      "[2023-08-09 00:20:26,255][root][INFO] - => Done in 5.959 s\n",
      "[2023-08-09 00:20:26,255][root][INFO] - \n",
      "[2023-08-09 00:20:26,255][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:20:26,262][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:20:26,263][root][INFO] - => Done in 7.478 ms\n",
      "[2023-08-09 00:20:26,263][root][INFO] - \n",
      "[2023-08-09 00:20:26,263][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:20:26,830][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:20:26,831][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:20:26,831][root][INFO] - => Done in 567.658 ms\n",
      "[2023-08-09 00:20:26,831][root][INFO] - \n",
      "[2023-08-09 00:20:26,831][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:20:26,831][root][INFO] - => Done in 113.726 us\n",
      "[2023-08-09 00:20:26,831][root][INFO] - \n",
      "[2023-08-09 00:20:26,831][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:20:26,831][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:20:27,590][root][INFO] - => Done in 759.146 ms\n",
      "[2023-08-09 00:20:27,590][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▆▇█▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇▇▇▆▇▇▇▇▆▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▂▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▅█▄▅▅▂▃▃▃▃▂▃▃▃▂▃▃▂▃▂▂▃▂▂▃▂▃▂▂▂▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▆▇▇▇▇▇▇█▇▇▇█▇▇█▇██▇██▇███████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▃▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▅█▄▅▅▂▃▃▃▃▂▃▃▃▂▃▃▂▃▂▂▃▂▂▃▂▃▂▂▂▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▆▇▇▇▇▇▇█▇▇▇█▇▇█▇██▇██▇███████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC █▃▁▁▁▄▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▄▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▅█▄▅▅▂▃▃▃▃▂▃▃▃▂▃▃▂▃▂▂▃▂▂▃▂▃▂▂▂▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇▇▇█▇▇██▇█████▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC █▃▁▁▁▄▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▄▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▅█▄▅▅▂▃▃▃▃▂▃▃▃▂▃▃▂▃▂▂▃▂▂▃▂▃▂▂▂▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇▇▇█▇▇██▇█████▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.09833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.2673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 211.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1756.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 13.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/em1mu1oq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002021-em1mu1oq/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:21:03,354][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002104-l3zzoz81\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/l3zzoz81\u001b[0m\n",
      "[2023-08-09 00:21:08,545][root][INFO] - => Done in 5.191 s\n",
      "[2023-08-09 00:21:08,546][root][INFO] - \n",
      "[2023-08-09 00:21:08,546][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:21:08,549][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:21:08,549][root][INFO] - => Done in 3.230 ms\n",
      "[2023-08-09 00:21:08,549][root][INFO] - \n",
      "[2023-08-09 00:21:08,549][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:21:09,122][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:21:09,122][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:21:09,123][root][INFO] - => Done in 573.468 ms\n",
      "[2023-08-09 00:21:09,123][root][INFO] - \n",
      "[2023-08-09 00:21:09,123][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:21:09,123][root][INFO] - => Done in 169.277 us\n",
      "[2023-08-09 00:21:09,123][root][INFO] - \n",
      "[2023-08-09 00:21:09,123][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:21:09,123][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:21:09,885][root][INFO] - => Done in 762.475 ms\n",
      "[2023-08-09 00:21:09,886][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▅▆▅▇▇▇▇▆▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇█▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▆▇▇▇██████████▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▅▆▆▇▇▇▇▆▇▇▇▇▇▇█▇█▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇█▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▅▄▂▂▁▂▂▂▂▁▂▂▂▂▂▂▃▃▃▂▃▃▂▂▃▃▂▂▂▃▂▃▃▂▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▇▇█████████████████▇▇▇█▇█████▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▅▄▂▂▁▂▂▂▂▁▂▂▂▂▂▂▃▃▃▂▃▃▂▂▃▃▂▂▂▃▂▃▃▂▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▂▇▄▁▃▁▃▃▄▄▂▄▄▄▄▅▆▆▆▆▆█▆▅▅▅▇▅▅▄▆▅█▆▄▆▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▅▇▇▇▇████▇▇█▇▇▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▆▆▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▂▇▄▁▃▁▃▃▄▄▂▄▄▄▄▅▆▆▆▆▆█▆▅▅▅▇▅▅▄▆▅█▆▄▆▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▅▇▇▇▇████▇▇█▇▇▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▆▆▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▇▄▁▃▁▃▃▄▄▂▄▄▄▄▅▆▆▆▆▆█▆▅▅▅▇▅▅▄▆▅█▆▄▆▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▅▇▇▇▇████▇▇█▇▇▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▆▆▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▇▄▁▃▁▃▃▄▄▂▄▄▄▄▅▆▆▆▆▆█▆▅▅▅▇▅▅▄▆▅█▆▄▆▇▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▅▇▇▇▇████▇▇█▇▇▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▆▆▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.61622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.21327\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.49055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.4645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.9285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.4645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 376.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1440.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 159.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1440\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/l3zzoz81\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002104-l3zzoz81/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:21:44,574][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a1_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002145-lqqboywl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a1_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/lqqboywl\u001b[0m\n",
      "[2023-08-09 00:21:49,378][root][INFO] - => Done in 4.804 s\n",
      "[2023-08-09 00:21:49,378][root][INFO] - \n",
      "[2023-08-09 00:21:49,378][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:21:49,381][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:21:49,381][root][INFO] - => Done in 3.214 ms\n",
      "[2023-08-09 00:21:49,381][root][INFO] - \n",
      "[2023-08-09 00:21:49,381][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:21:49,986][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:21:49,986][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:21:49,986][root][INFO] - => Done in 605.057 ms\n",
      "[2023-08-09 00:21:49,986][root][INFO] - \n",
      "[2023-08-09 00:21:49,986][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:21:49,986][root][INFO] - => Done in 105.143 us\n",
      "[2023-08-09 00:21:49,986][root][INFO] - \n",
      "[2023-08-09 00:21:49,987][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:21:49,987][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:21:50,759][root][INFO] - => Done in 772.733 ms\n",
      "[2023-08-09 00:21:50,760][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆▄▅▄▅▆▆▆▆▇▆▆▇▇▇█▇▇▇█▇▇▇▆█▆▇▇▆▇█▇▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▆▇▇▇▇▇█████████████████▇█▇██▇█▇█▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▆▅▆▅▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇█▇█▇▇▇█▇▇▇█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▅▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▆▆▇▇▇█▇██████████████▇█▇██▇███▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▅▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▇▅▂▃▂▁▁▁▁▁▂▂▁▂▂▂▁▂▃▁▂▂▂▂▂▂▁▂▂▂▂▂▃▂▂▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▃█▅▁▄▁▂▄▃▃▃▄▃▃▄▄▄▆▅▅▅▆▅▅▅▅▇▅▅▅▅▅█▅▄▆▆▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D █▆▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▅▇▆▇█▇█████████▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▂▂▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▇▅▁▃▁▁▁▁▁▁▁▂▁▁▂▂▁▃▃▁▃▂▁▂▂▂▁▂▂▁▂▂▃▁▁▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▃█▅▁▄▁▂▄▃▃▃▄▃▃▄▄▄▆▅▅▅▆▅▅▅▅▇▅▅▅▅▅█▅▄▆▆▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D █▆▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▅▇▆▇█▇█████████▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▂▂▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▇▅▂▃▂▁▁▁▁▁▂▂▁▂▂▂▁▂▃▁▂▂▂▂▂▂▁▂▂▂▂▂▃▂▂▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▃█▅▁▄▁▂▄▃▃▃▄▃▃▄▄▄▇▅▅▅▆▆▅▅▅▇▅▅▅▅▅█▅▄▆▆▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC █▆▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▅▇▇▇█▇█████████▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▂▂▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▇▅▁▃▁▁▁▁▁▁▁▂▁▁▂▂▁▃▃▁▃▂▁▂▂▂▁▂▂▁▂▂▃▁▁▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▃█▅▁▄▁▂▄▃▃▃▄▃▃▄▄▄▇▅▅▅▆▆▅▅▅▇▅▅▅▅▅█▅▄▆▆▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC █▆▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▅▇▇▇█▇█████████▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▂▁▁▂▂▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.13617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 2.67113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.23468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.46924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.7305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.412\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.99825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.412\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 354.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 4.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1530.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 88.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1530\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a1_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/lqqboywl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002145-lqqboywl/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:22:25,549][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a1_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002226-mj9mie4u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a1_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/mj9mie4u\u001b[0m\n",
      "[2023-08-09 00:22:30,585][root][INFO] - => Done in 5.036 s\n",
      "[2023-08-09 00:22:30,585][root][INFO] - \n",
      "[2023-08-09 00:22:30,586][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:22:30,589][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:22:30,589][root][INFO] - => Done in 3.760 ms\n",
      "[2023-08-09 00:22:30,589][root][INFO] - \n",
      "[2023-08-09 00:22:30,589][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:22:31,164][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:22:31,164][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:22:31,164][root][INFO] - => Done in 574.460 ms\n",
      "[2023-08-09 00:22:31,164][root][INFO] - \n",
      "[2023-08-09 00:22:31,164][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:22:31,164][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-09 00:22:31,164][root][INFO] - \n",
      "[2023-08-09 00:22:31,164][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:22:31,164][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:22:31,930][root][INFO] - => Done in 765.814 ms\n",
      "[2023-08-09 00:22:31,930][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▆▆▆▇▇▇████████████████████████▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▅▅▆▅▆▆▆▆▇▇▇█▇█▇█▆▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▆█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▅▆▆▆▆▇▇▇▇▇█▇█▇█▇█▇█████▇▇███▇▇▇██▇█▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇▇▇▇█▇▇▇████▇██▇▇██▇██▇▇█▇▇█▇██▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▆▆▇▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇▇▇▇█▇▇▇████▇██▇▇██▇██▇▇█▇▇█▇██▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▅▅▆▆▆▇▆▆▇▇▇▇█▇▇▇▇▇▇█▇▇█▆▇▇▇▇█▇▇▇▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▄▅▆▄▅▆▇▅▅▇▅▆▄▆▄▇▅▆▅▇▅▅▇▆▆▇▅▃▅▅▇█▆█▅▅▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▅▅▅▄▃▃▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▄▄▄▄▅▃▅▆▆▆▄▇▅▇▄▆▄▆▅▄▅▅▃█▅▄▄▅▇▇▄▄▆▆▄▇▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▄▄▄▃▃▂▂▁▂▂▁▁▁▂▂▂▁▂▁▂▁▂▂▁▂▁▁▂▂▂▂▁▁▂▁▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▅▅▆▆▆▇▆▆▇▇▇▇█▇▇▇▇▇▇█▇▇█▆▇▇▇▇█▇▇▇▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▄▅▆▄▅▆▇▅▅▇▅▆▄▆▄▇▅▆▅▇▅▅▇▆▆▇▅▃▅▅▇█▆█▅▅▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▄▅▅▅▄▃▃▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▄▄▄▄▅▃▅▆▆▆▄▇▅▇▄▆▄▆▅▄▅▅▃█▅▄▄▅▇▇▄▄▆▆▄▇▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▄▄▄▃▃▂▂▁▂▂▁▁▁▂▂▂▁▂▁▂▁▂▂▁▂▁▁▂▂▂▂▁▁▂▁▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▅▅▆▆▆▇▆▆▇▇▇▇█▇▇▇▇▇▇█▇▇█▆▇▇▇▇█▇▇▇▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▆▅▆▅▄▄▃▃▂▃▂▁▁▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂▂▂▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▄▄▄▆▅▅▆▇▆▆▇▆▇▅▇▆▇▆▇▆█▆▆▇▇▇█▆▅▆▆▇█▇█▆▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▅▅▅▄▃▃▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▄▄▄▄▅▃▅▆▆▆▄▇▅▇▄▆▄▆▅▄▅▅▃█▅▄▄▅▇▇▄▄▆▆▄▇▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▄▄▄▃▃▃▂▂▃▂▁▁▁▂▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▂▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▄▄▄▃▃▂▂▁▂▂▁▁▁▂▂▂▂▂▁▂▁▂▂▁▂▁▂▂▂▂▂▁▁▂▁▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▅▅▆▆▆▇▆▆▇▇▇▇█▇▇▇▇▇▇█▇▇█▆▇▇▇▇█▇▇▇▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▆▅▆▅▄▄▃▃▂▃▂▁▁▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂▂▂▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▄▄▄▆▅▅▆▇▆▆▇▆▇▅▇▆▇▆▇▆█▆▆▇▇▇█▆▅▆▆▇█▇█▆▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▄▅▅▅▄▃▃▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▄▄▄▄▅▃▅▆▆▆▄▇▅▇▄▆▄▆▅▄▅▅▃█▅▄▄▅▇▇▄▄▆▆▄▇▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▄▄▄▃▃▃▂▂▃▂▁▁▁▂▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▂▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▄▄▄▃▃▂▂▁▂▂▁▁▁▂▂▂▂▂▁▂▁▂▂▁▂▁▂▂▂▂▂▁▁▂▁▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.29211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.81302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 2.74241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.4595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.2755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1235\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.1145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 919.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 551.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 247.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 229.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 13.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.4595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.2575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1235\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.1125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 919\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a1_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/mj9mie4u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002226-mj9mie4u/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:23:07,541][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a1_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002308-y4xuxkbt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a1_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y4xuxkbt\u001b[0m\n",
      "[2023-08-09 00:23:12,367][root][INFO] - => Done in 4.826 s\n",
      "[2023-08-09 00:23:12,367][root][INFO] - \n",
      "[2023-08-09 00:23:12,367][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:23:12,370][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:23:12,370][root][INFO] - => Done in 3.065 ms\n",
      "[2023-08-09 00:23:12,371][root][INFO] - \n",
      "[2023-08-09 00:23:12,371][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:23:12,952][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:23:12,952][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:23:12,952][root][INFO] - => Done in 581.300 ms\n",
      "[2023-08-09 00:23:12,952][root][INFO] - \n",
      "[2023-08-09 00:23:12,952][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:23:12,952][root][INFO] - => Done in 110.865 us\n",
      "[2023-08-09 00:23:12,952][root][INFO] - \n",
      "[2023-08-09 00:23:12,952][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:23:12,952][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:23:13,709][root][INFO] - => Done in 756.571 ms\n",
      "[2023-08-09 00:23:13,709][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▆▇▇▇▇▇▇▇█████▇███▇████▇████▇▇█▇▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▆▆▆▇▇▇▇▇▇▇████▇▇▇██▇██▇█▇█▇███▇█▇▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▆▆▆▇▇█▇▇▇██▇▇▇█▇██████▇▇██▇▇▇█▇▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▆▇▇▇▇█▇▇▇█▇█▇█▇▇█▇▇██▇█▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▇▇▇▇█▇▇█████▇████████▇████████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▆▇▇▇▇█▇▇▇█▇█▇█▇▇█▇▇██▇█▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▅▅▆▆▆▇▇▇▇▇▇▇██▇▇▇▇█▇████▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁█▆▇▇▇▆▆▇▅▆▅▄▃▃▅▆▆▃▄▆▄▄▅▅▇▄▆▅▄▄▅▄▅▅▅▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▄▃▄▂▃▂▂▂▂▂▁▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▅▆▆▆▆▆▇▇▇▇▆▇▇▇▆▇▆▇▇▆▆▅▆▇▆▆▆▇▇█▆▆▇▇▆██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▄▃▃▂▃▂▂▁▂▂▁▂▁▂▁▂▂▁▁▂▁▁▂▁▂▁▁▂▂▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▅▅▆▆▆▇▇▇▇▇▇▇██▇▇▇▇█▇████▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁█▆▇▇▇▆▆▇▅▆▅▄▃▃▅▆▆▃▄▆▄▄▅▅▇▄▆▅▄▄▅▄▅▅▅▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▄▃▄▂▃▂▂▂▂▂▁▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▅▆▆▆▆▆▇▇▇▇▆▇▇▇▆▇▆▇▇▆▆▅▆▇▆▆▆▇▇█▆▆▇▇▆██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▄▃▃▂▃▂▂▁▂▂▁▂▁▂▁▂▂▁▁▂▁▁▂▁▂▁▁▂▂▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▅▅▆▆▆▇▇▇▇▇▇▇██▇▇▇▇█▇████▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▇▇██▇▇▇▅▄▅▅▃▃▂▆▅▄▄▃▃▄▃▂▄▅▄▄▃▂▃▅▄▃▄▄▂▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁█▅▅▅▅▄▅█▇▅▄▆▄▄▃▆█▂▅▇▄▄▆▄█▃▇▅▅▅▅▄▇▆▅▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▄▃▄▂▃▂▂▂▂▂▁▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▅▆▆▆▆▆▇▇▇▇▆▇▇▇▆▇▆▇▇▆▆▅▆▇▆▆▆▇▇█▆▆▇▇▆██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▃██▆▅▄▄▃▃▃▃▃▃▂▄▃▆▂▃▃▂▃▁▂▂▃▃▂▁▃▄▂▂▄▃▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▃▃▁▂▂▂▁▂▂▁▂▁▂▁▂▂▁▁▂▁▁▂▁▁▁▁▂▂▂▂▁▁▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▅▅▆▆▆▇▇▇▇▇▇▇██▇▇▇▇█▇████▇█▇▇▇▇▇█▇▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▇▇██▇▇▇▅▄▅▅▃▃▂▆▅▄▄▃▃▄▃▂▄▅▄▄▃▂▃▅▄▃▄▄▂▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁█▅▅▅▅▄▅█▇▅▄▆▄▄▃▆█▂▅▇▄▄▆▄█▃▇▅▅▅▅▄▇▆▅▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▄▃▄▂▃▂▂▂▂▂▁▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▅▆▆▆▆▆▇▇▇▇▆▇▇▇▆▇▆▇▇▆▆▅▆▇▆▆▆▇▇█▆▆▇▇▆██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▃██▆▅▄▄▃▃▃▃▃▃▂▄▃▆▂▃▃▂▃▁▂▂▃▃▂▁▃▄▂▂▄▃▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▃▃▁▂▂▂▁▂▂▁▂▁▂▁▂▂▁▁▂▁▁▂▁▁▁▁▂▂▂▂▁▁▂▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.38533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.5526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 2.99487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.6485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.9745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.59075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.9745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.5075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1015.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 430.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 33.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 293.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 199.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.5075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a1_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y4xuxkbt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002308-y4xuxkbt/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:23:47,689][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002348-ltpchnm1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ltpchnm1\u001b[0m\n",
      "[2023-08-09 00:23:52,641][root][INFO] - => Done in 4.952 s\n",
      "[2023-08-09 00:23:52,641][root][INFO] - \n",
      "[2023-08-09 00:23:52,641][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:23:52,644][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:23:52,644][root][INFO] - => Done in 3.251 ms\n",
      "[2023-08-09 00:23:52,644][root][INFO] - \n",
      "[2023-08-09 00:23:52,644][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:23:53,225][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:23:53,225][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:23:53,225][root][INFO] - => Done in 580.597 ms\n",
      "[2023-08-09 00:23:53,225][root][INFO] - \n",
      "[2023-08-09 00:23:53,225][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:23:53,225][root][INFO] - => Done in 102.758 us\n",
      "[2023-08-09 00:23:53,225][root][INFO] - \n",
      "[2023-08-09 00:23:53,225][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:23:53,225][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:23:53,987][root][INFO] - => Done in 761.376 ms\n",
      "[2023-08-09 00:23:53,987][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.014 MB of 1.014 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▇▇▇█▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▄▄▆▅▆▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▆▆█▇▇▇▇▇▇▇▇▇█▇▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▆▇▇▇▇▇▇██▇▆▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▄▅▆▆▆▇▇▆▆▇▇█▇█▇▇▇█▇▆▇▇▇█▇█▆▇▇▇▇█▇▇███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▅▆▆▆▇▇█▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▄▅▆▆▆▇▇▆▆▇▇█▇█▇▇▇█▇▆▇▇▇█▇█▆▇▇▇▇█▇▇███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▂▄▅▅█▅▃▄▃▂▃▂▄▁▃▂▄▂▄▄▃▆▂▁▃▃▄▁▃▂▃▄▃▃▂▂▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▅▆▆▆▆▇▆▆▇▆▇▇▆▇▆▆▆▆██▆▇▆▇▇▇▇▇█▆▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▄▅▄▄▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▃▃▅▄▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▆█▆▇▇▇▇▆▆▆▇█▇▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▆▅▅▅▄▃▂▃▃▂▂▁▂▁▂▂▂▁▂▂▃▂▂▁▂▁▂▁▂▂▂▁▁▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▂▄▅▅█▅▃▄▃▂▃▂▄▁▃▂▄▂▄▄▃▆▂▁▃▃▄▁▃▂▃▄▃▃▂▂▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▅▆▆▆▆▇▆▆▇▆▇▇▆▇▆▆▆▆██▆▇▆▇▇▇▇▇█▆▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▅▅▄▄▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▃▃▅▄▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▆█▆▇▇▇▇▆▆▆▇█▇▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▆▅▅▅▄▃▂▃▃▂▂▁▂▁▂▂▂▁▂▂▃▂▂▁▂▁▂▁▂▂▂▁▁▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▂▄▅▅█▅▃▄▃▂▃▂▄▁▃▂▄▂▄▄▃▆▂▁▃▃▄▁▃▂▃▄▃▃▂▂▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇▆██▆▇▇▇▇▇▇▇█▇▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▄▅▄▄▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▃▃▅▄▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▆█▆▇▇▇▇▆▆▆▇█▇▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▇█▅█▆▆▄▃▂▃▃▂▂▂▁▂▁▂▂▂▁▂▂▂▁▂▁▂▁▁▂▁▁▁▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▆▅▅▄▄▂▂▃▃▂▂▁▂▁▂▂▂▁▂▂▃▂▂▁▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▂▄▅▅█▅▃▄▃▂▃▂▄▁▃▂▄▂▄▄▃▆▂▁▃▃▄▁▃▂▃▄▃▃▂▂▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇▆██▆▇▇▇▇▇▇▇█▇▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▅▅▄▄▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▃▃▅▄▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▆█▆▇▇▇▇▆▆▆▇█▇▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▇█▅█▆▆▄▃▂▃▃▂▂▂▁▂▁▂▂▂▁▂▂▂▁▂▁▂▁▁▂▁▁▁▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▆▅▅▄▄▂▂▃▃▂▂▁▂▁▂▂▂▁▂▂▃▂▂▁▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.91767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.21883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 3.56906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.88075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.2895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 3.5835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.3285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 168.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 834.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 18.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 657.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 296.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 7.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.3285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.1465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ltpchnm1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002348-ltpchnm1/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:24:28,828][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002429-pzh9task\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pzh9task\u001b[0m\n",
      "[2023-08-09 00:24:34,194][root][INFO] - => Done in 5.366 s\n",
      "[2023-08-09 00:24:34,194][root][INFO] - \n",
      "[2023-08-09 00:24:34,194][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:24:34,198][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:24:34,199][root][INFO] - => Done in 4.320 ms\n",
      "[2023-08-09 00:24:34,199][root][INFO] - \n",
      "[2023-08-09 00:24:34,199][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:24:34,757][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:24:34,758][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:24:34,758][root][INFO] - => Done in 559.114 ms\n",
      "[2023-08-09 00:24:34,758][root][INFO] - \n",
      "[2023-08-09 00:24:34,758][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:24:34,758][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-09 00:24:34,758][root][INFO] - \n",
      "[2023-08-09 00:24:34,758][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:24:34,758][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:24:35,518][root][INFO] - => Done in 760.151 ms\n",
      "[2023-08-09 00:24:35,518][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pzh9task\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002429-pzh9task/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:25:10,727][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002511-r2zruk4x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/r2zruk4x\u001b[0m\n",
      "[2023-08-09 00:25:15,488][root][INFO] - => Done in 4.761 s\n",
      "[2023-08-09 00:25:15,488][root][INFO] - \n",
      "[2023-08-09 00:25:15,488][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:25:15,491][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:25:15,491][root][INFO] - => Done in 3.087 ms\n",
      "[2023-08-09 00:25:15,491][root][INFO] - \n",
      "[2023-08-09 00:25:15,491][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:25:16,070][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:25:16,070][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:25:16,070][root][INFO] - => Done in 578.707 ms\n",
      "[2023-08-09 00:25:16,070][root][INFO] - \n",
      "[2023-08-09 00:25:16,070][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:25:16,070][root][INFO] - => Done in 123.978 us\n",
      "[2023-08-09 00:25:16,070][root][INFO] - \n",
      "[2023-08-09 00:25:16,070][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:25:16,071][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:25:16,826][root][INFO] - => Done in 756.025 ms\n",
      "[2023-08-09 00:25:16,827][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.016 MB of 1.016 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9929\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/r2zruk4x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002511-r2zruk4x/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:25:51,907][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002553-bbqbkc1j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/bbqbkc1j\u001b[0m\n",
      "[2023-08-09 00:25:56,781][root][INFO] - => Done in 4.875 s\n",
      "[2023-08-09 00:25:56,782][root][INFO] - \n",
      "[2023-08-09 00:25:56,782][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:25:56,785][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:25:56,785][root][INFO] - => Done in 3.243 ms\n",
      "[2023-08-09 00:25:56,785][root][INFO] - \n",
      "[2023-08-09 00:25:56,785][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:25:57,366][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:25:57,366][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:25:57,366][root][INFO] - => Done in 580.778 ms\n",
      "[2023-08-09 00:25:57,366][root][INFO] - \n",
      "[2023-08-09 00:25:57,366][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:25:57,366][root][INFO] - => Done in 124.931 us\n",
      "[2023-08-09 00:25:57,366][root][INFO] - \n",
      "[2023-08-09 00:25:57,366][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:25:57,366][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:25:58,123][root][INFO] - => Done in 757.229 ms\n",
      "[2023-08-09 00:25:58,124][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 1.014 MB of 1.017 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▆█▆▅▆▆▅▆▇▆▅▅▆▆▄▆▆▅▆▅▅▅▄▆▄▅▅▆▅▅▅▆▅▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇█▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▇▇▇▇█▇█▇████▇████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▄▂▃▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▂▁▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▄▂▃▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▂▁▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▂▂▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▄▄▂▄▂▂▃▂▃▁▃▂▁▁▁▃▁▂▁▁▁▁▂▁▁▃▂▂▁▁▁▁▁▃▃▁▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▇█▇▆▆▄▄▃▄▂▄▅▄▃▃▅▄▂▄▄▂▃▂▃▃▂▃▁▂▃▄▂▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇██▇▇█▇█▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▂▂▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▄▄▂▄▂▂▃▂▃▁▃▂▁▁▁▃▁▂▁▁▁▁▂▁▁▃▂▂▁▁▁▁▁▃▃▁▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▇█▇▆▆▄▄▃▄▂▄▅▄▃▃▅▄▂▄▄▂▃▂▃▃▂▃▁▂▃▄▂▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇██▇▇█▇█▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▂▂▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC █▇▆▃▇▃▂▄▃▄▁▅▃▁▁▁▅▁▃▁▁▂▁▂▁▁▅▃▂▁▁▁▂▁▄▄▁▂▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▇█▇▆▆▄▄▃▄▂▄▅▄▃▃▅▄▂▄▄▂▃▂▃▃▂▃▁▂▃▄▂▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▅▆▇▇▇▇▇█▇▇▇▇█▇▇█▇▇█▇████▇███▇█████▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▂▂▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC █▇▆▃▇▃▂▄▃▄▁▅▃▁▁▁▅▁▃▁▁▂▁▂▁▁▅▃▂▁▁▁▂▁▄▄▁▂▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▇█▇▆▆▄▄▃▄▂▄▅▄▃▃▅▄▂▄▄▂▃▂▃▃▂▃▁▂▃▄▂▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▅▆▇▇▇▇▇█▇▇▇▇█▇▇█▇▇█▇████▇███▇█████▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.85273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.2872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 205.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1743.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 12.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/bbqbkc1j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002553-bbqbkc1j/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:26:34,939][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002635-x52xta13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/x52xta13\u001b[0m\n",
      "[2023-08-09 00:26:39,806][root][INFO] - => Done in 4.867 s\n",
      "[2023-08-09 00:26:39,806][root][INFO] - \n",
      "[2023-08-09 00:26:39,806][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:26:39,809][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:26:39,809][root][INFO] - => Done in 3.363 ms\n",
      "[2023-08-09 00:26:39,809][root][INFO] - \n",
      "[2023-08-09 00:26:39,810][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:26:40,362][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:26:40,363][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:26:40,363][root][INFO] - => Done in 553.079 ms\n",
      "[2023-08-09 00:26:40,363][root][INFO] - \n",
      "[2023-08-09 00:26:40,363][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:26:40,363][root][INFO] - => Done in 144.958 us\n",
      "[2023-08-09 00:26:40,363][root][INFO] - \n",
      "[2023-08-09 00:26:40,363][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:26:40,363][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:26:41,123][root][INFO] - => Done in 760.341 ms\n",
      "[2023-08-09 00:26:41,124][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/x52xta13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002635-x52xta13/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:27:15,881][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002716-kxxgvce0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/kxxgvce0\u001b[0m\n",
      "[2023-08-09 00:27:20,747][root][INFO] - => Done in 4.866 s\n",
      "[2023-08-09 00:27:20,747][root][INFO] - \n",
      "[2023-08-09 00:27:20,747][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:27:20,750][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:27:20,750][root][INFO] - => Done in 3.190 ms\n",
      "[2023-08-09 00:27:20,751][root][INFO] - \n",
      "[2023-08-09 00:27:20,751][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:27:21,332][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:27:21,332][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:27:21,332][root][INFO] - => Done in 581.353 ms\n",
      "[2023-08-09 00:27:21,332][root][INFO] - \n",
      "[2023-08-09 00:27:21,332][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:27:21,332][root][INFO] - => Done in 112.057 us\n",
      "[2023-08-09 00:27:21,332][root][INFO] - \n",
      "[2023-08-09 00:27:21,332][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:27:21,332][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:27:22,104][root][INFO] - => Done in 771.384 ms\n",
      "[2023-08-09 00:27:22,104][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 1.007 MB of 1.020 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▇█▆▇▇▇▇▇▇▇▇▆▇▇▆▇▇▆▇▇▇▇▆▇▆▇▇▆▆▇▇▆▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇██▇▇▇▇▇██▇█▇█▇▇██▇▇███▇▇▇██▇▇▇█▇▇█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▅▇▇▇▇████▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▅▃▂▂▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▅▃▂▂▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▆▃▂▂▁▁▄▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▄▂▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▆▄▂▄▂▃▃▃▃▁▂▂▁▁▂▃▂▂▁▂▂▁▂▁▁▄▂▃▁▂▂▃▁▃▂▂▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▄█▅▄▄▂▃▂▂▃▃▃▂▃▂▃▂▂▃▂▂▂▃▃▃▂▂▁▂▃▂▁▂▃▁▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▄▆▇▇▇▇█▇▇▇████▇██▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▆▃▂▂▁▁▄▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▄▂▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▆▄▂▄▂▃▃▃▃▁▂▂▁▁▂▃▂▂▁▂▂▁▂▁▁▄▂▃▁▂▂▃▁▃▂▂▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▄█▅▄▄▂▃▂▂▃▃▃▂▃▂▃▂▂▃▂▂▂▃▃▃▂▂▁▂▃▂▁▂▃▁▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▄▆▇▇▇▇█▇▇▇████▇██▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▆▃▂▂▁▁▄▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▄▂▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▇█▆▂▇▃▅▄▄▅▁▃▄▁▂▃▅▂▃▂▂▂▁▃▁▂▇▃▄▁▃▂▅▁▅▃▂▂▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▄█▅▄▄▂▃▂▂▃▃▃▂▃▂▃▂▂▃▂▂▂▃▃▃▂▂▁▂▃▂▁▂▃▁▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▄▆▇▇▇▇██▇█████▇████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▆▃▂▂▁▁▄▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▄▂▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▇█▆▂▇▃▅▄▄▅▁▃▄▁▂▃▅▂▃▂▂▂▁▃▁▂▇▃▄▁▃▂▅▁▅▃▂▂▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▄█▅▄▄▂▃▂▂▃▃▃▂▃▂▃▂▂▃▂▂▂▃▃▃▂▂▁▂▃▂▁▂▃▁▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▄▆▇▇▇▇██▇█████▇████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.13233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.2006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.85144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.29003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.05725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 241.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1706.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/kxxgvce0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002716-kxxgvce0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:27:57,001][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002757-f1y0uvud\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/f1y0uvud\u001b[0m\n",
      "[2023-08-09 00:28:01,962][root][INFO] - => Done in 4.961 s\n",
      "[2023-08-09 00:28:01,962][root][INFO] - \n",
      "[2023-08-09 00:28:01,962][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:28:01,965][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:28:01,966][root][INFO] - => Done in 3.017 ms\n",
      "[2023-08-09 00:28:01,966][root][INFO] - \n",
      "[2023-08-09 00:28:01,966][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:28:02,537][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:28:02,537][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:28:02,538][root][INFO] - => Done in 571.922 ms\n",
      "[2023-08-09 00:28:02,538][root][INFO] - \n",
      "[2023-08-09 00:28:02,538][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:28:02,538][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-09 00:28:02,538][root][INFO] - \n",
      "[2023-08-09 00:28:02,538][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:28:02,538][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:28:03,306][root][INFO] - => Done in 767.871 ms\n",
      "[2023-08-09 00:28:03,306][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/f1y0uvud\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002757-f1y0uvud/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:28:37,888][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002838-y8xvgniu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y8xvgniu\u001b[0m\n",
      "[2023-08-09 00:28:42,240][root][INFO] - => Done in 4.352 s\n",
      "[2023-08-09 00:28:42,240][root][INFO] - \n",
      "[2023-08-09 00:28:42,240][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:28:42,247][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:28:42,248][root][INFO] - => Done in 7.170 ms\n",
      "[2023-08-09 00:28:42,248][root][INFO] - \n",
      "[2023-08-09 00:28:42,248][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:28:42,854][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:28:42,854][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:28:42,854][root][INFO] - => Done in 606.561 ms\n",
      "[2023-08-09 00:28:42,854][root][INFO] - \n",
      "[2023-08-09 00:28:42,855][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:28:42,855][root][INFO] - => Done in 105.143 us\n",
      "[2023-08-09 00:28:42,855][root][INFO] - \n",
      "[2023-08-09 00:28:42,855][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:28:42,855][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:28:43,644][root][INFO] - => Done in 788.945 ms\n",
      "[2023-08-09 00:28:43,644][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y8xvgniu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002838-y8xvgniu/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:29:19,692][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_002920-42d0b8a4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/42d0b8a4\u001b[0m\n",
      "[2023-08-09 00:29:25,198][root][INFO] - => Done in 5.505 s\n",
      "[2023-08-09 00:29:25,198][root][INFO] - \n",
      "[2023-08-09 00:29:25,198][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:29:25,201][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:29:25,201][root][INFO] - => Done in 3.087 ms\n",
      "[2023-08-09 00:29:25,201][root][INFO] - \n",
      "[2023-08-09 00:29:25,201][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:29:25,778][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:29:25,779][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:29:25,779][root][INFO] - => Done in 577.706 ms\n",
      "[2023-08-09 00:29:25,779][root][INFO] - \n",
      "[2023-08-09 00:29:25,779][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:29:25,779][root][INFO] - => Done in 138.044 us\n",
      "[2023-08-09 00:29:25,779][root][INFO] - \n",
      "[2023-08-09 00:29:25,779][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:29:25,779][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:29:26,554][root][INFO] - => Done in 775.052 ms\n",
      "[2023-08-09 00:29:26,554][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆██▇▇▇▇▆▇█▇▇▆█▇▆▇▇▆▇▆▇▇▆▇▆▇▇▇▇▇▇▇▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆▇█▇▇▇▇▆▇▇▇▇▆▇▇▆▇▇▆▇▆▆▇▆▇▆▇▇▇▇▇▇▇▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇▇██▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ██▅▆▆▄▄▄▄▂▃▅▃▃▂▅▄▁▄▃▂▃▂▂▃▂▃▁▃▃▃▃▃▂▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇████▇███▇█▇███▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ██▅▆▆▄▄▄▄▂▃▅▃▃▂▅▄▁▄▃▂▃▂▂▃▂▃▁▃▃▃▃▃▂▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇████▇███▇█▇███▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ██▅▆▆▄▄▄▄▂▃▅▃▃▂▅▄▁▄▃▂▃▂▂▃▂▃▁▃▃▃▃▃▂▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ██▅▆▆▄▄▄▄▂▃▅▃▃▂▅▄▁▄▃▂▃▂▂▃▂▃▁▃▃▃▃▃▂▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.09633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.83881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 207.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 13.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1759\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/42d0b8a4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_002920-42d0b8a4/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:30:00,895][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003001-bp8syd4n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/bp8syd4n\u001b[0m\n",
      "[2023-08-09 00:30:06,011][root][INFO] - => Done in 5.117 s\n",
      "[2023-08-09 00:30:06,011][root][INFO] - \n",
      "[2023-08-09 00:30:06,012][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:30:06,015][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:30:06,015][root][INFO] - => Done in 3.226 ms\n",
      "[2023-08-09 00:30:06,015][root][INFO] - \n",
      "[2023-08-09 00:30:06,015][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:30:06,571][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:30:06,572][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:30:06,572][root][INFO] - => Done in 556.809 ms\n",
      "[2023-08-09 00:30:06,572][root][INFO] - \n",
      "[2023-08-09 00:30:06,572][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:30:06,572][root][INFO] - => Done in 106.812 us\n",
      "[2023-08-09 00:30:06,572][root][INFO] - \n",
      "[2023-08-09 00:30:06,572][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:30:06,572][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:30:07,347][root][INFO] - => Done in 774.862 ms\n",
      "[2023-08-09 00:30:07,347][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.012 MB of 1.024 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/bp8syd4n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003001-bp8syd4n/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:30:42,795][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003043-8vm54tgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/8vm54tgg\u001b[0m\n",
      "[2023-08-09 00:30:48,564][root][INFO] - => Done in 5.769 s\n",
      "[2023-08-09 00:30:48,566][root][INFO] - \n",
      "[2023-08-09 00:30:48,566][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:30:48,573][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:30:48,573][root][INFO] - => Done in 7.521 ms\n",
      "[2023-08-09 00:30:48,573][root][INFO] - \n",
      "[2023-08-09 00:30:48,573][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:30:49,144][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:30:49,144][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:30:49,144][root][INFO] - => Done in 570.450 ms\n",
      "[2023-08-09 00:30:49,144][root][INFO] - \n",
      "[2023-08-09 00:30:49,144][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:30:49,144][root][INFO] - => Done in 154.972 us\n",
      "[2023-08-09 00:30:49,144][root][INFO] - \n",
      "[2023-08-09 00:30:49,144][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:30:49,144][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:30:49,919][root][INFO] - => Done in 774.251 ms\n",
      "[2023-08-09 00:30:49,919][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.025 MB of 1.025 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▆█▃▄▅▂▃▂▃▂▂▂▂▃▂▂▃▁▃▂▁▃▁▂▂▁▂▁▂▂▂▁▂▂▁▃▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▇▇▇▇█▇███████▇█▇████████████████▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▆█▃▄▅▂▃▂▃▂▂▂▂▃▂▂▃▁▃▂▁▃▁▂▂▁▂▁▂▂▂▁▂▂▁▃▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▇▇▇▇█▇███████▇█▇████████████████▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▆█▃▄▅▂▃▂▃▂▂▂▂▃▂▂▃▁▃▂▁▃▁▂▂▁▂▁▂▂▂▁▂▂▁▃▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▆█▃▄▅▂▃▂▃▂▂▂▂▃▂▂▃▁▃▂▁▃▁▂▂▁▂▁▂▂▂▁▂▂▁▃▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.83994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.0385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 217.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1749.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.874\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/8vm54tgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003043-8vm54tgg/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:31:25,494][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003126-0oqyir1a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/0oqyir1a\u001b[0m\n",
      "[2023-08-09 00:31:30,073][root][INFO] - => Done in 4.580 s\n",
      "[2023-08-09 00:31:30,074][root][INFO] - \n",
      "[2023-08-09 00:31:30,074][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:31:30,079][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:31:30,079][root][INFO] - => Done in 4.942 ms\n",
      "[2023-08-09 00:31:30,079][root][INFO] - \n",
      "[2023-08-09 00:31:30,079][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:31:30,680][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:31:30,680][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:31:30,681][root][INFO] - => Done in 601.603 ms\n",
      "[2023-08-09 00:31:30,681][root][INFO] - \n",
      "[2023-08-09 00:31:30,681][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:31:30,681][root][INFO] - => Done in 123.024 us\n",
      "[2023-08-09 00:31:30,681][root][INFO] - \n",
      "[2023-08-09 00:31:30,681][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:31:30,681][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:31:31,461][root][INFO] - => Done in 780.138 ms\n",
      "[2023-08-09 00:31:31,461][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/0oqyir1a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003126-0oqyir1a/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:32:06,490][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003207-muj0e56t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/muj0e56t\u001b[0m\n",
      "[2023-08-09 00:32:11,289][root][INFO] - => Done in 4.799 s\n",
      "[2023-08-09 00:32:11,289][root][INFO] - \n",
      "[2023-08-09 00:32:11,289][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:32:11,293][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:32:11,293][root][INFO] - => Done in 3.815 ms\n",
      "[2023-08-09 00:32:11,293][root][INFO] - \n",
      "[2023-08-09 00:32:11,293][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:32:11,880][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:32:11,881][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:32:11,881][root][INFO] - => Done in 587.784 ms\n",
      "[2023-08-09 00:32:11,881][root][INFO] - \n",
      "[2023-08-09 00:32:11,881][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:32:11,881][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-09 00:32:11,881][root][INFO] - \n",
      "[2023-08-09 00:32:11,881][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:32:11,881][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:32:12,650][root][INFO] - => Done in 769.231 ms\n",
      "[2023-08-09 00:32:12,651][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.027 MB of 1.027 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/muj0e56t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003207-muj0e56t/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:32:47,940][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003248-1farvais\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/1farvais\u001b[0m\n",
      "[2023-08-09 00:32:52,322][root][INFO] - => Done in 4.382 s\n",
      "[2023-08-09 00:32:52,323][root][INFO] - \n",
      "[2023-08-09 00:32:52,323][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:32:52,327][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:32:52,327][root][INFO] - => Done in 4.495 ms\n",
      "[2023-08-09 00:32:52,327][root][INFO] - \n",
      "[2023-08-09 00:32:52,327][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:32:52,940][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:32:52,940][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:32:52,940][root][INFO] - => Done in 612.443 ms\n",
      "[2023-08-09 00:32:52,940][root][INFO] - \n",
      "[2023-08-09 00:32:52,940][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:32:52,940][root][INFO] - => Done in 122.786 us\n",
      "[2023-08-09 00:32:52,940][root][INFO] - \n",
      "[2023-08-09 00:32:52,940][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:32:52,940][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:32:53,723][root][INFO] - => Done in 782.734 ms\n",
      "[2023-08-09 00:32:53,723][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.028 MB of 1.028 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆██▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇▆▇▆▇▇▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆██▇▇▇▇▆▇▇▇▇▆▇▇▆▇▇▆▇▆▇▇▆▇▆▇▇▇▇▇▇▇▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▂▁▂▁▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▂▁▂▁▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ██▅▆▆▄▄▄▄▃▃▄▄▃▃▄▄▂▄▃▂▃▂▃▃▂▃▁▃▃▃▃▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇█▇█▇█▇███▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ██▅▆▆▄▄▄▄▃▃▄▄▃▃▄▄▂▄▃▂▃▂▃▃▂▃▁▃▃▃▃▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇█▇█▇█▇███▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ██▅▆▆▄▄▄▄▃▃▄▄▃▃▄▄▂▄▃▂▃▂▃▃▂▃▁▃▃▃▃▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▆▇▇▇▇▇█▇▇▇▇█▇▇█▇▇████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ██▅▆▆▄▄▄▄▃▃▄▄▃▃▄▄▂▄▃▂▃▂▃▃▂▃▁▃▃▃▃▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▆▇▇▇▇▇█▇▇▇▇█▇▇█▇▇████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.12033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20709\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.83952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.05275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 251.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1720.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 251\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1720\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/1farvais\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003248-1farvais/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:33:29,054][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003330-ai4uv5hn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ai4uv5hn\u001b[0m\n",
      "[2023-08-09 00:33:33,584][root][INFO] - => Done in 4.530 s\n",
      "[2023-08-09 00:33:33,584][root][INFO] - \n",
      "[2023-08-09 00:33:33,584][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:33:33,588][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:33:33,588][root][INFO] - => Done in 4.019 ms\n",
      "[2023-08-09 00:33:33,588][root][INFO] - \n",
      "[2023-08-09 00:33:33,589][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:33:34,185][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:33:34,185][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:33:34,185][root][INFO] - => Done in 596.667 ms\n",
      "[2023-08-09 00:33:34,185][root][INFO] - \n",
      "[2023-08-09 00:33:34,185][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:33:34,185][root][INFO] - => Done in 102.997 us\n",
      "[2023-08-09 00:33:34,185][root][INFO] - \n",
      "[2023-08-09 00:33:34,185][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:33:34,186][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:33:34,965][root][INFO] - => Done in 780.003 ms\n",
      "[2023-08-09 00:33:34,966][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ai4uv5hn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003330-ai4uv5hn/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:34:09,624][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003410-huw1t7am\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/huw1t7am\u001b[0m\n",
      "[2023-08-09 00:34:14,454][root][INFO] - => Done in 4.830 s\n",
      "[2023-08-09 00:34:14,454][root][INFO] - \n",
      "[2023-08-09 00:34:14,454][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:34:14,457][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:34:14,457][root][INFO] - => Done in 3.109 ms\n",
      "[2023-08-09 00:34:14,457][root][INFO] - \n",
      "[2023-08-09 00:34:14,457][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:34:15,024][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:34:15,024][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:34:15,024][root][INFO] - => Done in 566.730 ms\n",
      "[2023-08-09 00:34:15,024][root][INFO] - \n",
      "[2023-08-09 00:34:15,024][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:34:15,024][root][INFO] - => Done in 111.818 us\n",
      "[2023-08-09 00:34:15,024][root][INFO] - \n",
      "[2023-08-09 00:34:15,024][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:34:15,025][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:34:15,789][root][INFO] - => Done in 764.386 ms\n",
      "[2023-08-09 00:34:15,789][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.030 MB of 1.030 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▆█▃▄▅▂▃▃▂▃▂▂▃▃▂▂▂▁▃▂▂▂▂▂▃▁▂▁▂▂▁▁▂▂▂▃▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▇▇▇▇▇▇▇██▇▇████▇██▇██████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▆█▃▄▅▂▃▃▂▃▂▂▃▃▂▂▂▁▃▂▂▂▂▂▃▁▂▁▂▂▁▁▂▂▂▃▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▇▇▇▇▇▇▇██▇▇████▇██▇██████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▆█▃▄▅▂▃▃▂▃▂▂▃▃▂▂▂▁▃▂▂▂▂▂▃▁▂▁▂▂▁▁▂▂▂▃▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇█▇█████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▆█▃▄▅▂▃▃▂▃▂▂▃▃▂▂▂▁▃▂▂▂▂▂▃▁▂▁▂▂▁▁▂▂▂▃▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇█▇█████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.09883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20601\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.0435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 204.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1771.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/huw1t7am\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003410-huw1t7am/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:34:50,821][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003451-b23jxb3t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/b23jxb3t\u001b[0m\n",
      "[2023-08-09 00:34:55,861][root][INFO] - => Done in 5.040 s\n",
      "[2023-08-09 00:34:55,862][root][INFO] - \n",
      "[2023-08-09 00:34:55,862][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:34:55,865][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:34:55,865][root][INFO] - => Done in 3.014 ms\n",
      "[2023-08-09 00:34:55,865][root][INFO] - \n",
      "[2023-08-09 00:34:55,865][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:34:56,425][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:34:56,426][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:34:56,426][root][INFO] - => Done in 560.875 ms\n",
      "[2023-08-09 00:34:56,426][root][INFO] - \n",
      "[2023-08-09 00:34:56,426][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:34:56,426][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-09 00:34:56,426][root][INFO] - \n",
      "[2023-08-09 00:34:56,426][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:34:56,426][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:34:57,196][root][INFO] - => Done in 770.397 ms\n",
      "[2023-08-09 00:34:57,197][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1978.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/b23jxb3t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003451-b23jxb3t/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:35:32,214][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003533-pp0i4rvt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pp0i4rvt\u001b[0m\n",
      "[2023-08-09 00:35:38,652][root][INFO] - => Done in 6.438 s\n",
      "[2023-08-09 00:35:38,652][root][INFO] - \n",
      "[2023-08-09 00:35:38,652][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:35:38,655][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:35:38,655][root][INFO] - => Done in 2.936 ms\n",
      "[2023-08-09 00:35:38,655][root][INFO] - \n",
      "[2023-08-09 00:35:38,655][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:35:39,243][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:35:39,243][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:35:39,243][root][INFO] - => Done in 588.690 ms\n",
      "[2023-08-09 00:35:39,243][root][INFO] - \n",
      "[2023-08-09 00:35:39,244][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:35:39,244][root][INFO] - => Done in 103.951 us\n",
      "[2023-08-09 00:35:39,244][root][INFO] - \n",
      "[2023-08-09 00:35:39,244][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:35:39,244][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:35:40,013][root][INFO] - => Done in 769.273 ms\n",
      "[2023-08-09 00:35:40,013][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 1.033 MB of 1.033 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pp0i4rvt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003533-pp0i4rvt/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:36:14,283][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003615-d2929tqq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/d2929tqq\u001b[0m\n",
      "[2023-08-09 00:36:19,117][root][INFO] - => Done in 4.834 s\n",
      "[2023-08-09 00:36:19,118][root][INFO] - \n",
      "[2023-08-09 00:36:19,118][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:36:19,122][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:36:19,122][root][INFO] - => Done in 4.011 ms\n",
      "[2023-08-09 00:36:19,122][root][INFO] - \n",
      "[2023-08-09 00:36:19,122][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:36:19,706][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:36:19,706][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:36:19,706][root][INFO] - => Done in 584.186 ms\n",
      "[2023-08-09 00:36:19,706][root][INFO] - \n",
      "[2023-08-09 00:36:19,706][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:36:19,706][root][INFO] - => Done in 144.005 us\n",
      "[2023-08-09 00:36:19,706][root][INFO] - \n",
      "[2023-08-09 00:36:19,707][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:36:19,707][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:36:20,483][root][INFO] - => Done in 776.065 ms\n",
      "[2023-08-09 00:36:20,483][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.034 MB of 1.034 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆██▇▇▇▇▇▇█▇▇▆▇▇▆▇▇▆▆▆▇▇▆▇▆▇▇▇▇▇▇▇▆█▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆██▇▇▇▇▇▇█▇▇▆▇▇▆▇▇▆▆▆▆▇▆▇▆▇▇▇▆▇▇▇▆█▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▇▇▇▇█▇██▇████████████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▂▂▁▁▁▂▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▂▂▁▁▁▂▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▃▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▃▁▁▁▆▁█▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▄▁▁▂▁▁▆▅▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ██▆▇▆▄▄▃▄▃▄▅▃▃▃▄▄▂▄▃▂▂▂▂▃▂▃▁▃▃▃▂▃▃▃▂▆▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██████▇██▇▇███▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▃▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▃▁▁▁▆▁█▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▄▁▁▂▁▁▆▅▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ██▆▇▆▄▄▃▄▃▄▅▃▃▃▄▄▂▄▃▂▂▂▂▃▂▃▁▃▃▃▂▃▃▃▂▆▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██████▇██▇▇███▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▃▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▃▁▁▁▆▁█▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▄▁▁▂▁▁▆▅▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ██▆▇▆▄▄▃▄▃▄▅▃▃▃▄▄▂▄▃▂▂▂▂▃▂▃▁▃▃▃▂▃▃▃▂▆▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▆▇▇▇▇▇█▇▇▇▇█▇▇█▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▃▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▃▁▁▁▆▁█▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▄▁▁▂▁▁▆▅▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ██▆▇▆▄▄▃▄▃▄▅▃▃▃▄▄▂▄▃▂▂▂▂▃▂▃▁▃▃▃▂▃▃▃▂▆▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▆▇▇▇▇▇█▇▇▇▇█▇▇█▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 199.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1761.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/d2929tqq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003615-d2929tqq/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:36:56,652][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003657-avjp4r66\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/avjp4r66\u001b[0m\n",
      "[2023-08-09 00:37:01,594][root][INFO] - => Done in 4.942 s\n",
      "[2023-08-09 00:37:01,594][root][INFO] - \n",
      "[2023-08-09 00:37:01,594][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:37:01,598][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:37:01,598][root][INFO] - => Done in 3.370 ms\n",
      "[2023-08-09 00:37:01,598][root][INFO] - \n",
      "[2023-08-09 00:37:01,598][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:37:02,179][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:37:02,180][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:37:02,180][root][INFO] - => Done in 581.876 ms\n",
      "[2023-08-09 00:37:02,180][root][INFO] - \n",
      "[2023-08-09 00:37:02,180][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:37:02,180][root][INFO] - => Done in 112.057 us\n",
      "[2023-08-09 00:37:02,180][root][INFO] - \n",
      "[2023-08-09 00:37:02,180][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:37:02,180][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:37:02,956][root][INFO] - => Done in 776.055 ms\n",
      "[2023-08-09 00:37:02,956][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/avjp4r66\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003657-avjp4r66/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:37:39,426][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003740-p70v62kt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/p70v62kt\u001b[0m\n",
      "[2023-08-09 00:37:44,660][root][INFO] - => Done in 5.234 s\n",
      "[2023-08-09 00:37:44,660][root][INFO] - \n",
      "[2023-08-09 00:37:44,660][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:37:44,663][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:37:44,664][root][INFO] - => Done in 3.169 ms\n",
      "[2023-08-09 00:37:44,664][root][INFO] - \n",
      "[2023-08-09 00:37:44,664][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:37:45,234][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:37:45,234][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:37:45,234][root][INFO] - => Done in 570.180 ms\n",
      "[2023-08-09 00:37:45,234][root][INFO] - \n",
      "[2023-08-09 00:37:45,234][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:37:45,234][root][INFO] - => Done in 107.050 us\n",
      "[2023-08-09 00:37:45,234][root][INFO] - \n",
      "[2023-08-09 00:37:45,234][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:37:45,234][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:37:46,017][root][INFO] - => Done in 782.837 ms\n",
      "[2023-08-09 00:37:46,017][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▁▁▁▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▁▁▁▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▇▁▁▁▅▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▃▁▁▁▆▁▇▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▆▁▁█▁▁▇▄▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▇█▄▅▅▃▃▃▃▄▃▃▃▃▃▄▃▂▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▂▄▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▇▇▇▇█▇▇█████▇██▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▇▁▁▁▅▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▃▁▁▁▆▁▇▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▆▁▁█▁▁▇▄▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▇█▄▅▅▃▃▃▃▄▃▃▃▃▃▄▃▂▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▂▄▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▇▇▇▇█▇▇█████▇██▇█████████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▇▁▁▁▅▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▃▁▁▁▆▁▇▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▆▁▁█▁▁▇▄▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▇█▄▅▅▃▃▃▃▄▃▃▃▃▃▄▃▂▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▂▄▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇████▇████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▇▁▁▁▅▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▃▁▁▁▆▁▇▁▁▁▁▆▁▁▁▁▁▆▁▁▂▁▅▁▁▁▆▆▁▁█▁▁▇▄▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▇█▄▅▅▃▃▃▃▄▃▃▃▃▃▄▃▂▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▂▄▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇████▇████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.203\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.876\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 209.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1752.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 209\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/p70v62kt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003740-p70v62kt/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:38:20,948][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003822-krkvqdkc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/krkvqdkc\u001b[0m\n",
      "[2023-08-09 00:38:26,860][root][INFO] - => Done in 5.912 s\n",
      "[2023-08-09 00:38:26,860][root][INFO] - \n",
      "[2023-08-09 00:38:26,860][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:38:26,866][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:38:26,866][root][INFO] - => Done in 5.627 ms\n",
      "[2023-08-09 00:38:26,866][root][INFO] - \n",
      "[2023-08-09 00:38:26,866][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:38:27,436][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:38:27,436][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:38:27,436][root][INFO] - => Done in 569.879 ms\n",
      "[2023-08-09 00:38:27,436][root][INFO] - \n",
      "[2023-08-09 00:38:27,436][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:38:27,436][root][INFO] - => Done in 120.878 us\n",
      "[2023-08-09 00:38:27,436][root][INFO] - \n",
      "[2023-08-09 00:38:27,436][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:38:27,436][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:38:28,220][root][INFO] - => Done in 783.803 ms\n",
      "[2023-08-09 00:38:28,220][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/krkvqdkc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003822-krkvqdkc/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:39:02,671][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003903-272b1p4e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/272b1p4e\u001b[0m\n",
      "[2023-08-09 00:39:07,758][root][INFO] - => Done in 5.086 s\n",
      "[2023-08-09 00:39:07,758][root][INFO] - \n",
      "[2023-08-09 00:39:07,758][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:39:07,761][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:39:07,761][root][INFO] - => Done in 3.519 ms\n",
      "[2023-08-09 00:39:07,762][root][INFO] - \n",
      "[2023-08-09 00:39:07,762][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:39:08,331][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:39:08,331][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:39:08,331][root][INFO] - => Done in 569.375 ms\n",
      "[2023-08-09 00:39:08,331][root][INFO] - \n",
      "[2023-08-09 00:39:08,331][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:39:08,331][root][INFO] - => Done in 109.911 us\n",
      "[2023-08-09 00:39:08,331][root][INFO] - \n",
      "[2023-08-09 00:39:08,331][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:39:08,331][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:39:09,106][root][INFO] - => Done in 774.787 ms\n",
      "[2023-08-09 00:39:09,106][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/272b1p4e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003903-272b1p4e/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:39:45,023][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_003945-39mcfvbf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/39mcfvbf\u001b[0m\n",
      "[2023-08-09 00:39:49,947][root][INFO] - => Done in 4.924 s\n",
      "[2023-08-09 00:39:49,948][root][INFO] - \n",
      "[2023-08-09 00:39:49,948][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:39:49,951][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:39:49,951][root][INFO] - => Done in 3.284 ms\n",
      "[2023-08-09 00:39:49,951][root][INFO] - \n",
      "[2023-08-09 00:39:49,951][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:39:50,515][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:39:50,515][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:39:50,515][root][INFO] - => Done in 563.823 ms\n",
      "[2023-08-09 00:39:50,515][root][INFO] - \n",
      "[2023-08-09 00:39:50,515][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:39:50,515][root][INFO] - => Done in 110.865 us\n",
      "[2023-08-09 00:39:50,515][root][INFO] - \n",
      "[2023-08-09 00:39:50,515][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:39:50,515][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:39:51,283][root][INFO] - => Done in 768.211 ms\n",
      "[2023-08-09 00:39:51,284][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆▇█▇▇▇▇▆▇▇▇▇▆▇▇▆▇▇▆▇▆▆▇▆▇▆▇▇▇▇▇▇▇▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆▇█▇▇▇▇▆▇▇▇▇▆▇▇▆▇▇▆▇▆▇▇▆▇▆▇▇▇▆▇▇▇▆▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▃▃▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▂▂▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▃▃▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▂▂▂▁▁▁▁▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▇▄▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ██▅▆▇▄▄▄▄▃▃▅▄▃▃▅▄▁▅▃▂▃▂▃▃▂▃▁▃▃▄▃▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇█▇▇▇█▇▇██▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▇▄▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ██▅▆▇▄▄▄▄▃▃▅▄▃▃▅▄▁▅▃▂▃▂▃▃▂▃▁▃▃▄▃▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇████▇█▇▇▇█▇▇██▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▅▄▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ██▅▆▇▄▄▄▄▃▃▅▄▃▃▅▄▁▅▃▂▃▂▃▃▂▃▁▃▃▄▃▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇██████████▇█████▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▅▄▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ██▅▆▇▄▄▄▄▃▃▅▄▃▃▅▄▁▅▃▂▃▂▃▃▂▃▁▃▃▄▃▃▃▃▂▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇██████████▇█████▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.13217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.83838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.2688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.2565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 268.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1697.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 268\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/39mcfvbf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_003945-39mcfvbf/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:40:25,206][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004026-4orynohh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4orynohh\u001b[0m\n",
      "[2023-08-09 00:40:30,404][root][INFO] - => Done in 5.198 s\n",
      "[2023-08-09 00:40:30,405][root][INFO] - \n",
      "[2023-08-09 00:40:30,405][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:40:30,408][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:40:30,408][root][INFO] - => Done in 3.582 ms\n",
      "[2023-08-09 00:40:30,408][root][INFO] - \n",
      "[2023-08-09 00:40:30,408][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:40:30,967][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:40:30,967][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:40:30,967][root][INFO] - => Done in 558.952 ms\n",
      "[2023-08-09 00:40:30,967][root][INFO] - \n",
      "[2023-08-09 00:40:30,967][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:40:30,968][root][INFO] - => Done in 142.097 us\n",
      "[2023-08-09 00:40:30,968][root][INFO] - \n",
      "[2023-08-09 00:40:30,968][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:40:30,968][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:40:31,755][root][INFO] - => Done in 786.980 ms\n",
      "[2023-08-09 00:40:31,755][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.0155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4orynohh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004026-4orynohh/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:41:08,143][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004109-430qswlp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/430qswlp\u001b[0m\n",
      "[2023-08-09 00:41:14,311][root][INFO] - => Done in 6.167 s\n",
      "[2023-08-09 00:41:14,311][root][INFO] - \n",
      "[2023-08-09 00:41:14,311][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:41:14,318][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:41:14,319][root][INFO] - => Done in 7.323 ms\n",
      "[2023-08-09 00:41:14,319][root][INFO] - \n",
      "[2023-08-09 00:41:14,319][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:41:14,893][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:41:14,893][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:41:14,893][root][INFO] - => Done in 574.459 ms\n",
      "[2023-08-09 00:41:14,893][root][INFO] - \n",
      "[2023-08-09 00:41:14,893][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:41:14,893][root][INFO] - => Done in 124.216 us\n",
      "[2023-08-09 00:41:14,893][root][INFO] - \n",
      "[2023-08-09 00:41:14,893][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:41:14,894][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:41:15,690][root][INFO] - => Done in 796.586 ms\n",
      "[2023-08-09 00:41:15,690][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▅▄▁▁▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▆█▃▄▅▂▃▃▃▃▂▃▃▃▂▃▃▂▄▂▂▃▂▂▃▂▃▂▂▃▂▁▂▂▂▂▃▂▂���\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▇▇▇▇▇▇▇██▇▇█▇▇█▇██▇██▇███████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▅▄▁▁▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▆█▃▄▅▂▃▃▃▃▂▃▃▃▂▃▃▂▄▂▂▃▂▂▃▂▃▂▂▃▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▇▇▇▇▇▇▇██▇▇█▇▇█▇██▇██▇███████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▃▄▁▁▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▆█▃▄▅▂▃▃▃▃▂▃▃▃▂▃▃▂▄▂▂▃▂▂▃▂▃▂▂▃▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇▇▇█▇█████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▃▄▁▁▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▆█▃▄▅▂▃▃▃▃▂▃▃▃▂▃▃▂▄▂▂▃▂▂▃▂▃▂▂▃▂▁▂▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇▇▇█▇█████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.09833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 207.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1764.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.8815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/430qswlp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004109-430qswlp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:41:50,165][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004151-6mmy6dc1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/6mmy6dc1\u001b[0m\n",
      "[2023-08-09 00:41:55,584][root][INFO] - => Done in 5.419 s\n",
      "[2023-08-09 00:41:55,584][root][INFO] - \n",
      "[2023-08-09 00:41:55,584][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:41:55,590][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:41:55,590][root][INFO] - => Done in 6.195 ms\n",
      "[2023-08-09 00:41:55,591][root][INFO] - \n",
      "[2023-08-09 00:41:55,591][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:41:56,169][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:41:56,170][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:41:56,170][root][INFO] - => Done in 579.094 ms\n",
      "[2023-08-09 00:41:56,170][root][INFO] - \n",
      "[2023-08-09 00:41:56,170][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:41:56,170][root][INFO] - => Done in 146.866 us\n",
      "[2023-08-09 00:41:56,170][root][INFO] - \n",
      "[2023-08-09 00:41:56,170][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:41:56,170][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:41:56,957][root][INFO] - => Done in 787.147 ms\n",
      "[2023-08-09 00:41:56,957][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/6mmy6dc1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004151-6mmy6dc1/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:42:31,020][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004232-7axafrtx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/7axafrtx\u001b[0m\n",
      "[2023-08-09 00:42:35,849][root][INFO] - => Done in 4.829 s\n",
      "[2023-08-09 00:42:35,850][root][INFO] - \n",
      "[2023-08-09 00:42:35,850][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:42:35,854][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:42:35,854][root][INFO] - => Done in 4.193 ms\n",
      "[2023-08-09 00:42:35,854][root][INFO] - \n",
      "[2023-08-09 00:42:35,854][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:42:36,458][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:42:36,458][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:42:36,458][root][INFO] - => Done in 604.052 ms\n",
      "[2023-08-09 00:42:36,458][root][INFO] - \n",
      "[2023-08-09 00:42:36,458][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:42:36,458][root][INFO] - => Done in 117.064 us\n",
      "[2023-08-09 00:42:36,459][root][INFO] - \n",
      "[2023-08-09 00:42:36,459][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:42:36,459][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:42:37,248][root][INFO] - => Done in 789.069 ms\n",
      "[2023-08-09 00:42:37,248][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.043 MB of 1.043 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01557\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/7axafrtx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004232-7axafrtx/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:43:13,228][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s3a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004314-clh32tuk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s3a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/clh32tuk\u001b[0m\n",
      "[2023-08-09 00:43:17,940][root][INFO] - => Done in 4.713 s\n",
      "[2023-08-09 00:43:17,941][root][INFO] - \n",
      "[2023-08-09 00:43:17,941][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:43:17,944][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:43:17,944][root][INFO] - => Done in 3.443 ms\n",
      "[2023-08-09 00:43:17,944][root][INFO] - \n",
      "[2023-08-09 00:43:17,944][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:43:18,536][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:43:18,536][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:43:18,536][root][INFO] - => Done in 591.567 ms\n",
      "[2023-08-09 00:43:18,536][root][INFO] - \n",
      "[2023-08-09 00:43:18,536][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:43:18,536][root][INFO] - => Done in 142.097 us\n",
      "[2023-08-09 00:43:18,536][root][INFO] - \n",
      "[2023-08-09 00:43:18,536][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:43:18,536][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:43:19,320][root][INFO] - => Done in 783.646 ms\n",
      "[2023-08-09 00:43:19,320][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▆██▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▆▇▇▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▆██▇▇▇▇▇▇█▇▇▇▇▇▆▇▇▆▆▇▆▇▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇▇██▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▃▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▂▂▂▂▁▁▂▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▃▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▂▂▂▂▁▁▂▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▄▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▄▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▂▂▁▁▂▆▁█▁▄▁▁▆▁▁▁▁▁▆▁▁▂▁▆▁▁▁▆▇▁▁▂▁▁▆▂▁▁▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ██▅▆▆▄▄▃▄▃▃▅▄▃▃▄▄▂▄▄▂▂▂▂▃▂▃▁▃▃▄▂▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇██████▇███▇███▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▄▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▄▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▂▂▁▁▂▆▁█▁▄▁▁▆▁▁▁▁▁▆▁▁▂▁▆▁▁▁▆▇▁▁▂▁▁▆▂▁▁▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ██▅▆▆▄▄▃▄▃▃▅▄▃▃▄▄▂▄▄▂▂▂▂▃▂▃▁▃▃▄▂▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇██████▇███▇███▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▄▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▄▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▂▁▁▂▆▁█▁▄▁▁▆▁▁▁▁▁▆▁▁▂▁▆▁▁▁▆▇▁▁▂▁▁▆▂▁▁▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ██▅▆▆▄▄▃▄▃▃▅▄▃▃▄▄▂▄▄▂▂▂▂▃▂▃▁▃▃▄▂▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇██▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▄▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▄▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▂▁▁▂▆▁█▁▄▁▁▆▁▁▁▁▁▆▁▁▂▁▆▁▁▁▆▇▁▁▂▁▁▆▂▁▁▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ██▅▆▆▄▄▃▄▃▃▅▄▃▃▄▄▂▄▄▂▂▂▂▃▂▃▁▃▃▄▂▃▃▃▂▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▆▇▇▇▇▇▇█▇▇▇██▇▇█▇▇██████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.10833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.2053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.84372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.26953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.8885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.2345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.2345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.8755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 209.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1751.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 209\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1750\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s3a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/clh32tuk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004314-clh32tuk/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:43:54,865][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004355-4t90cbqj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4t90cbqj\u001b[0m\n",
      "[2023-08-09 00:44:00,465][root][INFO] - => Done in 5.600 s\n",
      "[2023-08-09 00:44:00,465][root][INFO] - \n",
      "[2023-08-09 00:44:00,465][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:44:00,472][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:44:00,472][root][INFO] - => Done in 7.451 ms\n",
      "[2023-08-09 00:44:00,473][root][INFO] - \n",
      "[2023-08-09 00:44:00,473][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:44:01,052][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:44:01,052][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:44:01,052][root][INFO] - => Done in 579.439 ms\n",
      "[2023-08-09 00:44:01,052][root][INFO] - \n",
      "[2023-08-09 00:44:01,052][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:44:01,052][root][INFO] - => Done in 132.084 us\n",
      "[2023-08-09 00:44:01,052][root][INFO] - \n",
      "[2023-08-09 00:44:01,053][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:44:01,053][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:44:01,831][root][INFO] - => Done in 778.640 ms\n",
      "[2023-08-09 00:44:01,831][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.9926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/4t90cbqj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004355-4t90cbqj/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 00:44:37,109][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_004438-5icts9ha\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/5icts9ha\u001b[0m\n",
      "[2023-08-09 00:44:41,778][root][INFO] - => Done in 4.669 s\n",
      "[2023-08-09 00:44:41,779][root][INFO] - \n",
      "[2023-08-09 00:44:41,779][root][INFO] - => Env setup ...\n",
      "[2023-08-09 00:44:41,782][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 00:44:41,782][root][INFO] - => Done in 3.261 ms\n",
      "[2023-08-09 00:44:41,782][root][INFO] - \n",
      "[2023-08-09 00:44:41,782][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 00:44:42,376][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 00:44:42,376][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 00:44:42,376][root][INFO] - => Done in 594.320 ms\n",
      "[2023-08-09 00:44:42,376][root][INFO] - \n",
      "[2023-08-09 00:44:42,376][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 00:44:42,377][root][INFO] - => Done in 113.010 us\n",
      "[2023-08-09 00:44:42,377][root][INFO] - \n",
      "[2023-08-09 00:44:42,377][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 00:44:42,377][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 00:44:43,148][root][INFO] - => Done in 771.246 ms\n",
      "[2023-08-09 00:44:43,148][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁█▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁█▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 █▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep █▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▂▁▁▁▆▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▂▁▁▁▅▁▅▁▃▁▁▅▁▁▁▁▁▅▁▁▂▁▅▁▁▁▅█▁▁▇▁▁▅▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▆█▃▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▃▄▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▅▇▇▇▇▇▇▇▇███▇▇▇▇█▇██▇█▇████████████▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▂▁▁▁▆▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▂▁▁▁▅▁▅▁▃▁▁▅▁▁▁▁▁▅▁▁▂▁▅▁▁▁▅█▁▁▇▁▁▅▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▆█▃▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▃▄▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▅▇▇▇▇▇▇▇▇███▇▇▇▇█▇██▇█▇████████████▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▂▁▁▁▆▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▂▁▁▁▅▁▅▁▃▁▁▅▁▁▁▁▁▅▁▁▂▁▅▁▁▁▅█▁▁▇▁▁▅▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▆█▃▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▃▄▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▅▇▇▇▇▇█▇▇████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▂▁▁▁▆▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▂▁▁▁▅▁▅▁▃▁▁▅▁▁▁▁▁▅▁▁▂▁▅▁▁▁▅█▁▁▇▁▁▅▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▆█▃▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▂▃▂▃▃▂▃▂▂▃▂▁▃▃▂▃▄▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▅▇▇▇▇▇█▇▇████████▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.11283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 3.20516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 2.8431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.2701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 3.2125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.04825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.1095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 219.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1742.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.1095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/5icts9ha\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_004438-5icts9ha/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# they all did ddc so I expect the shapers to be compatible to each other\n",
    "\n",
    "import yaml\n",
    "game= \"ipd\"\n",
    "num_pl = 3\n",
    "num_shap = 2\n",
    "mean = [(1,2), (2,2), (3,1), (4,2), (5,1)]\n",
    "nice = [(1,1), (2,1), (3,2), (4,1), (5,2)]\n",
    "\n",
    "# mean v mean\n",
    "for seed1,agent1 in mean:\n",
    "    for seed2,agent2 in mean:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        group = f'{num_pl}pl_{num_shap}shap_{game}-comp-mean_v_mean'\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group={group}\n",
    "\n",
    "\n",
    "# nice v nice\n",
    "for seed1,agent1 in nice:\n",
    "    for seed2,agent2 in nice:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        group = f'{num_pl}pl_{num_shap}shap_{game}-comp-nice_v_nice'\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group={group}\n",
    "\n",
    "\n",
    "# mean v nice\n",
    "for seed1,agent1 in mean:\n",
    "    for seed2,agent2 in nice:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        group = f'{num_pl}pl_{num_shap}shap_{game}-comp-mean_v_nice'\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group={group}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:15:31,890][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181533-ziwcp9yp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ziwcp9yp\u001b[0m\n",
      "[2023-08-08 18:15:36,714][root][INFO] - => Done in 4.825 s\n",
      "[2023-08-08 18:15:36,715][root][INFO] - \n",
      "[2023-08-08 18:15:36,715][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:15:36,718][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:15:36,719][root][INFO] - => Done in 3.866 ms\n",
      "[2023-08-08 18:15:36,719][root][INFO] - \n",
      "[2023-08-08 18:15:36,719][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:15:37,337][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:15:37,337][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:15:37,337][root][INFO] - => Done in 618.588 ms\n",
      "[2023-08-08 18:15:37,337][root][INFO] - \n",
      "[2023-08-08 18:15:37,338][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:15:37,338][root][INFO] - => Done in 275.373 us\n",
      "[2023-08-08 18:15:37,338][root][INFO] - \n",
      "[2023-08-08 18:15:37,338][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:15:37,338][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:15:38,147][root][INFO] - => Done in 808.660 ms\n",
      "[2023-08-08 18:15:38,147][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.906 MB of 0.906 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▅▄▅▅▅▆▆▅▇▇▆▆█▇█▇▇█▇███▇▇▇█▇▇▆▇▇█▇█▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▅▅▄▃▇▆▁▃▄▄▁▇▄█▅▂▇▄▇▇▇▅▄▆▇▃▅▁▄▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▄▅▄▃▇▆▁▅▇▄▂▇▄█▅▄▇▄▇▇▇▅▄▆▇▅▅▁▆▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▅▆▆▇▇▆▇▇▇▇█▇██▇█▇████▇████▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▄▅▄▃▇▆▁▅▆▄▂▇▄█▅▄▇▄▇▇▇▅▄▆▇▅▅▁▆▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▅▆▆▇▇▆▇▇▇▇█▇██▇█▇████▇████▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▃▄▂█▃▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▂▂▅▄▅▆▂▃█▃▂▅▇▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▃▄▂█▃▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▂▂▅▄▅▆▂▃█▃▂▅▇▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▃▄▂█▃▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▂▂▅▄▅▆▂▃█▃▂▅▇▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▃▄▂█▃▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▂▂▅▄▅▆▂▃█▃▂▅▇▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.919\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.97726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.84131\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1938.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 40.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ziwcp9yp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181533-ziwcp9yp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:16:14,088][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181615-f26h3k9q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/f26h3k9q\u001b[0m\n",
      "[2023-08-08 18:16:18,921][root][INFO] - => Done in 4.833 s\n",
      "[2023-08-08 18:16:18,921][root][INFO] - \n",
      "[2023-08-08 18:16:18,921][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:16:18,929][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:16:18,929][root][INFO] - => Done in 7.942 ms\n",
      "[2023-08-08 18:16:18,929][root][INFO] - \n",
      "[2023-08-08 18:16:18,929][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:16:19,520][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:16:19,520][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:16:19,520][root][INFO] - => Done in 590.735 ms\n",
      "[2023-08-08 18:16:19,520][root][INFO] - \n",
      "[2023-08-08 18:16:19,520][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:16:19,521][root][INFO] - => Done in 198.841 us\n",
      "[2023-08-08 18:16:19,521][root][INFO] - \n",
      "[2023-08-08 18:16:19,521][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:16:19,521][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:16:20,289][root][INFO] - => Done in 768.173 ms\n",
      "[2023-08-08 18:16:20,289][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.895 MB of 0.907 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▅▆▆▆▇▆▅▇▅▇▅█▅█▅▆▆▄▅▅▆▄▄▇▄▅▆▅▆▅▅▄▄▇▅▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██▇▇▇▇▆▆▅▅▅▃▆▄▆▄▅▃▄▃▂▂▃▃▂▂▅▁▂▄▄▃▂▂▂▁▄▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▅▆▅▄▇▅▂▅▄▄▁▆▃▆▃▄▄▂▃▄▄▁▁▅▃▃▃▁▅▃▃▃▃▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▆▆▆▆▆▇▇▆▇▇▇▇█▇█▇█▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▆▆▆▅▆▅▄▅▃▅▂▆▃▅▂▄▃▁▂▃▃▁▁▄▁▂▃▂▃▂▂▂▁▄▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▆▆▆▆▆▇▇▆▇▇▇▇█▇█▇█▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▅▆▆▆▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇█▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▂▁▂▂▂▃▄▃▄▆▃▄▃▄▄▆▄▆▆▇▆▆▇▆▄█▇▅▄▆▆▇▇█▅▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▃▃▂█▃▃▅▂▂▅▁▁▄▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▂▂▆▄▅▆▂▂█▃▂▅█▂▅▁▄▄▂▆▂▂▂▅▆▃▂▃▅▇▁▃▂▄▂▃▃▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▃▃▁█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▅▆▆▆▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇█▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▂▁▂▂▂▃▄▃▃▆▃▄▃▄▄▆▄▆▆▇▆▆▇▆▄█▇▅▄▆▆▇▇█▅▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▃▃▂█▃▃▅▂▂▅▁▁▄▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▂▂▆▄▅▆▂▂█▃▂▅█▂▅▁▄▄▂▆▂▂▂▅▆▃▂▃▅▇▁▃▂▄▂▃▃▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▃▃▁█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▅▆▆▆▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇█▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▃▃▂▂▂▅█▅▂▂▃▃▃▂▂▁▃▁▂▂▂▁▁▂▁▂▂▁▂▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▂▁▂▂▂▃▄▃▄▆▃▄▃▄▄▆▄▆▆▇▆▆▇▆▄█▇▅▄▆▆▇▇█▅▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▃▃▂█▃▃▅▂▂▅▁▁▄▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▂▂▆▄▅▆▂▂█▃▂▅█▂▅▁▄▄▂▆▂▂▂▅▆▃▂▃▅▇▁▃▂▄▂▃▃▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▃▃▁█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▅▆▆▆▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇█▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▃▃▂▂▂▅█▅▂▂▃▃▃▂▂▁▃▁▂▂▂▁▁▂▁▂▂▁▂▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▂▁▂▂▂▃▄▃▃▆▃▄▃▄▄▆▄▆▆▇▆▆▇▆▄█▇▅▄▆▆▇▇█▅▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▃▃▂█▃▃▅▂▂▅▁▁▄▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▂▂▆▄▅▆▂▂█▃▂▅█▂▅▁▄▄▂▆▂▂▂▅▆▃▂▃▅▇▁▃▂▄▂▃▃▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▃▃▁█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.78554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.87383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.59969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.6975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.75825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 24.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1811.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 105.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 24.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/f26h3k9q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181615-f26h3k9q/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:16:54,953][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181656-fw63yyq6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/fw63yyq6\u001b[0m\n",
      "[2023-08-08 18:16:59,869][root][INFO] - => Done in 4.916 s\n",
      "[2023-08-08 18:16:59,869][root][INFO] - \n",
      "[2023-08-08 18:16:59,869][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:16:59,872][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:16:59,872][root][INFO] - => Done in 3.090 ms\n",
      "[2023-08-08 18:16:59,872][root][INFO] - \n",
      "[2023-08-08 18:16:59,872][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:17:00,448][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:17:00,448][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:17:00,448][root][INFO] - => Done in 575.681 ms\n",
      "[2023-08-08 18:17:00,448][root][INFO] - \n",
      "[2023-08-08 18:17:00,448][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:17:00,448][root][INFO] - => Done in 193.834 us\n",
      "[2023-08-08 18:17:00,449][root][INFO] - \n",
      "[2023-08-08 18:17:00,449][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:17:00,449][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:17:01,175][root][INFO] - => Done in 726.458 ms\n",
      "[2023-08-08 18:17:01,175][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▅▄▅▅▅▆▆▅▇▇▆▆█▇█▇▇█▇███▇▇▇█▇▇▆█▇█▇█▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▅▅▄▃▇▆▁▆▇▄▁▇▄█▅▂▇▄▇▇▇▅▄▆▇▆▅▁▇▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▄▅▄▃▇▆▁▆▇▄▁▇▄█▅▄▇▄▇▇▇▅▄▆▇▆▅▁▇▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▅▆▆▇▇▆▇█▇▇█▇██▇█▇█████████▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▄▅▄▃▇▆▁▆▇▄▁▇▄█▅▄▇▄▇▇▇▅▄▆▇▆▅▁▇▆▇▆▇▆▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▅▆▆▇▇▆▇█▇▇█▇██▇█▇█████████▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▃▃▂█▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▂▂▅▄▅▆▂▃█▃▂▅█▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▃▃▂█▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▂▂▅▄▅▆▂▃█▃▂▅█▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▃▃▂█▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▂▂▅▄▅▆▂▃█▃▂▅█▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▅▆▆▇▇▇▇█▇▇██████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▃▃▂█▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▂▂▅▄▅▆▂▃█▃▂▅█▂▅▁▄▄▂▅▂▂▂▄▅▃▂▃▄▆▂▃▂▃▂▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.9195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.97968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.84377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.8985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.8985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1939.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 40.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/fw63yyq6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181656-fw63yyq6/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:17:34,900][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181736-l4hsvtm1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/l4hsvtm1\u001b[0m\n",
      "[2023-08-08 18:17:40,195][root][INFO] - => Done in 5.296 s\n",
      "[2023-08-08 18:17:40,195][root][INFO] - \n",
      "[2023-08-08 18:17:40,195][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:17:40,199][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:17:40,199][root][INFO] - => Done in 3.792 ms\n",
      "[2023-08-08 18:17:40,199][root][INFO] - \n",
      "[2023-08-08 18:17:40,199][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:17:40,789][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:17:40,790][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:17:40,790][root][INFO] - => Done in 590.378 ms\n",
      "[2023-08-08 18:17:40,790][root][INFO] - \n",
      "[2023-08-08 18:17:40,790][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:17:40,790][root][INFO] - => Done in 127.077 us\n",
      "[2023-08-08 18:17:40,790][root][INFO] - \n",
      "[2023-08-08 18:17:40,790][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:17:40,790][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:17:41,586][root][INFO] - => Done in 795.634 ms\n",
      "[2023-08-08 18:17:41,586][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.897 MB of 0.910 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▄▆▆▄▇▄▄▇▆▇▄█▅▇▃▇▆▆▃▅▆▄▅▇▄▆▆▆▆▅▆▂▅▇▆▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▆▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▃▄▅▂▄▄▅▄▃▄▁▃▅▄▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▅▆▅▄▆▄▂▅▅▄▁▆▃▅▂▃▄▃▃▄▄▂▂▄▃▃▃▂▄▃▄▁▃▄▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▆▆▆▇▆▆▇▇▇▆█▇█▇█▇█▇▇█▇▇█▇▇███▇█▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▆▇▆▅▆▄▄▆▄▅▃▆▄▅▂▄▄▄▂▃▄▃▃▅▂▄▄▄▄▃▄▁▃▅▄▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▆▆▆▇▆▆▇▇▇▆█▇█▇█▇█▇▇█▇▇█▇▇███▇█▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▆▆▆▇▆▆▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▄▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▃▃▂█▄▄▆▂▂▅▁▁▃▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▂▂▅▄▅▅▂▂█▃▂▅█▂▅▁▄▅▂▆▂▂▂▆▆▃▂▃▅▆▂▃▂▃▂▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▅▇▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▆▆▆▇▆▆▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▄▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▃▃▂█▄▄▆▂▂▅▁▁▃▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▂▂▅▄▅▅▂▂█▃▂▅█▂▅▁▄▅▂▆▂▂▂▆▆▃▂▃▅▆▂▃▂▃▂▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▅▇▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▆▆▆▆▆▆▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▅▇▅▆▆▃▅▆█▃▃▃▃▃▂▃▂▃▁▁▁▃▁▂▂▁▂▃▂▁▂▂▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▄▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▃▃▂█▄▄▆▂▂▅▁▁▃▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▂▂▅▄▅▅▂▂█▃▂▅█▂▅▁▄▅▂▆▂▂▂▆▆▃▂▃▅▆▂▃▂▃▂▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▅▇▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▆▆▆▆▆▆▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▅▇▅▆▆▃▅▆█▃▃▃▃▃▂▃▂▃▁▁▁▃▁▂▂▁▂▃▂▁▂▂▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▄▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▃▃▂█▄▄▆▂▂▅▁▁▃▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▂▂▅▄▅▅▂▂█▃▂▅█▂▅▁▄▅▂▆▂▂▂▆▆▃▂▃▅▆▂▃▂▃▂▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▅▇▁█▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.681\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.7482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.85993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.56976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.797\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1806.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 118.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 30.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/l4hsvtm1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181736-l4hsvtm1/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:18:15,742][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181816-8t8up1qg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/8t8up1qg\u001b[0m\n",
      "[2023-08-08 18:18:20,794][root][INFO] - => Done in 5.052 s\n",
      "[2023-08-08 18:18:20,795][root][INFO] - \n",
      "[2023-08-08 18:18:20,795][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:18:20,799][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:18:20,799][root][INFO] - => Done in 3.945 ms\n",
      "[2023-08-08 18:18:20,799][root][INFO] - \n",
      "[2023-08-08 18:18:20,799][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:18:21,386][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:18:21,387][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:18:21,387][root][INFO] - => Done in 587.796 ms\n",
      "[2023-08-08 18:18:21,387][root][INFO] - \n",
      "[2023-08-08 18:18:21,387][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:18:21,387][root][INFO] - => Done in 134.945 us\n",
      "[2023-08-08 18:18:21,387][root][INFO] - \n",
      "[2023-08-08 18:18:21,387][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:18:21,387][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:18:22,126][root][INFO] - => Done in 739.310 ms\n",
      "[2023-08-08 18:18:22,127][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▅▂▃▅▂▆▃▂▃▆▃▂▄▇▃▅▁▆▇▂▅▃▄▃▃▇▂▃▆▅▄▃▃▆▅█▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▅▂▄▆▄▆▄▃▄▅▃▃▅▇▄▅▂▅▆▂▄▃▃▃▃▆▁▂▅▄▃▃▂▄▃▆▃▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▃▂▃▁▃▄▅▂▅▅▁▃▆▄▅▃▄▇▃▆▅▆▅▄▅▆▄▅▄▇▄▇██▇▅█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▄▄▂▄▄▃▅▃▁▂▅▂▃▃▅▃▅▂▇▇▃▅▄▄▄▄▆▃▄▆▆▄▄▄▆▅█▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▅▂▃▄▂▅▃▃▃▅▂▂▄▇▃▄▁▄▆▂▄▃▃▃▂▅▁▂▅▃▃▂▂▄▄▆▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▄▄▂▄▄▃▅▃▁▂▅▂▃▃▅▃▅▂▇▇▃▅▄▄▄▄▆▃▄▆▆▄▄▄▆▅█▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▄▄▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▅▃▃▃▃▄▁▂▄▂▃▃▂▂▂▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▄▃▄▃▂▄▂▂▂▄▄▂▃▁▄▃▄▆▅▅▄▅▅▅▅▅▇▇▅█▆▅▇█▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▄▇▅▄▄▃▅▆▅▄▆▆▄▂▆▄▇▄▃▆▅▆▆▆▆▃█▇▄▅▆▆▇▅▆▃▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▂▂▃▇▆▃▇▃▄▇▃▁▃▇▁█▁▃▂▂▆▂▂▂▆▄▆▁▄▄▇▁▂▂▅▃▂▁▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▃▂▂█▆▇█▂▃▆▃▄▄▇▂▄▁▅▄▂▇▁▂▁▄▆▄▃▆▆▆▁▂▂▂▂▃▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▂▁▁▂▂█▅▁▄▂▁▁▇▂▁▄▂▅▁▁▄▁▂▂▂▃▁▂▁▂▃▁▁▂▂▁▂▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▄▄▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▅▃▃▃▃▄▁▂▄▂▃▃▂▂▂▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▄▃▄▃▂▄▂▂▂▄▄▂▃▁▄▃▄▆▅▅▄▅▅▅▅▅▇▇▅█▆▅▇█▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▄▇▅▄▄▃▅▆▅▄▆▆▄▂▆▄▇▄▃▆▅▆▆▆▆▃█▇▄▅▆▆▇▅▆▃▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▂▂▃▇▆▃▇▃▄▇▃▁▃▇▁█▁▃▂▂▆▂▂▂▆▄▆▁▄▄▇▁▂▂▅▃▂▁▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▃▂▂█▆▇█▂▃▆▃▄▄▇▂▄▁▅▄▂▇▁▂▁▄▆▄▃▆▆▆▁▂▂▂▂▃▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▂▁▁▂▂█▅▁▄▂▁▁▇▂▁▄▂▅▁▁▄▁▂▂▂▃▁▂▁▂▃▁▁▂▂▁▂▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▄▄▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▅▃▃▃▃▄▁▂▄▂▃▃▂▂▂▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▃▃▄▄▃▅▃▁▂▄▃▃▃▂▄▄▃▇▆▄▄▄▄▄▅▆▅▆▆█▅▅▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▅▄▄▂▂▂▃▅▂▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▃█▆▃▅▆▄▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▄▇▅▄▄▃▅▆▅▄▆▆▄▂▆▄▇▄▃▆▅▆▆▆▆▃█▇▄▅▆▆▇▅▆▃▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▂▂▃▇▆▃▇▃▄▇▃▁▃▇▁█▁▃▂▂▆▂▂▂▆▄▆▁▄▄▇▁▂▂▅▃▂▁▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▂▂█▇▇▆▂▃▆▃▃▃█▁▃▁▅▅▂▇▁▂▁▄▆▅▁▆▄▆▁▂▂▂▂▂▆▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▁▂▅▂▄█▁▃▅▁▃▄▂▂▃▁▃▁▂▄▁▁▁▂▄▁▅▂▆▅▂▁▁▁▃▅▁▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▂▁▁▂▂█▅▁▄▂▁▁▇▂▁▄▂▅▁▁▄▁▂▂▂▃▁▂▁▂▃▁▁▂▂▁▂▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▄▄▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▅▃▃▃▃▄▁▂▄▂▃▃▂▂▂▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▃▃▄▄▃▅▃▁▂▄▃▃▃▂▄▄▃▇▆▄▄▄▅▄▅▆▅▆▆█▅▅▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▅▄▃▂▂▂▃▅▂▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▃█▆▃▅▆▄▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▄▇▅▄▄▃▅▆▅▄▆▆▄▂▆▄▇▄▃▆▅▆▆▆▆▃█▇▄▅▆▆▇▅▆▃▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▂▂▃▇▆▃▇▃▄▇▃▁▃▇▁█▁▃▂▂▆▂▂▂▆▄▆▁▄▄▇▁▂▂▅▃▂▁▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▂▂█▇▇▆▂▃▆▃▃▃█▁▃▁▅▅▂▇▁▂▁▄▆▅▁▆▄▆▁▂▂▂▂▂▆▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▁▂▅▂▄█▁▃▅▁▃▄▂▂▃▁▃▁▂▄▁▁▁▂▄▁▅▂▆▅▂▁▁▁▃▅▁▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▂▁▁▂▂█▅▁▄▂▁▁▇▂▁▄▂▅▁▁▄▁▂▂▂▃▁▂▁▂▃▁▁▂▂▁▂▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.28633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.49244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.97844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 3.31927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.5195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 3.3355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.76175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 3.3355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.5845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 602.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1169.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 186.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 7.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.5135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/8t8up1qg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181816-8t8up1qg/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:18:55,135][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181856-dvjclvd9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/dvjclvd9\u001b[0m\n",
      "[2023-08-08 18:18:59,945][root][INFO] - => Done in 4.810 s\n",
      "[2023-08-08 18:18:59,945][root][INFO] - \n",
      "[2023-08-08 18:18:59,945][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:18:59,951][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:18:59,951][root][INFO] - => Done in 5.830 ms\n",
      "[2023-08-08 18:18:59,951][root][INFO] - \n",
      "[2023-08-08 18:18:59,951][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:19:00,533][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:19:00,533][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:19:00,533][root][INFO] - => Done in 581.588 ms\n",
      "[2023-08-08 18:19:00,533][root][INFO] - \n",
      "[2023-08-08 18:19:00,533][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:19:00,533][root][INFO] - => Done in 104.904 us\n",
      "[2023-08-08 18:19:00,533][root][INFO] - \n",
      "[2023-08-08 18:19:00,533][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:19:00,533][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:19:01,297][root][INFO] - => Done in 763.422 ms\n",
      "[2023-08-08 18:19:01,297][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.900 MB of 0.903 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇▇▇████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██████████▁▂██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██████████▁▃██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██████████▁▂██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.9976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.9991\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.89867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1978.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/dvjclvd9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181856-dvjclvd9/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:19:35,835][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_181937-czbf2r3g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/czbf2r3g\u001b[0m\n",
      "[2023-08-08 18:19:40,980][root][INFO] - => Done in 5.145 s\n",
      "[2023-08-08 18:19:40,980][root][INFO] - \n",
      "[2023-08-08 18:19:40,980][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:19:40,983][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:19:40,983][root][INFO] - => Done in 2.915 ms\n",
      "[2023-08-08 18:19:40,983][root][INFO] - \n",
      "[2023-08-08 18:19:40,983][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:19:41,578][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:19:41,578][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:19:41,578][root][INFO] - => Done in 595.257 ms\n",
      "[2023-08-08 18:19:41,578][root][INFO] - \n",
      "[2023-08-08 18:19:41,578][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:19:41,578][root][INFO] - => Done in 107.050 us\n",
      "[2023-08-08 18:19:41,579][root][INFO] - \n",
      "[2023-08-08 18:19:41,579][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:19:41,579][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:19:42,370][root][INFO] - => Done in 791.864 ms\n",
      "[2023-08-08 18:19:42,371][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.910 MB of 0.913 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▇▇▇▇▇▆▇▇▅█▇█▆▇▆▇▆▆▅▅▅▅▅▇▄▅▇▇▆▅▅▅▄▇▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██▇▇▇▇▇▆▄▆▅▃▆▅▆▄▅▃▄▃▃▂▂▂▂▂▄▁▂▄▄▃▂▂▂▂▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▇█▇▇▇▇▅▆▆▄▆▅▆▄▅▃▄▃▃▂▂▂▂▂▄▁▂▄▅▄▂▂▂▂▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▆▆▆▆▇▇▇▇▇▇▇██▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██▇▇▇▇▇▇▅▆▆▄▆▅▆▄▅▃▄▃▃▂▂▂▂▂▄▁▂▄▄▃▂▂▂▂▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▆▆▆▆▇▇▇▇▇▇▇██▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▇▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▇▇▇▅█▇▅▅▆▇▇▇▇▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▇▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▇▇▇▅█▇▅▅▆▇▇▇▇▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▄▂▃▃▂▅█▅▂▂▃▃▃▂▂▁▃▁▁▂▂▁▁▂▁▂▁▁▁▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▇▇▇▅█▇▅▅▆▇▇▇▇▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▄▂▃▃▂▅█▅▂▂▃▃▃▂▂▁▃▁▁▂▂▁▁▂▁▂▁▁▁▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▇▇▇▅█▇▅▅▆▇▇▇▇▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.78774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.64276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.6825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.77775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 18.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1835.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 127.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/czbf2r3g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_181937-czbf2r3g/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:20:28,946][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_182030-cg2f73mv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/cg2f73mv\u001b[0m\n",
      "[2023-08-08 18:20:38,513][root][INFO] - => Done in 9.567 s\n",
      "[2023-08-08 18:20:38,513][root][INFO] - \n",
      "[2023-08-08 18:20:38,513][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:20:38,516][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:20:38,516][root][INFO] - => Done in 2.880 ms\n",
      "[2023-08-08 18:20:38,516][root][INFO] - \n",
      "[2023-08-08 18:20:38,516][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:20:39,230][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:20:39,231][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:20:39,231][root][INFO] - => Done in 714.893 ms\n",
      "[2023-08-08 18:20:39,231][root][INFO] - \n",
      "[2023-08-08 18:20:39,231][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:20:39,231][root][INFO] - => Done in 106.812 us\n",
      "[2023-08-08 18:20:39,231][root][INFO] - \n",
      "[2023-08-08 18:20:39,231][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:20:39,231][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:20:40,080][root][INFO] - => Done in 849.332 ms\n",
      "[2023-08-08 18:20:40,080][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.99995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.90091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/cg2f73mv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 77 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_182030-cg2f73mv/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:21:13,707][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_182114-9zltb9m0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/9zltb9m0\u001b[0m\n",
      "[2023-08-08 18:21:18,432][root][INFO] - => Done in 4.725 s\n",
      "[2023-08-08 18:21:18,432][root][INFO] - \n",
      "[2023-08-08 18:21:18,433][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:21:18,436][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:21:18,436][root][INFO] - => Done in 3.624 ms\n",
      "[2023-08-08 18:21:18,436][root][INFO] - \n",
      "[2023-08-08 18:21:18,436][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:21:19,030][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:21:19,030][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:21:19,030][root][INFO] - => Done in 594.064 ms\n",
      "[2023-08-08 18:21:19,030][root][INFO] - \n",
      "[2023-08-08 18:21:19,031][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:21:19,031][root][INFO] - => Done in 106.812 us\n",
      "[2023-08-08 18:21:19,031][root][INFO] - \n",
      "[2023-08-08 18:21:19,031][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:21:19,031][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:21:19,881][root][INFO] - => Done in 850.337 ms\n",
      "[2023-08-08 18:21:19,881][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.915 MB of 0.915 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▇▆▇▄▆▇▆█▆█▆▆▄▇▆█▃▅▆▆▇█▄▆▇█▇▆▆▃▅▇▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▇▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▄▄▅▂▄▅▆▄▃▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▇▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▃▄▅▂▄▄▅▄▃▄▁▃▅▄▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▇▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▄▄▅▂▄▅▅▄▃▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▄▇▅▅▅▃▅▅█▃▃▃▃▃▂▂▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▄▇▅▅▅▃▅▅█▃▃▃▃▃▂▂▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.76535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.63069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.7025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.79175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1853.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 119.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/9zltb9m0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_182114-9zltb9m0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:21:54,051][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_182155-cktuqb3o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/cktuqb3o\u001b[0m\n",
      "[2023-08-08 18:21:58,670][root][INFO] - => Done in 4.619 s\n",
      "[2023-08-08 18:21:58,670][root][INFO] - \n",
      "[2023-08-08 18:21:58,670][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:21:58,674][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:21:58,674][root][INFO] - => Done in 3.697 ms\n",
      "[2023-08-08 18:21:58,674][root][INFO] - \n",
      "[2023-08-08 18:21:58,674][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:21:59,292][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:21:59,292][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:21:59,292][root][INFO] - => Done in 617.910 ms\n",
      "[2023-08-08 18:21:59,292][root][INFO] - \n",
      "[2023-08-08 18:21:59,292][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:21:59,292][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-08 18:21:59,292][root][INFO] - \n",
      "[2023-08-08 18:21:59,292][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:21:59,293][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:22:00,100][root][INFO] - => Done in 807.792 ms\n",
      "[2023-08-08 18:22:00,100][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▇▄▁▄▄▃▆▂▁▃▅▂▂▂▅▃▃▁▅▆▁▃▂▂▂▃▆▂▃▅▄▃▂▃▅▄█▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▄▂▄▅▄▆▃▂▄▅▂▃▃▇▃▄▂▄▆▁▃▂▂▂▂▅▁▂▅▃▃▂▁▃▃▆▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▃▅▄▂▆▃▅▃▅▄▁▃▅▄▃▃▄▆▅▄▄▅▄▅▆▇▆▆▅▆▃▇█▇█▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▄▃▂▄▄▃▅▂▁▃▅▂▃▂▄▃▃▂▆▆▃▄▃▃▃▄▆▃▄▆▆▄▄▄▆▄█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▄▁▃▄▂▅▃▂▃▄▂▁▂▆▃▃▁▄▆▁▃▂▂▁▂▅▂▂▄▃▃▁▂▄▃▆▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▄▃▂▄▄▃▅▂▁▃▅▂▃▂▄▃▃▂▆▆▃▄▃▃▃▄▆▃▄▆▆▄▄▄▆▄█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▅▅▅▅▅▆▅▅▅▄▅▅█▄▅▃▄▅▃▄▃▃▃▃▄▂▂▄▂▃▃▁▂▃▄▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▄▃▄▃▃▄▂▃▃▄▄▃▃▁▄▃▄▅▄▅▄▅▅▅▅▅▇▇▅▇▆▅▇▇▆▇▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▅▇▅▄▅▃▆▇▅▄▇▆▆▂▆▅█▅▃█▆▇▇█▇▄█▇▅▆▆▇█▆▆▃▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▅▅▅▅▅▆▅▅▅▄▅▅█▄▅▃▄▅▃▄▃▃▃▃▄▂▂▄▂▃▃▁▂▃▄▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▄▃▄▃▃▄▂▃▃▄▄▃▃▁▄▃▄▅▄▅▄▅▅▅▅▅▇▇▅▇▆▅▇▇▆▇▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▅▇▅▄▅▃▆▇▅▄▇▆▆▂▆▅█▅▃█▆▇▇█▇▄█▇▅▆▆▇█▆▆▃▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▅▅▅▅▅▆▅▅▅▄▅▅█▄▅▃▄▅▃▄▃▃▃▃▄▂▂▄▂▃▃▁▂▃▄▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▃▂▄▄▃▅▂▁▃▄▃▄▂▂▄▃▄▇▅▄▄▄▄▄▅▆▅▆▆▇▅▅▆▇▅█▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▅▄▄▂▂▃▃▅▃▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▃█▆▄▅▆▄█▇▇▄▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▅▇▅▄▅▃▆▇▅▄▇▆▆▂▆▅█▅▃█▆▇▇█▇▄█▇▅▆▆▇█▆▆▃▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▅▅▅▅▅▆▅▅▅▄▅▅█▄▅▃▄▅▃▄▃▃▃▃▄▂▂▄▂▃▃▁▂▃▄▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▃▂▄▄▃▅▂▁▃▄▃▄▂▂▄▃▄▇▅▄▄▄▄▄▅▆▅▆▆▇▅▅▆▇▅█▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▅▄▄▂▂▃▃▅▃▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▃█▆▄▅▆▄█▇▇▄▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▅▇▅▄▅▃▆▇▅▄▇▆▆▂▆▅█▅▃█▆▇▇█▇▄█▇▅▆▆▇█▆▆▃▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.30583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.48257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 3.33645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 3.361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.77825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 3.361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 604.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1184.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 192.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/cktuqb3o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_182155-cktuqb3o/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:22:33,946][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_182235-fgxu2kh4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/fgxu2kh4\u001b[0m\n",
      "[2023-08-08 18:22:38,971][root][INFO] - => Done in 5.025 s\n",
      "[2023-08-08 18:22:38,971][root][INFO] - \n",
      "[2023-08-08 18:22:38,971][root][INFO] - => Env setup ...\n",
      "[2023-08-08 18:22:38,975][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 18:22:38,975][root][INFO] - => Done in 4.124 ms\n",
      "[2023-08-08 18:22:38,975][root][INFO] - \n",
      "[2023-08-08 18:22:38,975][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 18:22:39,537][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 18:22:39,537][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 18:22:39,537][root][INFO] - => Done in 561.917 ms\n",
      "[2023-08-08 18:22:39,537][root][INFO] - \n",
      "[2023-08-08 18:22:39,537][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 18:22:39,537][root][INFO] - => Done in 108.004 us\n",
      "[2023-08-08 18:22:39,538][root][INFO] - \n",
      "[2023-08-08 18:22:39,538][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 18:22:39,538][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 18:22:40,260][root][INFO] - => Done in 721.985 ms\n",
      "[2023-08-08 18:22:40,260][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.905 MB of 0.905 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▅▆▆▇▇▇▇▇█▇█▇████████▇███▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ████▅▆▆█▆█▃▃█▄█▄█▆▃█████▄▆▆█▁█▆▃█████▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ████▂▄▃█▄▇▇▇█▁█▁█▄▇█████▁▄▄█▃█▄▇█████▄██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▅▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ████▃▄▃█▅▇▅▅█▁█▂█▅▅█████▁▅▅█▁█▅▅█████▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▅▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▅▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▇▅▆▁▅▂▁▁▁█▁█▁▅▁▁▁▁▁▁█▅▅▁▅▁▅▁▁▁▁▁▁▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▅▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▇▅▆▁▅▂▁▁▁█▁█▁▅▁▁▁▁▁▁█▅▅▁▅▁▅▁▁▁▁▁▁▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▅▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▇▅▆▁▅▂▁▁▁█▁█▁▅▁▁▁▁▁▁█▅▅▁▅▁▅▁▁▁▁▁▁▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▅▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▇▅▆▁▅▂▁▁▁█▁█▁▅▁▁▁▁▁▁█▅▅▁▅▁▅▁▁▁▁▁▁▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.9815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.99497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99235\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.8908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.991\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.98425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1970.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 9.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1970\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/fgxu2kh4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_182235-fgxu2kh4/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:25:35,117][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "Problem at: /Users/alexandrasouly/code/pax/pax/experiment.py 110 global_setup\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1108, in init\n",
      "    run = wi.init()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 742, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "[2023-08-08 18:59:09,643][urllib3.connectionpool][WARNING] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x16ae25310>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "[2023-08-08 18:59:09,645][urllib3.connectionpool][WARNING] - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1798371c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "[2023-08-08 18:59:09,646][urllib3.connectionpool][WARNING] - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x179837190>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "[2023-08-08 18:59:09,649][urllib3.connectionpool][WARNING] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x179837b80>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "[2023-08-08 18:59:09,650][urllib3.connectionpool][WARNING] - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x178e9f820>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "[2023-08-08 18:59:09,650][urllib3.connectionpool][WARNING] - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x178e9f6d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /api/5288891/store/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n",
      "[2023-08-08 18:59:11,654][root][INFO] - => Done in 33.609 m\n",
      "[2023-08-08 18:59:11,655][root][INFO] - \n",
      "Error executing job with overrides: ['+experiment=multi-shapers/comp/n3pl_2shap_tc_comp_eval', '++run_path1=ucl-dark/tensor-tc/8v8vokho', '++run_path2=ucl-dark/tensor-tc/0lg7du3n', '++model_path1=exp/3pl-2shap-tc/3pl_2shap_tc_3/2023-08-02_14.38.10.878306/generation_299_agent_0', '++model_path2=exp/3pl-2shap-tc/3pl_2shap_tc_2/2023-08-02_05.06.31.755800/generation_299_agent_1', '++wandb.name=3pl_2shap_tc_s3a1_s2a2_eval', '++wandb.group=3pl_2shap_tc-comp-nice_v_nice']\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1108, in init\n",
      "    run = wi.init()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 742, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/experiment.py\", line 780, in main\n",
      "    save_dir = global_setup(args)\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/experiment.py\", line 110, in global_setup\n",
      "    wandb.init(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1145, in init\n",
      "    raise Exception(\"problem\") from error_seen\n",
      "Exception: problem\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 18:59:15,831][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a1_s3a1_eval\n",
      "[2023-08-08 19:15:53,229][wandb.sdk.lib.retry][INFO] - Retry attempt failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/util/connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connection.py\", line 358, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x1628c5850>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1628c5850>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/lib/retry.py\", line 128, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/internal/internal_api.py\", line 241, in execute\n",
      "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/transport/requests.py\", line 38, in execute\n",
      "    request = requests.post(self.url, **post_args)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/adapters.py\", line 565, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1628c5850>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "Problem at: /Users/alexandrasouly/code/pax/pax/experiment.py 110 global_setup\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1108, in init\n",
      "    run = wi.init()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 742, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n",
      "[2023-08-08 19:43:10,597][root][INFO] - => Done in 43.913 m\n",
      "[2023-08-08 19:43:10,597][root][INFO] - \n",
      "Error executing job with overrides: ['+experiment=multi-shapers/comp/n3pl_2shap_tc_comp_eval', '++run_path1=ucl-dark/tensor-tc/8v8vokho', '++run_path2=ucl-dark/tensor-tc/8v8vokho', '++model_path1=exp/3pl-2shap-tc/3pl_2shap_tc_3/2023-08-02_14.38.10.878306/generation_299_agent_0', '++model_path2=exp/3pl-2shap-tc/3pl_2shap_tc_3/2023-08-02_14.38.10.878306/generation_299_agent_0', '++wandb.name=3pl_2shap_tc_s3a1_s3a1_eval', '++wandb.group=3pl_2shap_tc-comp-nice_v_nice']\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1108, in init\n",
      "    run = wi.init()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 742, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/experiment.py\", line 780, in main\n",
      "    save_dir = global_setup(args)\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/experiment.py\", line 110, in global_setup\n",
      "    wandb.init(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1145, in init\n",
      "    raise Exception(\"problem\") from error_seen\n",
      "Exception: problem\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:43:15,369][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194317-swn50fjp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/swn50fjp\u001b[0m\n",
      "[2023-08-08 19:43:24,079][root][INFO] - => Done in 8.710 s\n",
      "[2023-08-08 19:43:24,080][root][INFO] - \n",
      "[2023-08-08 19:43:24,080][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:43:24,089][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:43:24,089][root][INFO] - => Done in 9.391 ms\n",
      "[2023-08-08 19:43:24,090][root][INFO] - \n",
      "[2023-08-08 19:43:24,090][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:43:24,746][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:43:24,747][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:43:24,747][root][INFO] - => Done in 657.003 ms\n",
      "[2023-08-08 19:43:24,747][root][INFO] - \n",
      "[2023-08-08 19:43:24,747][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:43:24,747][root][INFO] - => Done in 119.925 us\n",
      "[2023-08-08 19:43:24,747][root][INFO] - \n",
      "[2023-08-08 19:43:24,747][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:43:24,747][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:43:25,545][root][INFO] - => Done in 798.293 ms\n",
      "[2023-08-08 19:43:25,546][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.913 MB of 0.925 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▆▅▇▄▆▇▆█▅█▆▆▄█▆▇▃▅▆▅▆▇▄▅▇█▇▆▆▃▅█▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▇▇▇▆▆▃▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▃▄▅▂▃▄▅▄▃▄▁▃▅▄▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▇▆▆▅▆▄▅▆▄▆▃▆▃▄▂▅▄▄▂▃▄▂▃▄▂▂▃▄▄▃▄▁▃▅▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▆▆▆▆▇▆▇▇▇█▇█▇▇▇█▇█▇▇▇▇██▇▇███▇█▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▇▇▆▅▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▃▄▅▂▃▄▅▄▃▄▁▃▅▄▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▆▆▆▆▇▆▇▇▇█▇█▇▇▇█▇█▇▇▇▇██▇▇███▇█▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▆▆▆▇▆▇▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▆▅▅▁▁▂▁▁▁▇▁▆▁▄▁▁▄▁▁▁█▅▅▁█▅▅▁▁▁▁▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▆▆▆▇▆▇▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▆▅▅▁▁▂▁▁▁▇▁▆▁▄▁▁▄▁▁▁█▅▅▁█▅▅▁▁▁▁▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▄▇▅▅▆▃▅▆█▃▃▃▃▃▂▂▁▃▁▁▁▁▁▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▆▅▅▁▁▂▁▁▁▇▁▆▁▄▁▁▄▁▁▁█▅▅▁█▅▅▁▁▁▁▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇█▇██▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▄▇▅▅▆▃▅▆█▃▃▃▃▃▂▂▁▃▁▁▁▁▁▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▆▅▅▁▁▂▁▁▁▇▁▆▁▄▁▁▄▁▁▁█▅▅▁█▅▅▁▁▁▁▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.76332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.90365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.62182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.7145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.79775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1851.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 111.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/swn50fjp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194317-swn50fjp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:44:00,926][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194402-12zfwgwn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/12zfwgwn\u001b[0m\n",
      "[2023-08-08 19:44:06,997][root][INFO] - => Done in 6.071 s\n",
      "[2023-08-08 19:44:06,997][root][INFO] - \n",
      "[2023-08-08 19:44:06,997][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:44:07,002][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:44:07,002][root][INFO] - => Done in 4.314 ms\n",
      "[2023-08-08 19:44:07,002][root][INFO] - \n",
      "[2023-08-08 19:44:07,002][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:44:07,554][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:44:07,554][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:44:07,555][root][INFO] - => Done in 552.756 ms\n",
      "[2023-08-08 19:44:07,555][root][INFO] - \n",
      "[2023-08-08 19:44:07,555][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:44:07,555][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-08 19:44:07,555][root][INFO] - \n",
      "[2023-08-08 19:44:07,555][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:44:07,555][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:44:08,281][root][INFO] - => Done in 726.073 ms\n",
      "[2023-08-08 19:44:08,281][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.915 MB of 0.927 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▇▄▁▄▅▃▆▃▁▃▅▂▂▃▆▃▂▂▅▆▁▄▃▂▃▄▆▂▃▅▅▄▃▃▅▅█▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▄▂▄▅▄▆▄▃▄▅▃▃▄█▃▄▂▅▆▁▄▃▂▂▃▅▁▂▄▄▃▂▂▃▃▆▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▃▅▄▂▅▃▅▃▅▄▁▃▅▄▂▄▄▆▄▄▅▅▅▅▆▇▆▅▅▆▃▆▇██▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▄▃▂▄▄▃▅▃▁▃▅▂▃▃▄▃▃▃▆▆▃▄▄▃▄▄▆▄▅▆▆▄▄▄▆▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▅▂▄▅▃▅▃▃▃▅▂▂▃▇▃▃▂▄▆▁▃▃▂▂▃▅▂▃▄▃▃▂▃▃▄▆▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▄▃▂▄▄▃▅▃▁▃▅▂▃▃▄▃▃▃▆▆▃▄▄▃▄▄▆▄▅▆▆▄▄▄▆▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▄▅▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▂▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▂▄▃▄▃▃▄▂▃▃▄▄▃▂▁▄▃▅▆▅▆▄▅▅▅▆▆▇▇▅▇▆▅▇█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▅▇▅▄▅▃▅▇▅▄▇▆▅▂▆▆▇▄▃█▅▆▇▇▆▄█▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▂█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▃█▅█▁▃▅▁▁▅▃▁▃▁▃▁▁▁▁▁▁▃▃▁▁▃▃▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁█▃▁▆▁▁▁▅▁▁▃▁▃▁▁▅▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▄▅▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▂▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▂▄▃▄▃▃▄▂▃▃▄▄▃▂▁▄▃▅▆▅▆▄▅▅▅▆▆▇▇▅▇▆▅▇█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▅▇▅▄▅▃▅▇▅▄▇▆▅▂▆▆▇▅▃█▅▆▇▇▆▄█▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▂█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▃█▅█▁▃▅▁▁▅▃▁▃▁▃▁▁▁▁▁▁▃▃▁▁▃▃▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁█▃▁▆▁▁▁▅▁▁▃▁▃▁▁▅▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▄▅▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▂▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▃▂▄▄▃▅▂▁▃▄▃▄▂▂▄▃▄▇▅▄▄▅▄▅▅▆▅▆▆█▅▆▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▃▅▄▄▃▂▃▃▅▃▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▄█▆▄▅▆▄▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▅▇▅▄▅▃▅▇▅▄▇▆▅▂▆▆▇▄▃█▅▆▇▇▆▄█▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▂█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁█▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▃▃▆█▁▃▃▁▁▆▃▁▃▁▃▁▁▁▁▁▁▁▃▁▁▃▃▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁█▃▁▆▁▁▁▅▁▁▃▁▃▁▁▅▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▄▅▅▅▅▅▅▅▅▄▅▅█▄▅▃▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▂▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▃▂▄▄▃▅▂▁▃▄▃▄▂▂▄▃▄▇▅▄▄▅▄▅▅▆▅▆▆█▅▆▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▃▅▄▄▃▂▃▃▅▃▃▅▂▃▁▄▂▅▃▃▆▄▅▆▅▅▄█▆▄▅▆▄▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▅▇▅▄▅▃▅▇▅▄▇▆▅▂▆▆▇▅▃█▅▆▇▇▆▄█▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▂█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁█▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▃▃▆█▁▃▃▁▁▆▃▁▃▁▃▁▁▁▁▁▁▁▃▁▁▃▃▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁█▃▁▆▁▁▁▅▁▁▃▁▃▁▁▅▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.28633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.48324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.9953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 3.33552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 3.3445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.75725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 3.3445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.2975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.5905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 595.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1181.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 202.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.2975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.5135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.077\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/12zfwgwn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194402-12zfwgwn/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:44:43,781][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194445-l8l4lvcx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/l8l4lvcx\u001b[0m\n",
      "[2023-08-08 19:44:49,537][root][INFO] - => Done in 5.756 s\n",
      "[2023-08-08 19:44:49,537][root][INFO] - \n",
      "[2023-08-08 19:44:49,537][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:44:49,541][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:44:49,541][root][INFO] - => Done in 3.160 ms\n",
      "[2023-08-08 19:44:49,541][root][INFO] - \n",
      "[2023-08-08 19:44:49,541][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:44:50,078][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:44:50,078][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:44:50,078][root][INFO] - => Done in 537.551 ms\n",
      "[2023-08-08 19:44:50,078][root][INFO] - \n",
      "[2023-08-08 19:44:50,078][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:44:50,079][root][INFO] - => Done in 108.004 us\n",
      "[2023-08-08 19:44:50,079][root][INFO] - \n",
      "[2023-08-08 19:44:50,079][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:44:50,079][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:44:50,799][root][INFO] - => Done in 720.296 ms\n",
      "[2023-08-08 19:44:50,799][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.916 MB of 0.928 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇▇▇████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██████████▁▂██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██████████▁▃██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██████████▁▂██████▁█████████▁██▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.99751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.89838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.9985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.9985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/l8l4lvcx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194445-l8l4lvcx/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:45:25,010][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194526-c3gcvh44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/c3gcvh44\u001b[0m\n",
      "[2023-08-08 19:45:30,943][root][INFO] - => Done in 5.933 s\n",
      "[2023-08-08 19:45:30,943][root][INFO] - \n",
      "[2023-08-08 19:45:30,943][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:45:30,947][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:45:30,947][root][INFO] - => Done in 3.179 ms\n",
      "[2023-08-08 19:45:30,947][root][INFO] - \n",
      "[2023-08-08 19:45:30,947][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:45:31,504][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:45:31,504][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:45:31,504][root][INFO] - => Done in 557.494 ms\n",
      "[2023-08-08 19:45:31,504][root][INFO] - \n",
      "[2023-08-08 19:45:31,504][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:45:31,504][root][INFO] - => Done in 113.010 us\n",
      "[2023-08-08 19:45:31,505][root][INFO] - \n",
      "[2023-08-08 19:45:31,505][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:45:31,505][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:45:32,259][root][INFO] - => Done in 754.774 ms\n",
      "[2023-08-08 19:45:32,260][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.917 MB of 0.929 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▇▇▇▇▇▆▇▇▅█▇█▆▇▆▇▆▆▅▅▅▅▅▇▄▅▇▇▆▅▅▅▄▇▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██▇▇▇▇▇▆▄▆▅▃▆▅▆▄▅▃▄▃▃▂▂▃▂▂▄▁▂▄▄▃▂▂▂▁▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▇▇▇▇▇▇▅▆▆▃▆▅▆▄▅▃▄▃▃▂▂▃▂▂▄▁▂▄▄▃▂▂▂▁▄▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▆▆▆▆▇▇▇▇▇▇▇██▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██▇▇▇▇▇▇▅▆▅▃▆▅▆▄▅▃▄▃▃▂▂▃▂▂▄▁▂▄▄▃▂▂▂▁▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▆▆▆▆▇▇▇▇▇▇▇██▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▇▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▇▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▃▂▂▄▂▅█▅▂▂▃▃▃▂▂▁▃▁▁▂▂▁▂▂▁▂▂▁▁▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇███████▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▃▂▂▄▂▅█▅▂▂▃▃▃▂▂▁▃▁▁▂▂▁▂▂▁▂▂▁▁▂▂▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▂▂▂▂▂▃▅▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.7355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.7875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.6426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1835.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 128.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/c3gcvh44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194526-c3gcvh44/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:46:07,450][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194609-oimg4lh2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/oimg4lh2\u001b[0m\n",
      "[2023-08-08 19:46:13,661][root][INFO] - => Done in 6.211 s\n",
      "[2023-08-08 19:46:13,661][root][INFO] - \n",
      "[2023-08-08 19:46:13,661][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:46:13,665][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:46:13,665][root][INFO] - => Done in 3.834 ms\n",
      "[2023-08-08 19:46:13,665][root][INFO] - \n",
      "[2023-08-08 19:46:13,665][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:46:14,221][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:46:14,221][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:46:14,221][root][INFO] - => Done in 556.463 ms\n",
      "[2023-08-08 19:46:14,221][root][INFO] - \n",
      "[2023-08-08 19:46:14,221][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:46:14,222][root][INFO] - => Done in 123.262 us\n",
      "[2023-08-08 19:46:14,222][root][INFO] - \n",
      "[2023-08-08 19:46:14,222][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:46:14,222][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:46:14,975][root][INFO] - => Done in 753.760 ms\n",
      "[2023-08-08 19:46:14,976][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.930 MB of 0.930 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██████████████████▁█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.99986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.9997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.90055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.9985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.9985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/oimg4lh2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194609-oimg4lh2/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:46:50,665][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194652-ogf0jb4w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ogf0jb4w\u001b[0m\n",
      "[2023-08-08 19:46:56,533][root][INFO] - => Done in 5.867 s\n",
      "[2023-08-08 19:46:56,533][root][INFO] - \n",
      "[2023-08-08 19:46:56,533][root][INFO] - => Env setup ...\n",
      "[2023-08-08 19:46:56,536][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 19:46:56,536][root][INFO] - => Done in 3.110 ms\n",
      "[2023-08-08 19:46:56,536][root][INFO] - \n",
      "[2023-08-08 19:46:56,536][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 19:46:57,110][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 19:46:57,111][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 19:46:57,111][root][INFO] - => Done in 574.650 ms\n",
      "[2023-08-08 19:46:57,111][root][INFO] - \n",
      "[2023-08-08 19:46:57,111][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 19:46:57,111][root][INFO] - => Done in 108.004 us\n",
      "[2023-08-08 19:46:57,111][root][INFO] - \n",
      "[2023-08-08 19:46:57,111][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 19:46:57,111][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 19:46:57,864][root][INFO] - => Done in 752.591 ms\n",
      "[2023-08-08 19:46:57,864][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.919 MB of 0.931 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▅▆▆▇▆▆▄▆▇▆█▆█▆▆▄▇▆█▃▅▆▆▇█▄▆▆█▇▆▆▃▅▇▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▇▇▇▆▆▃▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▄▄▅▂▄▄▆▄▃▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▇▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▃▄▅▂▄▃▅▄▃▄▁▃▅▄▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▇▇▇▆▆▄▅▆▄▆▄▆▄▄▂▅▄▅▂▃▄▄▄▅▂▄▄▅▄▃▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▂▂▂▂▂▃▃▆▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▂▂▂▂▂▃▃▆▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▅▇▅▅▆▃▅▆█▃▃▃▃▃▂▂▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▂▂▂▂▂▃▃▆▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▅▇▅▅▆▃▅▆█▃▃▃▃▃▂▂▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▂▂▂▂▂▃▃▆▄▃▅▃▅▃▅▅▇▄▅▄▇▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.759\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.76514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.90985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.63009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.69\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.7935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.69\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1852.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 118.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ogf0jb4w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194652-ogf0jb4w/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 19:47:33,451][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_194734-3vrdsmzf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/3vrdsmzf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "[2023-08-08 20:01:38,415][root][INFO] - => Done in 14.083 m\n",
      "[2023-08-08 20:01:38,415][root][INFO] - \n",
      "[2023-08-08 20:01:38,415][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:01:38,421][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:01:38,421][root][INFO] - => Done in 5.476 ms\n",
      "[2023-08-08 20:01:38,421][root][INFO] - \n",
      "[2023-08-08 20:01:38,421][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:01:39,011][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:01:39,012][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:01:39,012][root][INFO] - => Done in 590.822 ms\n",
      "[2023-08-08 20:01:39,012][root][INFO] - \n",
      "[2023-08-08 20:01:39,012][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:01:39,012][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-08 20:01:39,012][root][INFO] - \n",
      "[2023-08-08 20:01:39,012][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:01:39,012][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:01:39,795][root][INFO] - => Done in 783.444 ms\n",
      "[2023-08-08 20:01:39,796][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.932 MB of 0.932 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▇▅▁▄▅▃▆▂▁▃▅▂▂▃▆▃▄▁▆▆▂▄▃▂▃▃▆▂▃▅▅▄▃▃▅▅█▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▅▂▄▅▄▆▃▃▄▅▂▃▄█▃▄▁▅▆▁▄▃▂▂▃▅▁▂▅▃▃▃▂▄▃▆▂▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▃▅▅▂▅▃▅▃▅▄▁▃▅▄▃▂▅▆▄▄▅▄▄▅▆▇▆▅▅▆▄▇▇██▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▄▄▂▄▄▄▅▃▁▃▅▃▃▃▄▃▄▃▇▆▃▄▄▃▄▄▆▄▅▅▆▄▅▄▆▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▅▂▄▅▃▅▃▃▃▅▂▂▄▇▃▄▁▅▆▁▄▃▂▂▃▅▂▃▅▄▄▃▂▄▄▆▃▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▄▄▂▄▄▄▅▃▁▃▅▃▃▃▄▃▄▃▇▆▃▄▄▃▄▄▆▄▅▅▆▄▅▄▆▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▄▄▅▅▅▅▅▅▅▄▄▅█▄▅▂▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▁▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▂▄▃▄▄▃▄▃▂▃▄▄▃▃▁▄▃▅▆▅▅▄▅▅▆▅▆▇▇▅▇▆▆▇█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▄▇▅▄▅▃▅▆▅▄▇▆▅▂▆▅█▄▃█▅▆▇▇▆▄█▇▄▆▆▆▇▅▆▃▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▄▂▂▅▂▄▄▁▂▄▄▁▁▂▅▂▁▁▁▁▁▄▄▁▁▂▁█▁▂▄▂▅▁▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▄▁▃▃▃▅▆▄▄▅▃▃▅▃▂▃▃█▁▃▃▃▁▂▄▃▁▁▅▅▃▁▂▁▇▂▁▄▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▂▂▇▂▅▃▁▂▁█▁▂▂▇█▂▁▃▁▁▂▂▃▁▁▁▂▅▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▄▄▅▅▅▅▅▅▅▄▄▅█▄▅▂▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▁▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▂▄▃▄▄▃▄▃▂▃▄▄▃▃▁▄▃▅▆▅▅▄▅▅▆▅▆▇▇▅▇▆▆▇█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▄▇▅▄▅▃▅▆▅▄▇▆▅▂▆▅█▄▃█▅▆▇▇▆▄█▇▄▆▆▆▇▅▆▃▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▄▂▂▅▂▄▄▁▂▄▄▁▁▂▅▂▁▁▁▁▁▄▄▁▁▂▁█▁▂▄▂▅▁▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▄▁▃▃▃▅▆▄▄▅▃▃▅▃▂▃▃█▁▃▃▃▁▂▄▃▁▁▅▅▃▁▂▁▇▂▁▄▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▂▂▇▂▅▃▁▂▁█▁▂▂▇█▂▁▃▁▁▂▂▃▁▁▁▂▅▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▄▄▅▅▅▅▅▅▅▄▄▅█▄▅▂▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▁▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▂▃▃▄▄▃▅▃▁▃▄▃▄▃▂▄▄▄▇▆▅▄▅▄▅▅▆▅▆▅█▅▆▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▃▅▄▄▃▂▃▃▅▃▃▅▂▃▁▄▃▅▃▃▆▄▅▆▅▅▄█▇▃▅▆▄▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▄▇▅▄▅▃▅▆▅▄▇▆▅▂▆▅█▄▃█▅▆▇▇▆▄█▇▄▆▆▆▇▅▆▃▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▄▂▂▅▂▄▄▁▂▄▄▁▁▂▅▂▁▁▁▁▁▄▄▁▁▂▁█▁▂▄▂▅▁▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▁▃▂▃▅▆▅▃▃▃▂▃▃▁▂▂▅▁▂▁▁▁▂▃▂▁▁▅▁▂▁▂▁█▂▁▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▅▁▁▃▁▃▃▁▃▅▁▃▅▁▃▃▃█▁▃▅▅▁▁▃▃▁▁▃█▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▂▂▇▂▅▃▁▂▁█▁▂▂▇█▂▁▃▁▁▂▂▃▁▁▁▂▅▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▄▄▅▅▅▅▅▅▅▄▄▅█▄▅▂▄▅▂▄▃▃▃▃▄▁▂▄▂▃▃▁▂▂▃▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▂▃▃▄▄▃▅▃▁▃▄▃▄▃▂▄▄▄▇▆▅▄▅▄▅▅▆▅▆▅█▅▆▆▇▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▃▅▄▄▃▂▃▃▅▃▃▅▂▃▁▄▃▅▃▃▆▄▅▆▅▅▄█▇▃▅▆▅▇▇▇▄▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▄▇▅▄▅▃▅▆▅▄▇▆▅▂▆▅█▄▃█▅▆▇▇▆▄█▇▄▆▆▆▇▅▆▃▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▄▂▂▅▂▄▄▁▂▄▄▁▁▂▅▂▁▁▁▁▁▄▄▁▁▂▁█▁▂▄▂▅▁▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▁▃▂▃▅▆▅▃▃▃▂▃▃▁▂▂▅▁▂▁▁▁▂▃▂▁▁▅▁▂▁▂▁█▂▁▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▅▁▁▃▁▃▃▁▃▅▁▃▅▁▃▃▃█▁▃▅▅▁▁▃▃▁▁▃█▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▂▂▇▂▅▃▁▂▁█▁▂▂▇█▂▁▃▁▁▂▂▃▁▁▁▂▅▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.31133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.48745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 3.33233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.5185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 3.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.7795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 3.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 590.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1196.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 190.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 590\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 190\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/3vrdsmzf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_194734-3vrdsmzf/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:02:18,422][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200219-84hb8tow\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/84hb8tow\u001b[0m\n",
      "[2023-08-08 20:02:24,877][root][INFO] - => Done in 6.456 s\n",
      "[2023-08-08 20:02:24,877][root][INFO] - \n",
      "[2023-08-08 20:02:24,877][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:02:24,881][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:02:24,881][root][INFO] - => Done in 3.784 ms\n",
      "[2023-08-08 20:02:24,881][root][INFO] - \n",
      "[2023-08-08 20:02:24,881][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:02:25,450][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:02:25,451][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:02:25,451][root][INFO] - => Done in 569.192 ms\n",
      "[2023-08-08 20:02:25,451][root][INFO] - \n",
      "[2023-08-08 20:02:25,451][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:02:25,451][root][INFO] - => Done in 109.911 us\n",
      "[2023-08-08 20:02:25,451][root][INFO] - \n",
      "[2023-08-08 20:02:25,451][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:02:25,451][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:02:26,209][root][INFO] - => Done in 757.936 ms\n",
      "[2023-08-08 20:02:26,209][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.921 MB of 0.933 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▅▆▆▇▇▇▇▇████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ████▄█▇█▅█▁▂██████▁█████████▁█▅▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ████▁█▄█▃█▆▇██████▆█████████▆█▃▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ████▁█▅█▃█▃▄██████▃█████████▃█▃▃████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁█▇▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.99714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.89728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/84hb8tow\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200219-84hb8tow/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:03:01,299][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200302-ah5saqxs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ah5saqxs\u001b[0m\n",
      "[2023-08-08 20:03:07,187][root][INFO] - => Done in 5.887 s\n",
      "[2023-08-08 20:03:07,187][root][INFO] - \n",
      "[2023-08-08 20:03:07,187][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:03:07,191][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:03:07,191][root][INFO] - => Done in 3.186 ms\n",
      "[2023-08-08 20:03:07,191][root][INFO] - \n",
      "[2023-08-08 20:03:07,191][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:03:07,742][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:03:07,742][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:03:07,742][root][INFO] - => Done in 551.682 ms\n",
      "[2023-08-08 20:03:07,743][root][INFO] - \n",
      "[2023-08-08 20:03:07,743][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:03:07,743][root][INFO] - => Done in 148.058 us\n",
      "[2023-08-08 20:03:07,743][root][INFO] - \n",
      "[2023-08-08 20:03:07,743][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:03:07,743][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:03:08,498][root][INFO] - => Done in 755.408 ms\n",
      "[2023-08-08 20:03:08,498][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.922 MB of 0.934 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▇▆▇▇▇▆▇▇▅█▇█▆▇▆▇▆▆▅▅▅▅▅▇▄▅▇▇▆▅▅▅▄▇▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ██▇▇▇▇▇▆▅▆▅▃▆▅▆▄▅▃▄▃▃▂▂▃▂▂▄▁▂▄▄▃▂▂▂▁▄▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▇▇▆▇▆▇▅▆▆▃▆▅▆▄▅▃▄▃▃▂▂▂▂▁▄▁▂▄▃▃▂▂▁▁▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ██▇▇▇▇▆▇▅▆▆▃▆▅▆▄▅▃▄▃▃▂▂▃▂▂▄▁▂▄▄▃▂▂▂▁▄▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇▇▇▇▇▇█▇▇███▇▇▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇██████▇▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▂▂▂▂▂▃▄▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▆▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▆▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇██████▇▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▂▂▂▂▂▃▄▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▆▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▆▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇██████▇▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▃▂▂▄▂▅█▅▂▂▃▄▃▂▂▁▃▁▁▂▂▁▁▂▁▂▂▁▁▂▁▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▂▂▂▂▂▃▄▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▆▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▆▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇▇▇▇██▇█▇███▇▇▇▇██▇██████▇▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▃▂▂▄▂▅█▅▂▂▃▄▃▂▂▁▃▁▁▂▂▁▁▂▁▂▂▁▁▂▁▁▁▂▁▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▂▂▂▂▂▃▄▃▄▆▃▄▃▅▄▆▅▆▆▇▇▆▇▇▅█▇▅▅▆▇▇▇█▅▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▆▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▆▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.78778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.64089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.6825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.77775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 18.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1835.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 127.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ah5saqxs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200302-ah5saqxs/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:03:43,459][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200344-vkj8dcc6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/vkj8dcc6\u001b[0m\n",
      "[2023-08-08 20:03:49,467][root][INFO] - => Done in 6.008 s\n",
      "[2023-08-08 20:03:49,467][root][INFO] - \n",
      "[2023-08-08 20:03:49,467][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:03:49,471][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:03:49,471][root][INFO] - => Done in 3.501 ms\n",
      "[2023-08-08 20:03:49,471][root][INFO] - \n",
      "[2023-08-08 20:03:49,471][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:03:50,027][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:03:50,027][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:03:50,027][root][INFO] - => Done in 555.794 ms\n",
      "[2023-08-08 20:03:50,027][root][INFO] - \n",
      "[2023-08-08 20:03:50,027][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:03:50,027][root][INFO] - => Done in 112.295 us\n",
      "[2023-08-08 20:03:50,027][root][INFO] - \n",
      "[2023-08-08 20:03:50,027][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:03:50,027][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:03:50,783][root][INFO] - => Done in 756.264 ms\n",
      "[2023-08-08 20:03:50,784][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.923 MB of 0.935 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▅▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ████▄█▇█▅█████████▁███████████▅█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ████▁█▄█▃█████████▆███████████▃█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ████▁█▅█▃█████████▃███████████▃█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▆▆▆▆▆▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.99879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.89975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1978.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/vkj8dcc6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200344-vkj8dcc6/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:04:25,896][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200427-jyytpk5q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/jyytpk5q\u001b[0m\n",
      "[2023-08-08 20:04:31,673][root][INFO] - => Done in 5.777 s\n",
      "[2023-08-08 20:04:31,673][root][INFO] - \n",
      "[2023-08-08 20:04:31,673][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:04:31,676][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:04:31,676][root][INFO] - => Done in 2.963 ms\n",
      "[2023-08-08 20:04:31,676][root][INFO] - \n",
      "[2023-08-08 20:04:31,676][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:04:32,231][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:04:32,231][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:04:32,231][root][INFO] - => Done in 554.971 ms\n",
      "[2023-08-08 20:04:32,231][root][INFO] - \n",
      "[2023-08-08 20:04:32,231][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:04:32,231][root][INFO] - => Done in 134.230 us\n",
      "[2023-08-08 20:04:32,231][root][INFO] - \n",
      "[2023-08-08 20:04:32,231][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:04:32,231][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:04:32,981][root][INFO] - => Done in 749.883 ms\n",
      "[2023-08-08 20:04:32,982][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.930 MB of 0.936 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▅▆▆▆▇▅▇▄▆▇▆█▆█▆▆▄█▆█▃▅▆▆▇█▄▆▇█▇▆▆▂▅█▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▇▇▇▇▇▆▆▄▅▆▄▆▅▆▄▄▃▅▄▅▂▃▄▄▄▅▂▄▅▅▄▄▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▇▇▇▆▇▅▇▄▆▆▅▆▅▆▅▄▃▅▄▅▂▄▄▄▅▅▃▄▄▅▅▄▄▁▃▅▅▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▇▇▇▇▇▆▆▄▅▆▄▆▅▆▄▄▃▅▄▅▂▃▄▄▄▅▂▄▄▅▅▄▄▁▃▅▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▆▇▇▇██▇▇███▇▇▆▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄█▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▁▁▁█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▁▁▁▇▁▁▁▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄█▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▁▁▁█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▁▁▁▇▁▁▁▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▄▇▅▅▅▃▅▅█▃▃▃▃▃▂▂▁▃▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄█▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▁▁▁▇▁▁▁▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▅▅▅▆▆▆▆▆▇▇▇▇▇█▇█▇███▇▇████▇██████▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▄▇▅▅▅▃▅▅█▃▃▃▃▃▂▂▁▃▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▂▂▂▂▂▃▃▅▄▃▅▃▅▃▅▅▆▄▅▄█▆▅▅▅▄▇▅▄▃▅▆▅█▆▄▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▁▁▁▇▁▁▁▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.76423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.90749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 4.62737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.7025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 4.6905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.79175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 4.6905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.9265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1853.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 119.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.9265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/jyytpk5q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200427-jyytpk5q/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:05:08,238][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200509-73duypxp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/73duypxp\u001b[0m\n",
      "[2023-08-08 20:05:14,183][root][INFO] - => Done in 5.945 s\n",
      "[2023-08-08 20:05:14,183][root][INFO] - \n",
      "[2023-08-08 20:05:14,183][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:05:14,186][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:05:14,187][root][INFO] - => Done in 3.258 ms\n",
      "[2023-08-08 20:05:14,187][root][INFO] - \n",
      "[2023-08-08 20:05:14,187][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:05:14,744][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:05:14,744][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:05:14,744][root][INFO] - => Done in 557.538 ms\n",
      "[2023-08-08 20:05:14,744][root][INFO] - \n",
      "[2023-08-08 20:05:14,744][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:05:14,745][root][INFO] - => Done in 113.964 us\n",
      "[2023-08-08 20:05:14,745][root][INFO] - \n",
      "[2023-08-08 20:05:14,745][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:05:14,745][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:05:15,529][root][INFO] - => Done in 784.171 ms\n",
      "[2023-08-08 20:05:15,529][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▆▂▆▅▃▇▄▃▃▆▃▃▅▇▃▄▁▇▇▂▅▄▃▄▅▇▄▄▆▅▅▄▅▅▆█▅▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▅▂▅▅▄▆▄▄▄▅▃▃▅▇▃▅▁▅▆▁▄▃▂▃▄▅▂▂▅▃▄▃▃▄▃▆▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ██▂▆▂▁▅▃▅▁▄▄▁▄▅▄▂▁▅▇▂▄▄▄▄▅▆█▅▅▄▆▄▇▆█▆▅▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▄▄▂▆▄▃▆▃▁▃▅▃▃▃▄▃▃▂▇▇▂▄▄▃▄▅▇▄▅▅▆▄▅▅▅▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▆▂▅▄▃▆▄▄▃▅▃▃▅▇▃▄▁▅▆▁▄▃▃▃▄▅▃▃▅▄▄▃▄▄▅▆▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▄▄▂▆▄▃▆▃▁▃▅▃▃▃▄▃▃▂▇▇▂▄▄▃▄▅▇▄▅▅▆▄▅▅▅▅█▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D █▅▄▅▅▅▅▅▅▅▆▄▅▆█▄▆▂▄▅▂▅▃▃▃▃▄▂▂▄▂▃▃▂▂▂▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▂▄▃▅▃▂▄▃▂▂▃▄▃▃▁▄▂▅▆▅▅▄▅▅▅▆▆▇▇▅▇▅▆▇▇▇▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▄▇▄▄▅▃▅▆▅▃▆▅▄▂▆▄█▄▃▇▅▇▇▇▆▄▇▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▂▁▂▂▅▃▄▃▄▃▂▁▃▄▁▄▂▄▂▃▃▃▅▂▅▃▂▂▃▃▇▅▃▂█▅▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▃▂▅▃█▅▅▃▄▆▃▅▅▄▃▂▅▆▂▂▇▃▄▃▅▄▁▂▅▄▅▂▁▂▇▃▅▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▂▁▂▂▂▅▃▃▄▃▃▁▆▃▂▂▇█▂▁▅▁▄▃▂▃▁▁▂▃▃▁▁▃▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D █▅▄▅▅▅▅▅▅▅▆▄▅▆█▄▆▂▄▅▂▅▃▃▃▃▄▂▂▄▂▃▃▂▂▂▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▂▄▃▅▃▂▄▃▂▂▃▄▃▃▁▄▂▅▆▅▅▄▅▅▅▆▆▇▇▅▇▅▆▇▇▇▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▄▇▄▄▅▃▅▆▅▃▆▅▄▂▆▄█▄▃▇▅▇▇▇▆▄▇▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▂▁▂▂▅▃▄▃▄▃▂▁▃▄▁▄▂▄▂▃▃▃▅▂▅▃▂▂▃▃▇▅▃▂█▅▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▃▂▅▃█▅▅▃▄▆▃▅▅▄▃▂▅▆▂▂▇▃▄▃▅▄▁▂▅▄▅▂▁▂▇▃▅▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▂▁▂▂▂▅▃▃▄▃▃▁▆▃▂▂▇█▂▁▅▁▄▃▂▃▁▁▂▃▃▁▁▃▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC █▅▄▅▅▅▅▅▅▅▆▄▅▆█▄▆▂▄▅▂▅▃▃▃▃▄▂▂▄▂▃▃▂▂▂▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▂▃▃▅▄▃▅▃▁▃▄▃▄▃▂▄▃▄▇▆▄▄▅▄▅▅▇▅▆▆█▅▆▆▆▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▂▅▄▄▂▂▃▃▅▂▃▅▂▃▁▄▂▅▃▃▅▃▅▅▅▅▄█▆▄▅▅▄▇▇▇▄▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▄▇▄▄▅▃▅▆▅▃▆▅▄▂▆▄█▄▃▇▅▇▇▇▆▄▇▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▂▁▂▂▅▃▄▃▄▃▂▁▃▄▁▄▂▄▂▃▃▃▅▂▅▃▂▂▃▃▇▅▃▂█▅▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▂▂▄▃█▅▅▃▃▄▃▄▄▃▃▂▃▅▂▁▅▂▄▂▅▃▁▃▅▃▄▂▁▂▇▄▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▅▁▃▂▃▄▄▂▄▆▂▃▅▃▂▂▆▆▁▃█▄▂▄▂▄▁▁▃▄▄▃▁▁▂▁▅▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▂▁▂▂▂▅▃▃▄▃▃▁▆▃▂▂▇█▂▁▅▁▄▃▂▃▁▁▂▃▃▁▁▃▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC █▅▄▅▅▅▅▅▅▅▆▄▅▆█▄▆▂▄▅▂▅▃▃▃▃▄▂▂▄▂▃▃▂▂▂▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▂▃▃▅▄▃▅▃▁▃▄▃▄▃▂▄▃▄▇▆▄▄▅▄▅▅▇▅▆▆█▅▆▆▆▆█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▂▅▄▄▂▂▃▃▅▂▃▅▂▃▁▄▂▅▃▃▅▃▅▅▅▅▄█▆▄▅▅▄▇▇▇▄▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▄▇▄▄▅▃▅▆▅▃▆▅▄▂▆▄█▄▃▇▅▇▇▇▆▄▇▇▅▆▆▇▇▆▆▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▂▁▂▂▅▃▄▃▄▃▂▁▃▄▁▄▂▄▂▃▃▃▅▂▅▃▂▂▃▃▇▅▃▂█▅▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▂▂▄▃█▅▅▃▃▄▃▄▄▃▃▂▃▅▂▁▅▂▄▂▅▃▁▃▅▃▄▂▁▂▇▄▃▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▅▁▃▂▃▄▄▂▄▆▂▃▅▃▂▂▆▆▁▃█▄▂▄▂▄▁▁▃▄▄▃▁▁▂▁▅▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▂▁▂▂▂▅▃▃▄▃▃▁▆▃▂▂▇█▂▁▅▁▄▃▂▃▁▁▂▃▃▁▁▃▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.28017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.50541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.97855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 3.31926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 3.3365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 3.3365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.2995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 599.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1172.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 188.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.2995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 1028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/73duypxp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200509-73duypxp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:07:15,964][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200717-2ltv0iid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/2ltv0iid\u001b[0m\n",
      "[2023-08-08 20:07:22,409][root][INFO] - => Done in 6.445 s\n",
      "[2023-08-08 20:07:22,409][root][INFO] - \n",
      "[2023-08-08 20:07:22,409][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:07:22,413][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:07:22,413][root][INFO] - => Done in 4.117 ms\n",
      "[2023-08-08 20:07:22,413][root][INFO] - \n",
      "[2023-08-08 20:07:22,414][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:07:22,964][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:07:22,964][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:07:22,964][root][INFO] - => Done in 550.318 ms\n",
      "[2023-08-08 20:07:22,964][root][INFO] - \n",
      "[2023-08-08 20:07:22,964][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:07:22,964][root][INFO] - => Done in 111.818 us\n",
      "[2023-08-08 20:07:22,964][root][INFO] - \n",
      "[2023-08-08 20:07:22,964][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:07:22,964][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:07:23,720][root][INFO] - => Done in 755.534 ms\n",
      "[2023-08-08 20:07:23,720][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.929 MB of 0.939 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.95633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.88074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.91653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1959.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/2ltv0iid\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200717-2ltv0iid/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:07:58,638][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200800-xkazm4qp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/xkazm4qp\u001b[0m\n",
      "[2023-08-08 20:08:04,480][root][INFO] - => Done in 5.842 s\n",
      "[2023-08-08 20:08:04,481][root][INFO] - \n",
      "[2023-08-08 20:08:04,481][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:08:04,484][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:08:04,484][root][INFO] - => Done in 3.238 ms\n",
      "[2023-08-08 20:08:04,484][root][INFO] - \n",
      "[2023-08-08 20:08:04,484][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:08:05,060][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:08:05,060][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:08:05,060][root][INFO] - => Done in 575.790 ms\n",
      "[2023-08-08 20:08:05,060][root][INFO] - \n",
      "[2023-08-08 20:08:05,060][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:08:05,060][root][INFO] - => Done in 109.911 us\n",
      "[2023-08-08 20:08:05,060][root][INFO] - \n",
      "[2023-08-08 20:08:05,060][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:08:05,060][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:08:05,813][root][INFO] - => Done in 752.352 ms\n",
      "[2023-08-08 20:08:05,813][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.928 MB of 0.936 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.9778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/xkazm4qp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200800-xkazm4qp/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:08:40,737][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200842-v1yjznes\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/v1yjznes\u001b[0m\n",
      "[2023-08-08 20:08:46,553][root][INFO] - => Done in 5.816 s\n",
      "[2023-08-08 20:08:46,553][root][INFO] - \n",
      "[2023-08-08 20:08:46,553][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:08:46,556][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:08:46,556][root][INFO] - => Done in 3.172 ms\n",
      "[2023-08-08 20:08:46,556][root][INFO] - \n",
      "[2023-08-08 20:08:46,556][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:08:47,140][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:08:47,141][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:08:47,141][root][INFO] - => Done in 584.270 ms\n",
      "[2023-08-08 20:08:47,141][root][INFO] - \n",
      "[2023-08-08 20:08:47,141][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:08:47,141][root][INFO] - => Done in 116.825 us\n",
      "[2023-08-08 20:08:47,141][root][INFO] - \n",
      "[2023-08-08 20:08:47,141][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:08:47,141][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:08:47,898][root][INFO] - => Done in 756.499 ms\n",
      "[2023-08-08 20:08:47,898][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.929 MB of 0.932 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.93568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.96953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/v1yjznes\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200842-v1yjznes/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:09:22,918][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_200924-viobektg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/viobektg\u001b[0m\n",
      "[2023-08-08 20:09:28,666][root][INFO] - => Done in 5.749 s\n",
      "[2023-08-08 20:09:28,667][root][INFO] - \n",
      "[2023-08-08 20:09:28,667][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:09:28,670][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:09:28,670][root][INFO] - => Done in 3.029 ms\n",
      "[2023-08-08 20:09:28,670][root][INFO] - \n",
      "[2023-08-08 20:09:28,670][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:09:29,251][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:09:29,251][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:09:29,251][root][INFO] - => Done in 581.590 ms\n",
      "[2023-08-08 20:09:29,251][root][INFO] - \n",
      "[2023-08-08 20:09:29,252][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:09:29,252][root][INFO] - => Done in 108.004 us\n",
      "[2023-08-08 20:09:29,252][root][INFO] - \n",
      "[2023-08-08 20:09:29,252][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:09:29,252][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:09:30,002][root][INFO] - => Done in 750.534 ms\n",
      "[2023-08-08 20:09:30,002][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.930 MB of 0.930 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/viobektg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_200924-viobektg/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:10:08,482][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201010-7j9uu9oz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/7j9uu9oz\u001b[0m\n",
      "[2023-08-08 20:10:14,269][root][INFO] - => Done in 5.787 s\n",
      "[2023-08-08 20:10:14,269][root][INFO] - \n",
      "[2023-08-08 20:10:14,269][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:10:14,273][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:10:14,273][root][INFO] - => Done in 3.670 ms\n",
      "[2023-08-08 20:10:14,273][root][INFO] - \n",
      "[2023-08-08 20:10:14,273][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:10:14,851][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:10:14,851][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:10:14,851][root][INFO] - => Done in 578.319 ms\n",
      "[2023-08-08 20:10:14,851][root][INFO] - \n",
      "[2023-08-08 20:10:14,851][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:10:14,851][root][INFO] - => Done in 109.911 us\n",
      "[2023-08-08 20:10:14,851][root][INFO] - \n",
      "[2023-08-08 20:10:14,851][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:10:14,852][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:10:15,604][root][INFO] - => Done in 752.038 ms\n",
      "[2023-08-08 20:10:15,604][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.931 MB of 0.943 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▁▁▁▁▁▁���▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.9756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/7j9uu9oz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201010-7j9uu9oz/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:10:49,683][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201051-u2lh1osx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/u2lh1osx\u001b[0m\n",
      "[2023-08-08 20:10:55,673][root][INFO] - => Done in 5.990 s\n",
      "[2023-08-08 20:10:55,673][root][INFO] - \n",
      "[2023-08-08 20:10:55,673][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:10:55,676][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:10:55,676][root][INFO] - => Done in 2.969 ms\n",
      "[2023-08-08 20:10:55,676][root][INFO] - \n",
      "[2023-08-08 20:10:55,676][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:10:56,256][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:10:56,257][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:10:56,257][root][INFO] - => Done in 580.714 ms\n",
      "[2023-08-08 20:10:56,257][root][INFO] - \n",
      "[2023-08-08 20:10:56,257][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:10:56,257][root][INFO] - => Done in 113.964 us\n",
      "[2023-08-08 20:10:56,257][root][INFO] - \n",
      "[2023-08-08 20:10:56,257][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:10:56,257][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:10:57,016][root][INFO] - => Done in 758.918 ms\n",
      "[2023-08-08 20:10:57,016][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.944 MB of 0.944 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.95833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.91649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9475\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1960.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1960\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/u2lh1osx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201051-u2lh1osx/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:11:34,636][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201135-thw1pmb8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/thw1pmb8\u001b[0m\n",
      "[2023-08-08 20:11:43,584][root][INFO] - => Done in 8.949 s\n",
      "[2023-08-08 20:11:43,584][root][INFO] - \n",
      "[2023-08-08 20:11:43,585][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:11:43,588][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:11:43,588][root][INFO] - => Done in 3.304 ms\n",
      "[2023-08-08 20:11:43,588][root][INFO] - \n",
      "[2023-08-08 20:11:43,588][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:11:44,156][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:11:44,156][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:11:44,157][root][INFO] - => Done in 568.576 ms\n",
      "[2023-08-08 20:11:44,157][root][INFO] - \n",
      "[2023-08-08 20:11:44,157][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:11:44,157][root][INFO] - => Done in 116.110 us\n",
      "[2023-08-08 20:11:44,157][root][INFO] - \n",
      "[2023-08-08 20:11:44,157][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:11:44,157][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:11:44,904][root][INFO] - => Done in 747.628 ms\n",
      "[2023-08-08 20:11:44,905][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/thw1pmb8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201135-thw1pmb8/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:13:14,930][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201316-ugk04zzc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ugk04zzc\u001b[0m\n",
      "[2023-08-08 20:13:22,241][root][INFO] - => Done in 7.312 s\n",
      "[2023-08-08 20:13:22,242][root][INFO] - \n",
      "[2023-08-08 20:13:22,242][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:13:22,245][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:13:22,245][root][INFO] - => Done in 3.423 ms\n",
      "[2023-08-08 20:13:22,245][root][INFO] - \n",
      "[2023-08-08 20:13:22,245][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:13:22,814][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:13:22,814][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:13:22,814][root][INFO] - => Done in 568.738 ms\n",
      "[2023-08-08 20:13:22,814][root][INFO] - \n",
      "[2023-08-08 20:13:22,814][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:13:22,814][root][INFO] - => Done in 160.933 us\n",
      "[2023-08-08 20:13:22,814][root][INFO] - \n",
      "[2023-08-08 20:13:22,814][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:13:22,815][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:13:23,589][root][INFO] - => Done in 774.873 ms\n",
      "[2023-08-08 20:13:23,590][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.934 MB of 0.934 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▂▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▂▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.93603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.941\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.96955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ugk04zzc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201316-ugk04zzc/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:14:00,212][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201401-h9ov544n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/h9ov544n\u001b[0m\n",
      "[2023-08-08 20:14:06,404][root][INFO] - => Done in 6.192 s\n",
      "[2023-08-08 20:14:06,405][root][INFO] - \n",
      "[2023-08-08 20:14:06,405][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:14:06,408][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:14:06,408][root][INFO] - => Done in 3.088 ms\n",
      "[2023-08-08 20:14:06,408][root][INFO] - \n",
      "[2023-08-08 20:14:06,408][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:14:06,975][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:14:06,975][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:14:06,975][root][INFO] - => Done in 566.958 ms\n",
      "[2023-08-08 20:14:06,975][root][INFO] - \n",
      "[2023-08-08 20:14:06,975][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:14:06,975][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-08 20:14:06,975][root][INFO] - \n",
      "[2023-08-08 20:14:06,975][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:14:06,975][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:14:07,730][root][INFO] - => Done in 754.500 ms\n",
      "[2023-08-08 20:14:07,730][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.935 MB of 0.943 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/h9ov544n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201401-h9ov544n/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:14:42,641][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201444-yco0a73a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/yco0a73a\u001b[0m\n",
      "[2023-08-08 20:14:48,535][root][INFO] - => Done in 5.894 s\n",
      "[2023-08-08 20:14:48,535][root][INFO] - \n",
      "[2023-08-08 20:14:48,535][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:14:48,539][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:14:48,539][root][INFO] - => Done in 3.159 ms\n",
      "[2023-08-08 20:14:48,539][root][INFO] - \n",
      "[2023-08-08 20:14:48,539][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:14:49,110][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:14:49,110][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:14:49,110][root][INFO] - => Done in 571.396 ms\n",
      "[2023-08-08 20:14:49,110][root][INFO] - \n",
      "[2023-08-08 20:14:49,110][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:14:49,110][root][INFO] - => Done in 142.097 us\n",
      "[2023-08-08 20:14:49,110][root][INFO] - \n",
      "[2023-08-08 20:14:49,110][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:14:49,111][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:14:49,869][root][INFO] - => Done in 758.268 ms\n",
      "[2023-08-08 20:14:49,869][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.936 MB of 0.948 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97564\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/yco0a73a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201444-yco0a73a/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:15:24,847][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201526-1pfhlsh6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/1pfhlsh6\u001b[0m\n",
      "[2023-08-08 20:15:30,839][root][INFO] - => Done in 5.991 s\n",
      "[2023-08-08 20:15:30,839][root][INFO] - \n",
      "[2023-08-08 20:15:30,839][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:15:30,842][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:15:30,842][root][INFO] - => Done in 2.964 ms\n",
      "[2023-08-08 20:15:30,842][root][INFO] - \n",
      "[2023-08-08 20:15:30,842][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:15:31,416][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:15:31,417][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:15:31,417][root][INFO] - => Done in 574.775 ms\n",
      "[2023-08-08 20:15:31,417][root][INFO] - \n",
      "[2023-08-08 20:15:31,417][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:15:31,417][root][INFO] - => Done in 123.024 us\n",
      "[2023-08-08 20:15:31,417][root][INFO] - \n",
      "[2023-08-08 20:15:31,417][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:15:31,417][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:15:32,173][root][INFO] - => Done in 756.404 ms\n",
      "[2023-08-08 20:15:32,174][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.949 MB of 0.949 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.88102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.91651\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1960.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1960\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/1pfhlsh6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201526-1pfhlsh6/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:16:07,400][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201608-ohf178yd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ohf178yd\u001b[0m\n",
      "[2023-08-08 20:16:13,142][root][INFO] - => Done in 5.742 s\n",
      "[2023-08-08 20:16:13,143][root][INFO] - \n",
      "[2023-08-08 20:16:13,143][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:16:13,146][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:16:13,146][root][INFO] - => Done in 3.048 ms\n",
      "[2023-08-08 20:16:13,146][root][INFO] - \n",
      "[2023-08-08 20:16:13,146][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:16:13,720][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:16:13,721][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:16:13,721][root][INFO] - => Done in 574.737 ms\n",
      "[2023-08-08 20:16:13,721][root][INFO] - \n",
      "[2023-08-08 20:16:13,721][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:16:13,721][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-08 20:16:13,721][root][INFO] - \n",
      "[2023-08-08 20:16:13,721][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:16:13,721][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:16:14,476][root][INFO] - => Done in 754.646 ms\n",
      "[2023-08-08 20:16:14,476][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.939 MB of 0.950 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ohf178yd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201608-ohf178yd/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:16:49,455][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201650-yx22p40d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/yx22p40d\u001b[0m\n",
      "[2023-08-08 20:16:55,763][root][INFO] - => Done in 6.308 s\n",
      "[2023-08-08 20:16:55,763][root][INFO] - \n",
      "[2023-08-08 20:16:55,764][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:16:55,768][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:16:55,768][root][INFO] - => Done in 4.161 ms\n",
      "[2023-08-08 20:16:55,768][root][INFO] - \n",
      "[2023-08-08 20:16:55,768][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:16:56,346][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:16:56,346][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:16:56,346][root][INFO] - => Done in 578.618 ms\n",
      "[2023-08-08 20:16:56,346][root][INFO] - \n",
      "[2023-08-08 20:16:56,347][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:16:56,347][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-08 20:16:56,347][root][INFO] - \n",
      "[2023-08-08 20:16:56,347][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:16:56,347][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:16:57,096][root][INFO] - => Done in 749.602 ms\n",
      "[2023-08-08 20:16:57,097][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      "Error executing job with overrides: ['+experiment=multi-shapers/comp/n3pl_2shap_tc_comp_eval', '++run_path1=ucl-dark/tensor-tc/8v8vokho', '++run_path2=ucl-dark/tensor-tc/8v8vokho', '++model_path1=exp/3pl-2shap-tc/3pl_2shap_tc_3/2023-08-02_14.38.10.878306/generation_299_agent_1', '++model_path2=exp/3pl-2shap-tc/3pl_2shap_tc_3/2023-08-02_14.38.10.878306/generation_299_agent_0', '++wandb.name=3pl_2shap_tc_s3a2_s3a1_eval', '++wandb.group=3pl_2shap_tc-comp-mean_v_nice']\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/http/client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/http/client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/http/client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/packages/six.py\", line 770, in reraise\n",
      "    raise value\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 451, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 340, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Read timed out. (read timeout=9)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/normalize.py\", line 26, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 870, in run\n",
      "    entity, project, run_id = self._parse_path(path)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 631, in _parse_path\n",
      "    entity = self.settings[\"entity\"] or self.default_entity\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 533, in default_entity\n",
      "    res = self._client.execute(self.VIEWER_QUERY)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/lib/retry.py\", line 209, in wrapped_fn\n",
      "    return retrier(*args, **kargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/lib/retry.py\", line 128, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 240, in execute\n",
      "    return self._client.execute(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/transport/requests.py\", line 38, in execute\n",
      "    request = requests.post(self.url, **post_args)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/adapters.py\", line 578, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='api.wandb.ai', port=443): Read timed out. (read timeout=9)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/experiment.py\", line 823, in main\n",
      "    runner.run_loop(env_params, agent_pair, watchers)\n",
      "  File \"/Users/alexandrasouly/code/pax/pax/runners/runner_eval_multipshaper.py\", line 519, in run_loop\n",
      "    wandb.restore(\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 3636, in restore\n",
      "    api_run = api.run(run_path)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/normalize.py\", line 64, in wrapper\n",
      "    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/normalize.py\", line 26, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 870, in run\n",
      "    entity, project, run_id = self._parse_path(path)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 631, in _parse_path\n",
      "    entity = self.settings[\"entity\"] or self.default_entity\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 533, in default_entity\n",
      "    res = self._client.execute(self.VIEWER_QUERY)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/lib/retry.py\", line 209, in wrapped_fn\n",
      "    return retrier(*args, **kargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/sdk/lib/retry.py\", line 128, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/apis/public.py\", line 240, in execute\n",
      "    return self._client.execute(*args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/transport/requests.py\", line 38, in execute\n",
      "    request = requests.post(self.url, **post_args)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/requests/adapters.py\", line 578, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "wandb.errors.CommError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Read timed out. (read timeout=9)\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/yx22p40d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201650-yx22p40d/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:18:20,756][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201822-lpzk1l8q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/lpzk1l8q\u001b[0m\n",
      "[2023-08-08 20:18:28,604][root][INFO] - => Done in 7.848 s\n",
      "[2023-08-08 20:18:28,604][root][INFO] - \n",
      "[2023-08-08 20:18:28,604][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:18:28,610][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:18:28,610][root][INFO] - => Done in 5.499 ms\n",
      "[2023-08-08 20:18:28,610][root][INFO] - \n",
      "[2023-08-08 20:18:28,610][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:18:29,189][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:18:29,189][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:18:29,189][root][INFO] - => Done in 578.782 ms\n",
      "[2023-08-08 20:18:29,189][root][INFO] - \n",
      "[2023-08-08 20:18:29,189][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:18:29,189][root][INFO] - => Done in 115.871 us\n",
      "[2023-08-08 20:18:29,189][root][INFO] - \n",
      "[2023-08-08 20:18:29,189][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:18:29,189][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:18:29,962][root][INFO] - => Done in 772.597 ms\n",
      "[2023-08-08 20:18:29,962][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.941 MB of 0.953 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/lpzk1l8q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201822-lpzk1l8q/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:19:05,728][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201907-zb42f3h5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/zb42f3h5\u001b[0m\n",
      "[2023-08-08 20:19:12,099][root][INFO] - => Done in 6.372 s\n",
      "[2023-08-08 20:19:12,100][root][INFO] - \n",
      "[2023-08-08 20:19:12,100][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:19:12,106][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:19:12,107][root][INFO] - => Done in 6.607 ms\n",
      "[2023-08-08 20:19:12,107][root][INFO] - \n",
      "[2023-08-08 20:19:12,107][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:19:12,687][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:19:12,688][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:19:12,688][root][INFO] - => Done in 581.084 ms\n",
      "[2023-08-08 20:19:12,688][root][INFO] - \n",
      "[2023-08-08 20:19:12,688][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:19:12,688][root][INFO] - => Done in 166.178 us\n",
      "[2023-08-08 20:19:12,688][root][INFO] - \n",
      "[2023-08-08 20:19:12,688][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:19:12,688][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:19:13,447][root][INFO] - => Done in 758.552 ms\n",
      "[2023-08-08 20:19:13,447][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/zb42f3h5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201907-zb42f3h5/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:19:52,864][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_201954-g89ot0y0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/g89ot0y0\u001b[0m\n",
      "[2023-08-08 20:19:59,039][root][INFO] - => Done in 6.175 s\n",
      "[2023-08-08 20:19:59,040][root][INFO] - \n",
      "[2023-08-08 20:19:59,040][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:19:59,043][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:19:59,043][root][INFO] - => Done in 3.196 ms\n",
      "[2023-08-08 20:19:59,043][root][INFO] - \n",
      "[2023-08-08 20:19:59,043][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:19:59,604][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:19:59,604][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:19:59,604][root][INFO] - => Done in 560.965 ms\n",
      "[2023-08-08 20:19:59,604][root][INFO] - \n",
      "[2023-08-08 20:19:59,604][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:19:59,604][root][INFO] - => Done in 113.010 us\n",
      "[2023-08-08 20:19:59,604][root][INFO] - \n",
      "[2023-08-08 20:19:59,604][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:19:59,604][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:20:00,360][root][INFO] - => Done in 755.380 ms\n",
      "[2023-08-08 20:20:00,360][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.955 MB of 0.955 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.95633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.88023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.9165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1959.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/g89ot0y0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_201954-g89ot0y0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:20:35,770][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_202037-ia7d8b6x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ia7d8b6x\u001b[0m\n",
      "[2023-08-08 20:20:41,519][root][INFO] - => Done in 5.749 s\n",
      "[2023-08-08 20:20:41,519][root][INFO] - \n",
      "[2023-08-08 20:20:41,519][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:20:41,522][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:20:41,522][root][INFO] - => Done in 2.970 ms\n",
      "[2023-08-08 20:20:41,522][root][INFO] - \n",
      "[2023-08-08 20:20:41,522][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:20:42,110][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:20:42,110][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:20:42,111][root][INFO] - => Done in 588.104 ms\n",
      "[2023-08-08 20:20:42,111][root][INFO] - \n",
      "[2023-08-08 20:20:42,111][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:20:42,111][root][INFO] - => Done in 109.673 us\n",
      "[2023-08-08 20:20:42,111][root][INFO] - \n",
      "[2023-08-08 20:20:42,111][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:20:42,111][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:20:42,864][root][INFO] - => Done in 753.579 ms\n",
      "[2023-08-08 20:20:42,865][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.955 MB of 0.956 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ia7d8b6x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_202037-ia7d8b6x/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:21:19,013][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_202120-a9xbx0hr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/a9xbx0hr\u001b[0m\n",
      "[2023-08-08 20:21:25,122][root][INFO] - => Done in 6.109 s\n",
      "[2023-08-08 20:21:25,122][root][INFO] - \n",
      "[2023-08-08 20:21:25,122][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:21:25,125][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:21:25,126][root][INFO] - => Done in 3.091 ms\n",
      "[2023-08-08 20:21:25,126][root][INFO] - \n",
      "[2023-08-08 20:21:25,126][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:21:25,682][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:21:25,683][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:21:25,683][root][INFO] - => Done in 557.099 ms\n",
      "[2023-08-08 20:21:25,683][root][INFO] - \n",
      "[2023-08-08 20:21:25,683][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:21:25,683][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-08 20:21:25,683][root][INFO] - \n",
      "[2023-08-08 20:21:25,683][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:21:25,683][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:21:26,439][root][INFO] - => Done in 755.570 ms\n",
      "[2023-08-08 20:21:26,439][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.945 MB of 0.953 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▂▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▂▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.93525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.96952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/a9xbx0hr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_202120-a9xbx0hr/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:46:13,494][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204615-c1ha0f51\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/c1ha0f51\u001b[0m\n",
      "[2023-08-08 20:46:20,067][root][INFO] - => Done in 6.573 s\n",
      "[2023-08-08 20:46:20,067][root][INFO] - \n",
      "[2023-08-08 20:46:20,067][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:46:20,071][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:46:20,071][root][INFO] - => Done in 4.363 ms\n",
      "[2023-08-08 20:46:20,071][root][INFO] - \n",
      "[2023-08-08 20:46:20,071][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:46:20,723][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:46:20,723][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:46:20,723][root][INFO] - => Done in 652.094 ms\n",
      "[2023-08-08 20:46:20,723][root][INFO] - \n",
      "[2023-08-08 20:46:20,724][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:46:20,724][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-08 20:46:20,724][root][INFO] - \n",
      "[2023-08-08 20:46:20,724][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:46:20,724][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:46:21,564][root][INFO] - => Done in 840.679 ms\n",
      "[2023-08-08 20:46:21,565][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.946 MB of 0.958 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.9449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/c1ha0f51\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204615-c1ha0f51/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:46:57,769][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204658-tlovi4k0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/tlovi4k0\u001b[0m\n",
      "[2023-08-08 20:47:02,794][root][INFO] - => Done in 5.024 s\n",
      "[2023-08-08 20:47:02,794][root][INFO] - \n",
      "[2023-08-08 20:47:02,794][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:47:02,797][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:47:02,797][root][INFO] - => Done in 3.074 ms\n",
      "[2023-08-08 20:47:02,797][root][INFO] - \n",
      "[2023-08-08 20:47:02,797][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:47:03,365][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:47:03,365][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:47:03,366][root][INFO] - => Done in 568.120 ms\n",
      "[2023-08-08 20:47:03,366][root][INFO] - \n",
      "[2023-08-08 20:47:03,366][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:47:03,366][root][INFO] - => Done in 127.077 us\n",
      "[2023-08-08 20:47:03,366][root][INFO] - \n",
      "[2023-08-08 20:47:03,366][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:47:03,366][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:47:04,143][root][INFO] - => Done in 776.781 ms\n",
      "[2023-08-08 20:47:04,143][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/tlovi4k0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204658-tlovi4k0/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:47:38,424][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a1_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204739-1gsyr4eg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a1_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/1gsyr4eg\u001b[0m\n",
      "[2023-08-08 20:47:43,231][root][INFO] - => Done in 4.807 s\n",
      "[2023-08-08 20:47:43,231][root][INFO] - \n",
      "[2023-08-08 20:47:43,231][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:47:43,234][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:47:43,234][root][INFO] - => Done in 2.926 ms\n",
      "[2023-08-08 20:47:43,234][root][INFO] - \n",
      "[2023-08-08 20:47:43,234][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:47:43,816][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:47:43,816][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:47:43,816][root][INFO] - => Done in 582.051 ms\n",
      "[2023-08-08 20:47:43,816][root][INFO] - \n",
      "[2023-08-08 20:47:43,816][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:47:43,816][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-08 20:47:43,816][root][INFO] - \n",
      "[2023-08-08 20:47:43,817][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:47:43,817][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:47:44,602][root][INFO] - => Done in 785.200 ms\n",
      "[2023-08-08 20:47:44,602][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇██████████████████████████████████���\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.95633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.87853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91723\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.91657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1959.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a1_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/1gsyr4eg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204739-1gsyr4eg/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:48:18,618][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a1_s2a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204819-i769bxo2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a1_s2a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/i769bxo2\u001b[0m\n",
      "[2023-08-08 20:48:23,676][root][INFO] - => Done in 5.058 s\n",
      "[2023-08-08 20:48:23,677][root][INFO] - \n",
      "[2023-08-08 20:48:23,677][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:48:23,679][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:48:23,679][root][INFO] - => Done in 2.902 ms\n",
      "[2023-08-08 20:48:23,680][root][INFO] - \n",
      "[2023-08-08 20:48:23,680][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:48:24,244][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:48:24,244][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:48:24,244][root][INFO] - => Done in 564.399 ms\n",
      "[2023-08-08 20:48:24,244][root][INFO] - \n",
      "[2023-08-08 20:48:24,244][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:48:24,244][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-08 20:48:24,244][root][INFO] - \n",
      "[2023-08-08 20:48:24,244][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:48:24,244][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:48:25,012][root][INFO] - => Done in 767.994 ms\n",
      "[2023-08-08 20:48:25,013][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94203\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a1_s2a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/i769bxo2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204819-i769bxo2/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:49:01,456][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a1_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204902-rb3y7myy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a1_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/rb3y7myy\u001b[0m\n",
      "[2023-08-08 20:49:07,610][root][INFO] - => Done in 6.154 s\n",
      "[2023-08-08 20:49:07,610][root][INFO] - \n",
      "[2023-08-08 20:49:07,610][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:49:07,615][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:49:07,616][root][INFO] - => Done in 5.434 ms\n",
      "[2023-08-08 20:49:07,616][root][INFO] - \n",
      "[2023-08-08 20:49:07,616][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:49:08,183][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:49:08,183][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:49:08,183][root][INFO] - => Done in 567.171 ms\n",
      "[2023-08-08 20:49:08,183][root][INFO] - \n",
      "[2023-08-08 20:49:08,183][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:49:08,183][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-08 20:49:08,183][root][INFO] - \n",
      "[2023-08-08 20:49:08,183][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:49:08,183][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:49:08,945][root][INFO] - => Done in 761.746 ms\n",
      "[2023-08-08 20:49:08,945][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.951 MB of 0.963 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.93343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.96947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a1_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/rb3y7myy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204902-rb3y7myy/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:49:45,639][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a1_s4a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_204946-5xo0rlbr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a1_s4a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/5xo0rlbr\u001b[0m\n",
      "[2023-08-08 20:49:51,188][root][INFO] - => Done in 5.549 s\n",
      "[2023-08-08 20:49:51,189][root][INFO] - \n",
      "[2023-08-08 20:49:51,189][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:49:51,194][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:49:51,194][root][INFO] - => Done in 5.015 ms\n",
      "[2023-08-08 20:49:51,194][root][INFO] - \n",
      "[2023-08-08 20:49:51,194][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:49:51,769][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:49:51,769][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:49:51,769][root][INFO] - => Done in 575.132 ms\n",
      "[2023-08-08 20:49:51,769][root][INFO] - \n",
      "[2023-08-08 20:49:51,769][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:49:51,769][root][INFO] - => Done in 114.918 us\n",
      "[2023-08-08 20:49:51,770][root][INFO] - \n",
      "[2023-08-08 20:49:51,770][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:49:51,770][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:49:52,533][root][INFO] - => Done in 763.221 ms\n",
      "[2023-08-08 20:49:52,533][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94475\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a1_s4a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/5xo0rlbr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_204946-5xo0rlbr/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-08 20:50:28,187][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a1_s5a2_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230808_205029-m7sb7gh7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a1_s5a2_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/m7sb7gh7\u001b[0m\n",
      "[2023-08-08 20:50:34,041][root][INFO] - => Done in 5.853 s\n",
      "[2023-08-08 20:50:34,041][root][INFO] - \n",
      "[2023-08-08 20:50:34,041][root][INFO] - => Env setup ...\n",
      "[2023-08-08 20:50:34,048][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-08 20:50:34,048][root][INFO] - => Done in 7.313 ms\n",
      "[2023-08-08 20:50:34,049][root][INFO] - \n",
      "[2023-08-08 20:50:34,049][root][INFO] - => Agent setup ...\n",
      "[2023-08-08 20:50:34,618][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-08 20:50:34,618][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-08 20:50:34,618][root][INFO] - => Done in 569.468 ms\n",
      "[2023-08-08 20:50:34,618][root][INFO] - \n",
      "[2023-08-08 20:50:34,618][root][INFO] - => Watcher setup ...\n",
      "[2023-08-08 20:50:34,618][root][INFO] - => Done in 110.149 us\n",
      "[2023-08-08 20:50:34,618][root][INFO] - \n",
      "[2023-08-08 20:50:34,618][root][INFO] - => Runner setup ...\n",
      "[2023-08-08 20:50:34,619][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-08 20:50:35,381][root][INFO] - => Done in 762.939 ms\n",
      "[2023-08-08 20:50:35,382][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.94045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.97566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a1_s5a2_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/m7sb7gh7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_205029-m7sb7gh7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # they all did dcc or cdc so I expect the shapers to have two categories: mean and nice once\n",
    "\n",
    "import yaml\n",
    "game= \"tc\"\n",
    "num_pl = 3\n",
    "num_shap = 2\n",
    "\n",
    "mean = [(1,2),(2,1),(3,2),(4,1),(5,1)]\n",
    "nice = [(1,1), (2,2), (3,1),(4,2),(5,2)]\n",
    "\n",
    "# mean v mean\n",
    "for seed1,agent1 in mean:\n",
    "    for seed2,agent2 in mean:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group='3pl_2shap_tc-comp-mean_v_mean'\n",
    "\n",
    "\n",
    "# nice v nice\n",
    "for seed1,agent1 in nice:\n",
    "    for seed2,agent2 in nice:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group='3pl_2shap_tc-comp-nice_v_nice'\n",
    "\n",
    "# mean v nice\n",
    "for seed1,agent1 in mean:\n",
    "    for seed2,agent2 in nice:\n",
    "        ########## shaper1\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path1 = config[f'run_path{agent1}']\n",
    "        agent_path1 =   config[f'model_path{agent1}']\n",
    "        ########## shaper 2\n",
    "        yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "        with open(yaml_f) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_path2 = config[f'run_path{agent2}']\n",
    "        agent_path2 =   config[f'model_path{agent2}']\n",
    "        ############# run experiment\n",
    "        name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "        !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group='3pl_2shap_tc-comp-mean_v_nice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 08:56:43,832][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s1a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_085645-m06fjzow\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s1a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/m06fjzow\u001b[0m\n",
      "[2023-08-09 08:56:50,418][root][INFO] - => Done in 6.587 s\n",
      "[2023-08-09 08:56:50,419][root][INFO] - \n",
      "[2023-08-09 08:56:50,419][root][INFO] - => Env setup ...\n",
      "[2023-08-09 08:56:50,422][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 08:56:50,422][root][INFO] - => Done in 3.222 ms\n",
      "[2023-08-09 08:56:50,422][root][INFO] - \n",
      "[2023-08-09 08:56:50,422][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 08:56:51,142][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 08:56:51,143][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 08:56:51,143][root][INFO] - => Done in 720.664 ms\n",
      "[2023-08-09 08:56:51,143][root][INFO] - \n",
      "[2023-08-09 08:56:51,143][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 08:56:51,143][root][INFO] - => Done in 113.010 us\n",
      "[2023-08-09 08:56:51,143][root][INFO] - \n",
      "[2023-08-09 08:56:51,143][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 08:56:51,143][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 08:56:52,078][root][INFO] - => Done in 934.924 ms\n",
      "[2023-08-09 08:56:52,078][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 1.036 MB of 1.036 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s1a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/m06fjzow\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_085645-m06fjzow/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 08:57:30,523][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s2a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_085731-y0lep3ac\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s2a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y0lep3ac\u001b[0m\n",
      "[2023-08-09 08:57:36,005][root][INFO] - => Done in 5.482 s\n",
      "[2023-08-09 08:57:36,005][root][INFO] - \n",
      "[2023-08-09 08:57:36,006][root][INFO] - => Env setup ...\n",
      "[2023-08-09 08:57:36,009][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 08:57:36,009][root][INFO] - => Done in 3.656 ms\n",
      "[2023-08-09 08:57:36,009][root][INFO] - \n",
      "[2023-08-09 08:57:36,009][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 08:57:36,832][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 08:57:36,833][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 08:57:36,833][root][INFO] - => Done in 823.364 ms\n",
      "[2023-08-09 08:57:36,833][root][INFO] - \n",
      "[2023-08-09 08:57:36,833][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 08:57:36,833][root][INFO] - => Done in 111.103 us\n",
      "[2023-08-09 08:57:36,833][root][INFO] - \n",
      "[2023-08-09 08:57:36,833][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 08:57:36,833][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 08:57:37,736][root][INFO] - => Done in 902.597 ms\n",
      "[2023-08-09 08:57:37,736][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.049 MB of 1.049 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s2a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/y0lep3ac\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_085731-y0lep3ac/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 08:58:19,097][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s3a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_085820-ell4qvr1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s3a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ell4qvr1\u001b[0m\n",
      "[2023-08-09 08:58:24,134][root][INFO] - => Done in 5.037 s\n",
      "[2023-08-09 08:58:24,134][root][INFO] - \n",
      "[2023-08-09 08:58:24,134][root][INFO] - => Env setup ...\n",
      "[2023-08-09 08:58:24,137][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 08:58:24,137][root][INFO] - => Done in 3.335 ms\n",
      "[2023-08-09 08:58:24,137][root][INFO] - \n",
      "[2023-08-09 08:58:24,137][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 08:58:24,799][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 08:58:24,799][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 08:58:24,799][root][INFO] - => Done in 662.200 ms\n",
      "[2023-08-09 08:58:24,800][root][INFO] - \n",
      "[2023-08-09 08:58:24,800][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 08:58:24,800][root][INFO] - => Done in 108.957 us\n",
      "[2023-08-09 08:58:24,800][root][INFO] - \n",
      "[2023-08-09 08:58:24,800][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 08:58:24,800][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 08:58:25,705][root][INFO] - => Done in 905.300 ms\n",
      "[2023-08-09 08:58:25,705][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 1.050 MB of 1.050 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01534\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1979.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.9895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s3a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/ell4qvr1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_085820-ell4qvr1/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 08:59:05,230][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s4a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_085906-pc9qh8vk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s4a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pc9qh8vk\u001b[0m\n",
      "[2023-08-09 08:59:11,601][root][INFO] - => Done in 6.370 s\n",
      "[2023-08-09 08:59:11,601][root][INFO] - \n",
      "[2023-08-09 08:59:11,601][root][INFO] - => Env setup ...\n",
      "[2023-08-09 08:59:11,605][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 08:59:11,605][root][INFO] - => Done in 3.846 ms\n",
      "[2023-08-09 08:59:11,605][root][INFO] - \n",
      "[2023-08-09 08:59:11,605][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 08:59:12,279][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 08:59:12,279][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 08:59:12,279][root][INFO] - => Done in 673.750 ms\n",
      "[2023-08-09 08:59:12,279][root][INFO] - \n",
      "[2023-08-09 08:59:12,279][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 08:59:12,279][root][INFO] - => Done in 109.911 us\n",
      "[2023-08-09 08:59:12,279][root][INFO] - \n",
      "[2023-08-09 08:59:12,279][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 08:59:12,279][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 08:59:13,185][root][INFO] - => Done in 905.754 ms\n",
      "[2023-08-09 08:59:13,185][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.0155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s4a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/pc9qh8vk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_085906-pc9qh8vk/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 08:59:51,378][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_ipd_s5a2_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_085952-uam9zirr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_ipd_s5a2_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/uam9zirr\u001b[0m\n",
      "[2023-08-09 08:59:57,203][root][INFO] - => Done in 5.825 s\n",
      "[2023-08-09 08:59:57,203][root][INFO] - \n",
      "[2023-08-09 08:59:57,203][root][INFO] - => Env setup ...\n",
      "[2023-08-09 08:59:57,206][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 08:59:57,206][root][INFO] - => Done in 2.956 ms\n",
      "[2023-08-09 08:59:57,206][root][INFO] - \n",
      "[2023-08-09 08:59:57,206][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 08:59:57,862][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 08:59:57,862][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 08:59:57,863][root][INFO] - => Done in 656.611 ms\n",
      "[2023-08-09 08:59:57,863][root][INFO] - \n",
      "[2023-08-09 08:59:57,863][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 08:59:57,863][root][INFO] - => Done in 118.017 us\n",
      "[2023-08-09 08:59:57,863][root][INFO] - \n",
      "[2023-08-09 08:59:57,863][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 08:59:57,863][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 08:59:58,774][root][INFO] - => Done in 911.522 ms\n",
      "[2023-08-09 08:59:58,775][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.01516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.01579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 0.99303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_ipd_s5a2_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-ipd/runs/uam9zirr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_085952-uam9zirr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# switch agents\n",
    "import yaml\n",
    "game= \"ipd\"\n",
    "num_pl = 3\n",
    "num_shap = 2\n",
    "\n",
    "# mean v mean\n",
    "for seed in range(1,6):\n",
    "    seed2=seed\n",
    "    seed1=seed\n",
    "    agent1=2\n",
    "    agent2=1\n",
    "\n",
    "    ########## shaper1\n",
    "    yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "    with open(yaml_f) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    run_path1 = config[f'run_path{agent1}']\n",
    "    agent_path1 =   config[f'model_path{agent1}']\n",
    "    ########## shaper 2\n",
    "    yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "    with open(yaml_f) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    run_path2 = config[f'run_path{agent2}']\n",
    "    agent_path2 =   config[f'model_path{agent2}']\n",
    "    ############# run experiment\n",
    "    name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "    group = f'{num_pl}pl_{num_shap}shap_{game}-comp-switch_agents'\n",
    "    !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group={group}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 09:00:38,209][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s1a2_s1a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_090039-qh03tm8b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s1a2_s1a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/qh03tm8b\u001b[0m\n",
      "[2023-08-09 09:00:44,355][root][INFO] - => Done in 6.146 s\n",
      "[2023-08-09 09:00:44,356][root][INFO] - \n",
      "[2023-08-09 09:00:44,356][root][INFO] - => Env setup ...\n",
      "[2023-08-09 09:00:44,359][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 09:00:44,359][root][INFO] - => Done in 3.788 ms\n",
      "[2023-08-09 09:00:44,359][root][INFO] - \n",
      "[2023-08-09 09:00:44,360][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 09:00:45,013][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 09:00:45,013][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 09:00:45,013][root][INFO] - => Done in 653.375 ms\n",
      "[2023-08-09 09:00:45,013][root][INFO] - \n",
      "[2023-08-09 09:00:45,013][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 09:00:45,013][root][INFO] - => Done in 120.878 us\n",
      "[2023-08-09 09:00:45,013][root][INFO] - \n",
      "[2023-08-09 09:00:45,013][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 09:00:45,013][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 09:00:45,926][root][INFO] - => Done in 912.919 ms\n",
      "[2023-08-09 09:00:45,926][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 1.041 MB of 1.041 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇███████▇██████████▇█▇██▇█████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▄▄▃▂▂▁▄▂▄▂▄▇▂▅▄▄▂▄▂▂▄▅█▄▇▄▅▇▅▄▂▅▅▄▅▄▂▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.95633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.88074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.91734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.91653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1959.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.9795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s1a2_s1a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/qh03tm8b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_090039-qh03tm8b/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 09:01:25,478][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s2a2_s2a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_090126-3w47ak5c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s2a2_s2a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/3w47ak5c\u001b[0m\n",
      "[2023-08-09 09:01:30,599][root][INFO] - => Done in 5.121 s\n",
      "[2023-08-09 09:01:30,599][root][INFO] - \n",
      "[2023-08-09 09:01:30,600][root][INFO] - => Env setup ...\n",
      "[2023-08-09 09:01:30,603][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 09:01:30,603][root][INFO] - => Done in 3.490 ms\n",
      "[2023-08-09 09:01:30,603][root][INFO] - \n",
      "[2023-08-09 09:01:30,603][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 09:01:31,263][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 09:01:31,263][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 09:01:31,263][root][INFO] - => Done in 660.025 ms\n",
      "[2023-08-09 09:01:31,263][root][INFO] - \n",
      "[2023-08-09 09:01:31,263][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 09:01:31,264][root][INFO] - => Done in 197.172 us\n",
      "[2023-08-09 09:01:31,264][root][INFO] - \n",
      "[2023-08-09 09:01:31,264][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 09:01:31,264][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 09:01:32,174][root][INFO] - => Done in 910.187 ms\n",
      "[2023-08-09 09:01:32,174][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇████▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▆▆▆▇▇█▇█▇▇███████████████████▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇████▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▆▅▅▆▆▇▆▇▅▇▇▇▇▇█▇▇▆▇▆██▆▇▇█▇▆▇▅▇█▇▆█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▇▇█▇█▇▇███████▇█▇█████████▇███▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▆▅▅▆▆▇▆▇▅▇▇▇▇▇█▇▇▆▇▆██▆▇▇█▇▆▇▅▇█▇▆█▅▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇█▇██▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▁▃▄▆▅▅▅▆▅█▅▆▆▆▆▄▆▅▇▅█▄▄▇▆▆▄▅▇▆▇▆▄▆▇▄█▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▇▆█▆▄▆▅▄▅▅▅▂▁▃▃▂▃▂▇▂▄▁▁▂▂▃▂▂▁▁▂▃▂▃▅▅▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇█▇██▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▁▃▄▆▅▅▅▆▅█▅▆▆▆▆▄▆▅▇▅█▄▄▇▆▆▄▅▇▆▇▆▄▆▇▄█▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▇▆█▆▄▆▅▄▅▅▅▂▁▃▃▂▃▂▇▂▄▁▁▂▂▃▂▂▁▁▂▃▂▃▅▅▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇█▇██▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▁▃▄▆▅▅▅▆▅█▅▆▆▆▆▄▆▅▇▅█▄▄▇▆▆▄▅▇▆▇▆▄▆▇▄█▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▇▆█▆▄▆▅▄▅▅▅▂▁▃▃▂▃▂▇▂▄▁▁▂▂▃▂▂▁▁▂▃▂▃▅▅▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▆▆▆▇▇▇▇▇▇▇▇████▇█▇█▇██▇█▇██▇█▇▇█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▁▃▄▆▅▅▅▆▅█▅▆▆▆▆▄▆▅▇▅█▄▄▇▆▆▄▅▇▆▇▆▄▆▇▄█▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▇▆█▆▄▆▅▄▅▅▅▂▁▃▃▂▃▂▇▂▄▁▁▂▂▃▂▂▁▁▂▃▂▃▅▅▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.61583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.74197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 4.58268\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.67786\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.759\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 4.5175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.13825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1788.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 31.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 158.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 1788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s2a2_s2a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/3w47ak5c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_090126-3w47ak5c/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 09:02:10,200][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s3a2_s3a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_090211-aod97xhk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s3a2_s3a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/aod97xhk\u001b[0m\n",
      "[2023-08-09 09:02:16,235][root][INFO] - => Done in 6.035 s\n",
      "[2023-08-09 09:02:16,235][root][INFO] - \n",
      "[2023-08-09 09:02:16,235][root][INFO] - => Env setup ...\n",
      "[2023-08-09 09:02:16,238][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 09:02:16,238][root][INFO] - => Done in 3.055 ms\n",
      "[2023-08-09 09:02:16,238][root][INFO] - \n",
      "[2023-08-09 09:02:16,238][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 09:02:16,872][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 09:02:16,873][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 09:02:16,873][root][INFO] - => Done in 634.615 ms\n",
      "[2023-08-09 09:02:16,873][root][INFO] - \n",
      "[2023-08-09 09:02:16,873][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 09:02:16,873][root][INFO] - => Done in 120.163 us\n",
      "[2023-08-09 09:02:16,873][root][INFO] - \n",
      "[2023-08-09 09:02:16,873][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 09:02:16,873][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 09:02:17,767][root][INFO] - => Done in 894.268 ms\n",
      "[2023-08-09 09:02:17,768][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 1.043 MB of 1.055 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▇▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▃▁▁▁▂▁▁▁▁▁██▁█▁▁▁▁█▁██▁█▁██▁█▁▁█▁▁█▁▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.99833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 4.93592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 1.94092\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.96952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 4.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 1.9975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.4975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 1980.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 1980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s3a2_s3a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/aod97xhk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_090211-aod97xhk/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 09:02:56,292][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s4a2_s4a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_090257-ibpoj3xx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s4a2_s4a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ibpoj3xx\u001b[0m\n",
      "[2023-08-09 09:03:02,628][root][INFO] - => Done in 6.336 s\n",
      "[2023-08-09 09:03:02,629][root][INFO] - \n",
      "[2023-08-09 09:03:02,629][root][INFO] - => Env setup ...\n",
      "[2023-08-09 09:03:02,632][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 09:03:02,632][root][INFO] - => Done in 3.751 ms\n",
      "[2023-08-09 09:03:02,632][root][INFO] - \n",
      "[2023-08-09 09:03:02,633][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 09:03:03,299][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 09:03:03,300][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 09:03:03,300][root][INFO] - => Done in 667.143 ms\n",
      "[2023-08-09 09:03:03,300][root][INFO] - \n",
      "[2023-08-09 09:03:03,300][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 09:03:03,300][root][INFO] - => Done in 106.096 us\n",
      "[2023-08-09 09:03:03,300][root][INFO] - \n",
      "[2023-08-09 09:03:03,300][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 09:03:03,300][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 09:03:04,196][root][INFO] - => Done in 896.379 ms\n",
      "[2023-08-09 09:03:04,197][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 ▁▆▆▇▇▇█▇█▇████████████████████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▅▅▅▆▆▇▆▇▅▇▇▇▇▇▇▇▇▆▇▆▇▇▇█▇█▇▆▇▆▇█▇▇▇▅▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep ▁▆▆▆▇▇▇▇█▇██▇▇██▇█▇█▇███████▇█▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▅▅▅▆▆▇▆▇▅▇▇▇▇▇▇▇▇▆▇▆▇▇▇█▇█▇▆▇▆▇█▇▇▇▅▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D ▅▄▅▅▄▄▃▆▄▇▄▄▄▄▄▃▄▃▆▄▆▂▂▄▂▄▂▃▅▃▆▄▁▄▄▃█▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D ▅▄▅▅▄▄▃▆▄▇▄▄▄▄▄▃▄▃▆▄▆▂▂▄▂▄▂▃▅▃▆▄▁▄▄▃█▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC ▅▄▅▅▄▄▃▆▄▇▄▄▄▄▄▃▄▃▆▄▆▂▂▄▂▄▂▃▅▃▆▄▁▄▄▃█▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC ▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇██▇█▇██▇█▇▇█▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC ▅▄▅▅▄▄▃▆▄▇▄▄▄▄▄▃▄▃▆▄▆▂▂▄▂▄▂▃▅▃▆▄▁▄▄▃█▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep 2.64683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 1.80763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 4.63606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 1.68713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 1.814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 4.5575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 1.569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep 3.18575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep 1.569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 1804.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 170.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 1804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 170\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s4a2_s4a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ibpoj3xx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_090257-ibpoj3xx/logs\u001b[0m\n",
      "/Users/alexandrasouly/code/pax/pax/experiment.py:774: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"conf\", config_name=\"config\")\n",
      "/Users/alexandrasouly/miniconda3/envs/pax/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2023-08-09 09:03:43,339][root][INFO] - => Global setup ...\n",
      "name 3pl_2shap_tc_s5a2_s5a1_eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandrasouly\u001b[0m (\u001b[33mucl-dark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/alexandrasouly/code/pax/wandb/run-20230809_090344-ltlvez9d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m3pl_2shap_tc_s5a2_s5a1_eval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ltlvez9d\u001b[0m\n",
      "[2023-08-09 09:03:48,337][root][INFO] - => Done in 4.998 s\n",
      "[2023-08-09 09:03:48,337][root][INFO] - \n",
      "[2023-08-09 09:03:48,337][root][INFO] - => Env setup ...\n",
      "[2023-08-09 09:03:48,340][root][INFO] - Env Type: meta s| Inner Episode Length: 100\n",
      "[2023-08-09 09:03:48,341][root][INFO] - => Done in 3.793 ms\n",
      "[2023-08-09 09:03:48,341][root][INFO] - \n",
      "[2023-08-09 09:03:48,341][root][INFO] - => Agent setup ...\n",
      "[2023-08-09 09:03:48,996][root][INFO] - Agent Pair: ['PPO_memory', 'PPO_memory', 'PPO_memory']\n",
      "[2023-08-09 09:03:48,996][root][INFO] - Agent seeds: [0, 1, 2]\n",
      "[2023-08-09 09:03:48,996][root][INFO] - => Done in 655.361 ms\n",
      "[2023-08-09 09:03:48,996][root][INFO] - \n",
      "[2023-08-09 09:03:48,996][root][INFO] - => Watcher setup ...\n",
      "[2023-08-09 09:03:48,996][root][INFO] - => Done in 142.097 us\n",
      "[2023-08-09 09:03:48,996][root][INFO] - \n",
      "[2023-08-09 09:03:48,996][root][INFO] - => Runner setup ...\n",
      "[2023-08-09 09:03:48,997][root][INFO] - Training with multishaper eval Runner\n",
      "[2023-08-09 09:03:49,908][root][INFO] - => Done in 911.286 ms\n",
      "[2023-08-09 09:03:49,908][root][INFO] - \n",
      "Number of Training Iterations: 1\n",
      "Training\n",
      "-----------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.058 MB of 1.058 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep ▁▄▅▅▂▆▃▁▂▃▇▄▆█▇▅▃▄▂▆▅▅▃▅█▆▄▇▅▆▅▅▄▅▆▆▅█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 █▅▅▅▁▆▂▁▂▃▆▄▆▇▇▄▃▄▁▅▅▄▃▄▇▆▃▆▅▆▅▄▂▅▆▅▄▇▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 █▃▁▁▂▁▂▁▂▁▁▂▁▁▁▂▂▂▂▂▁▁▂▃▂▂▂▁▂▁▁▂▅▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 ▁▇▇▇█▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep █▄▄▄▁▅▂▁▂▂▅▃▄▅▅▃▂▃▁▄▄▃▃▃▆▄▃▅▄▄▄▃▃▄▄▄▄▅▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep ▁▇▇▇█▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D █▃▁▁▂▁▃▂▂▁▁▂▁▁▁▂▂▂▂▂▂▁▂▃▂▂▃▁▂▁▁▂▅▂▂▂▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D ▂▄▄▃█▂▇█▇▅▂▅▃▁▂▅▆▅▇▃▄▄▆▅▁▃▆▂▄▂▃▄█▄▃▃▅▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D ▁▅▆▇▄▇▅▄▅▆▇▆▇██▆▆▆▅▇▆▆▅▅█▇▅▇▇▇▇▆▄▆▇▇▆█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D █▃▁▁▂▁▃▂▂▁▁▂▁▁▁▂▂▂▂▂▂▁▂▃▂▂▃▁▂▁▁▂▅▂▂▂▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D ▂▄▄▃█▂▇█▇▅▂▅▃▁▂▅▆▅▇▃▄▄▆▅▁▃▆▂▄▂▃▄█▄▃▃▅▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D ▁▅▆▇▄▇▅▄▅▆▇▆▇██▆▆▆▅▇▆▆▅▅█▇▅▇▇▇▇▆▄▆▇▇▆█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC █▃▁▁▂▁▂▂▂▁▁▂▁▁▁▂▂▂▂▂▂▁▂▃▂▂▃▁▂▁▁▂▅▂▂▂▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD ▂▄▄▃█▂▇█▇▅▂▅▃▁▂▅▆▅▇▃▄▄▆▅▁▃▆▂▄▂▃▄█▄▃▃▅▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD ▁▁▁▄▅▁▄▅▁▁▁▁▁▁▁▁▁▁▅▁█▁▁▁▁▅█▁▁▁▁▁▁▁▁▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD ▁▅▆▇▄▇▅▄▅▆▇▆▇██▆▆▆▅▇▆▆▅▅█▇▅▇▇▇▇▆▄▆▇▇▆█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD ▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC █▃▁▁▂▁▂▂▂▁▁▂▁▁▁▂▂▂▂▂▂▁▂▃▂▂▃▁▂▁▁▂▅▂▂▂▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD ▂▄▄▃█▂▇█▇▅▂▅▃▁▂▅▆▅▇▃▄▄▆▅▁▃▆▂▄▂▃▄█▄▃▃▅▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD ▁▁▁▄▅▁▄▅▁▁▁▁▁▁▁▁▁▁▅▁█▁▁▁▁▅█▁▁▁▁▁▁▁▁▁▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD ▁▅▆▇▄▇▅▄▅▆▇▆▇██▆▆▆▅▇▆▆▅▅█▇▅▇▇▇▇▆▄▆▇▇▆█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          episodes 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/global_welfare_per_timestep -0.119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper1 -0.46431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/shaper2 0.03967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/meta_reward/target1 -0.01922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_1 -0.415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/shaper_2 0.065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/reward_per_timestep/target_1 -0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/shaper_welfare_per_timestep -0.175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/target_welfare_per_timestep -0.007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C1D 0.013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/C2D 0.147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D1D 0.011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     grouped_state_probability/D2D 0.819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   grouped_state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C1D 26.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/C2D 294.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D0D 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D1D 22.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      grouped_state_visitation/D2D 1638.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    grouped_state_visitation/START 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDC 0.013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/CDD 0.147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCC 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DCD 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDC 0.011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             state_probability/DDD 0.819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           state_probability/START 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDC 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/CDD 294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCC 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DCD 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDC 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              state_visitation/DDD 1638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            state_visitation/START 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_iteration 999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m3pl_2shap_tc_s5a2_s5a1_eval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucl-dark/tensor-tc/runs/ltlvez9d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230809_090344-ltlvez9d/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# switch agents\n",
    "import yaml\n",
    "game= \"tc\"\n",
    "num_pl = 3\n",
    "num_shap = 2\n",
    "\n",
    "# mean v mean\n",
    "for seed in range(1,6):\n",
    "    seed2=seed\n",
    "    seed1=seed\n",
    "    agent1=2\n",
    "    agent2=1\n",
    "\n",
    "    ########## shaper1\n",
    "    yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed1}.yaml\"\n",
    "    with open(yaml_f) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    run_path1 = config[f'run_path{agent1}']\n",
    "    agent_path1 =   config[f'model_path{agent1}']\n",
    "    ########## shaper 2\n",
    "    yaml_f = f\"/Users/alexandrasouly/code/pax/pax/conf/experiment/multi-shapers/eval/n{num_pl}pl_{num_shap}shap_{game}_eval{seed2}.yaml\"\n",
    "    with open(yaml_f) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    run_path2 = config[f'run_path{agent2}']\n",
    "    agent_path2 =   config[f'model_path{agent2}']\n",
    "    ############# run experiment\n",
    "    name = f\"{num_pl}pl_{num_shap}shap_{game}_s{seed1}a{agent1}_s{seed2}a{agent2}_eval\"\n",
    "    group = f'{num_pl}pl_{num_shap}shap_{game}-comp-switch_agents'\n",
    "    !/Users/alexandrasouly/miniconda3/envs/pax/bin/python3 -m pax.experiment +experiment=multi-shapers/comp/n{num_pl}pl_{num_shap}shap_{game}_comp_eval ++run_path1={run_path1} ++run_path2={run_path2} ++model_path1={agent_path1} ++model_path2={agent_path2} ++wandb.name={name} ++wandb.group={group}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
